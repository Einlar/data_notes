%&latex
%
\documentclass[../template.tex]{subfiles}
\begin{document}


\lesson{3}{10/10/19}
There will be 8 lectures by Prof. Baiesi - the first two about some mathematical tools, and then about scattered arguments, to show how to apply the theoretical physics framework to various topics.\\

\section{Moments and Generating Functions}
Consider a continuous function $f\colon \mathbb{R} \to \mathbb{R}$, ${x} \mapsto f(x)$. The $n$-th \textbf{moment} of $f$ about a point $c \in \mathbb{R}$ is defined as the integral:
\begin{align*}
    \mu_n = \int_{-\infty}^{\infty} (x-c)^n f(x) \dd{x}
\end{align*}


Moments provide a way to quantify, in a certain sense, the \textit{shape} of $f$. For example, if $f(x)$ is a linear density ($[\si{\kilo\g\per\m}]$), then the $0$-th moment is the total mass, the first one (with $c=0$) is the center of mass, and the second is the \textit{moment of inertia}.\\

Moments are especially useful if $f(x)$ is a probability density function (pdf), i.e. a non-negative normalized function. In this case the first moment about $0$ is the \textbf{mean}:
\begin{align*}
    \mu_1 &\equiv \int_{-\infty}^{\infty} x f(x) \dd{x} = \operatorname{E}[X] \equiv \mu; \qquad X \sim f
\end{align*}
where $X$ is a random variable sampled from $f$. Note that, if not specified, a moment is intended to be centered around $c = 0$ (it is a \textit{raw moment} or \textit{crude moment}).\\

The \textit{central second moment}, that is $\mu_2$ with $c = \mu$ is the \textbf{variance}:
\begin{align*}
    \int_{-\infty}^{\infty} (x- \mu)^2 f(x) \dd{x} \equiv \operatorname{E} [(X- \mu)^2] = \operatorname{Var}[X] 
\end{align*}  


A \textbf{moment-generating function} of a real-valued random variable is a certain function $f \colon \mathbb{R}^n \to \mathbb{R}$, $\bm{x} \mapsto f(\bm{x})$ that can be used to \textit{compute} the moment of the distribution where $X$ comes from.\\
More precisely, for a random variable $X$, the moment-generating function $M_X$ is defined as:
\begin{align*}
    M_X(t) \equiv \operatorname{E}[e^{tX}], \quad t \in \mathbb{R} 
\end{align*}
In fact, recall that:
\begin{align*}
    e^{tX} = 1 + tX + \frac{t^2 X^2}{2!} + \dots 
\end{align*}
Hence, as the \textit{expected value} is a linear operator: 
\begin{align*}
    M_X(t) &= \operatorname{E}[e^{tX}] = 1+ t \operatorname{E}[X] + \frac{t^2 \operatorname{E}[X^2]}{2!} + \dots =\\
    &= 1 + t \mu_1 + \frac{t^2 \mu_2}{2!} + \dots
\end{align*}
Note that the distribution's moments are the coefficients of the power series that defines $M_X(t)$.

\begin{expl}
    In fact, the more general definition of a \textbf{generating function} is that of a power-series with \q{hand-picked} coefficients $a_n$, such that by simply knowing the function one can compute $a_n$ in an iterative way.   
\end{expl}

To recover a certain $\mu_n$ we start by differentiating $M_X$ $n$ times with respect to $t$, such that the first $n-1$ terms vanish:
\begin{align*}
    \dv[n]{t} M_X(t) = \underbrace{\frac{n(n-1)\dots 1}{n!}}_{=1} \mu_n + \frac{(n+1)n\dots 2}{(n+1)!} t \mu_{n+1} + \dots   
\end{align*}    
Then, by setting $t=0$, all $\mu_r$ with $r>n$ vanish, leaving only the desired $\mu_n$:  
\begin{align*}
    \dv[n]{t} M_X(t) \big|_{t=0} = \mu_n
\end{align*}   

Finally, we note that a moment-generating function can be constructed even for a multi-dimensional vector $\bm{X} = (X_1, \dots, X_n)^T$ of random variables, by simply taking a scalar product in the exponential:
\begin{align*}
    M_{\bm{X}}(\bm{t}) \equiv \operatorname{E}\left(e^{\bm{t}^T \bm{X}}\right) \qquad \bm{t} \in \mathbb{R}^n
\end{align*} 

\section{Multivariate Gaussian}
Consider now a normal pdf in $d=1$:
\begin{align*}
    f(x;\mu,\sigma) = \frac{1}{\sqrt{2 \pi}\sigma} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2} \right) 
\end{align*} 
We denote a random variable sampled from $f(x;\mu,\sigma)$ as $X \sim \mathcal{N}(\mu,\sigma)$.\\
Suppose that we have multiple random variables $\{X_i\}_{i=1,\dots,n}$, each normally distributed ($X_i \sim \mathcal{N}(\mu_i, \sigma_i)$), with covariance matrix $\Sigma \in \mathbb{R}^{n\times n}$ defined as:
\begin{align*}
    \Sigma_{ij} = \operatorname{E}[(X_i- \mu_i)(X_j - \mu_j)] 
\end{align*}
Their joint pdf is given by a \textbf{multivariate normal distribution} :
\begin{align*}
    f(x_1, \dots, x_n; \bm{\mu}, \Sigma) = \frac{1}{\sqrt{(2\pi)^n \operatorname{det}(\Sigma) }} \exp \left(-\frac{1}{2}(\bm{x}- \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu}) \right) 
\end{align*}

\section{Moments and Gaussians}
We want now to compute the moment generating function for a multivariate gaussian, that is the value of the integral:
\begin{align*}
    M_{\bm{X}}(\bm{t}) = \int_{\mathbb{R}^n} e^{\bm{t}\cdot \bm{x}} f(\bm{x};\bm{\mu},\Sigma) \dd[n]{x}
\end{align*} 

Let's start from the easiest case, and work our way out to the most general one.\\

Recall that the \textbf{gaussian integral}, i.e. the $0$-th moment of a normal univariate distribution is:
\begin{align*}
    \int_{-\infty}^{\infty} \exp\left(-\frac{a}{2}x^2 \right) \mathrm{dx} = \sqrt{\frac{2}{a} \pi } 
\end{align*}

\begin{expl}
    \textbf{Proof}. The integral as is can't be computed in terms of elementary functions. However, its square can be calculated:
    \begin{align*}
        \left(\int_{-\infty}^{\infty}\dd{x}  \exp\left(-\frac{a}{2}x^2 \right)\right)^2 =
        \int_{-\infty}^{\infty} \dd{x} \int_{-\infty}^{\infty} \dd{y} \exp \left(-\frac{2}{2} (x^2 + y^2) \right)
    \end{align*} 
    Transforming to polar coordinates:
    \begin{align*}
        = \int_0^{2\pi} \dd{\theta} \int_{0}^{\infty} \dd{r} \exp\left(-\frac{a}{2}r^2 \right) r = -\frac{2 \pi}{a} \exp\left(-\frac{a}{2}r^2 \right) \Big|_{0}^{\infty} = \frac{2 \pi}{a}  
    \end{align*}
    and we arrive at the desired result by simply taking the square root.
\end{expl}

Consider now the integral of the multivariate case, with $\bm{\mu} = \bm{0}$ (meaning we applied a translation from the general case): 
\begin{align*}
    Z(\Sigma) = \int_{\mathbb{R}^n} \dd[n]{\bm{x}}\exp\left(-\frac{1}{2}\bm{x}^T \Sigma^{-1} \bm{x} \right)
\end{align*}
Notice that the inverse of the covariance matrix $\Sigma^{-1} \equiv A$ is a symmetric positive-definite matrix, thus can be used to define a quadratic form:
\begin{align*}
    \mathbb{A}(\bm{x}) = \sum_{i,j=1}^n x_i A_{ij} x_j
\end{align*} 
To compute the integral, then 


We will show now how to generalize this result to many dimensions.\\
Let $\vec{x}$ be a vector of $n$ dimension: $\vec{x} = (x_i)_{i=1,\dots, n}$. Suppose that $x_i$ are random variables, that can be correlated - that is $p(x_i | x_j) \neq p(x_i) p(x_j)$. All the information about that is contained in the \textit{covariance matrix}, $A_{i i} = \operatorname{var} (x_i)$ and $A_{ij} = A_{ji} = \operatorname{cov}(x_i, x_j)$. \\
The covariance matrix induces a metric:
\begin{align*}
    \mathbb{A}(\vec{x}) = \sum_{i,j=1}^{n} x_i A_{ij} x_j
\end{align*}
We will prove that:
\begin{align*}
    Z(\mathbb{A}) = \int d^n x \exp\left(-\frac{1}{2} \mathbb{A}(\vec{x}) \right) = (2\pi)^{n/2} \prod_{i=1}^n a_i^{-1/2} = (2\pi)^{n/2} (\operatorname{det} A)^{-1/2}
\end{align*}
where $a_i$ are the eigenvalues of $A$.\\

\textbf{Proof}. We can simply apply a coordinate transform through an orthogonal matrix $O$, with $|\operatorname{det}O|=1$:
\begin{align*}
    x_i' = \sum_j O_{ij}x_j
\end{align*}    
So that $A$ becomes a diagonal matrix with eigenvalues $a_i$ on the diagonal. Then $Z(\mathbb{A})$ becomes the multi-dimensional integral of independent gaussians, and we can integrate wrt to each variable separately, arriving at the final result.\\

Let's now consider a more general case:
\begin{align*}
    Z(A, \vec{b}) = \int_{-\infty}^{\infty} d^n x \exp\left(-\frac{1}{2} \mathbb{A}(\vec{x}) + \vec{b} \cdot \vec{x} \right)
\end{align*}
where $\vec{b}\cdot \vec{x} = \sum_i x_i b_i$.\\
We start by finding the minimum of the exponent, by differentiation:
\begin{align*}
    \frac{\partial}{\partial x_i} \left(\frac{1}{2} \mathbb{A}(\vec{x}) - \vec{b} \cdot \vec{x} \right)  = 0 \quad \forall i
\end{align*} 
The solution for the minimum is:
\begin{align*}
    \sum_j A_{ij} x_j = b_i \Rightarrow x_i^* = \sum_j (A^{-1})_{ij} b_i
\end{align*}
Then we apply a coordinate change:
\begin{align*}
    \vec{x} \rightarrow \vec{y}; \vec{x} = \vec{x}^* + \vec{y}
\end{align*}
so that:
\begin{align*}
    -\frac{\mathbb{A}(\vec{x})}{2} + \vec{b}\cdot \vec{x} = -\frac{\mathbb{A}(\vec{y})}{2} + w_2(\vec{b})  
\end{align*}
with:
\begin{align*}
    w_2(\vec{b}) = \frac{1}{2} \sum_{i,j=1}^n b_i (A^{-1})_{ij} b_i = \frac{1}{2} \vec{b}\cdot \vec{x}^*  
\end{align*}
And so:
\begin{align*}
    Z(A,\vec{b}) = e^{w_x(\vec{b})} \int d^n y \exp \left(-\frac{1}{2} \mathbb{A}(\vec{y}) \right) = e^{w_2(\vec{b})} (2\pi)^{n/2} (\operatorname{det} A )^{-1/2}
\end{align*}

\textbf{Exercise}
Compute $Z(A)$ and $Z(A,\vec{b})$ with:
\begin{align*}
    A = \left(\begin{array}{cc}
    3 & -1 \\ 
    -1 & 3
    \end{array}\right); \quad b = \left(\begin{array}{c}
    1 \\ 
    0
    \end{array}\right)
\end{align*}   
(Solve the exercise, and put in a notebook to bring to the exam!)

\subsection{Gaussian expectation values}
We want to compute the average value of the \textit{product} of multiple random variables, following a gaussian distribution of covariance matrix $A$, centered on $0$ ($\vec{b} = 0$).   
\begin{align*}
    \langle x_{k_1} x_{k_2} \dots x_{k_l} \rangle \equiv \frac{1}{Z(A)} \int d^n x x_{k_1} x_{k_2} \dots x_{k_l} \exp\left(-\frac{1}{2} \mathbb{A}(\vec{x}) \right) 
\end{align*}
where the factor $Z(A)^{-1}$ is just a normalization constant.\\
We then introduce a term $\vec{b}$ and compute the derivatives:
\begin{align*}
    \frac{\partial}{\partial b_k} Z(A,\vec{b}) = \int d^n x x_k \exp\left(+\frac{\mathbb{A}}{2} + \vec{b}\cdot \vec{x} \right) 
\end{align*} 
In physics, we say that $b_k$ is \q{coupled} to $x_k$, and that $Z(A,\vec{b})$ is used as "generating function" for $\vec{x}$.\\
Then we have:
\begin{align*}
    \langle x_{k_1} x_{k_2 }\dots x_{k_l} \rangle &= (2\pi)^{-n/2}(\operatorname{det}A )^{-1/2} \left[\frac{\partial}{\partial b_{k_1 }} \frac{\partial}{\partial b_{k_2 }} \dots \frac{\partial}{\partial b_{k_l}} Z(A,\vec{b})   \right]_{\vec{b}=0} =\\
    &= \frac{\partial}{\partial b_{k_1 }} \frac{\partial}{\partial b_{k_2 }} \dots \frac{\partial}{\partial b_{k_l}} e^{w_2(\vec{b})} \Big|_{\vec{b}=0}
\end{align*}    

\subsection{Wick's Theorem}
From the previous formula we know that:
\begin{align*}
    \frac{\partial}{\partial b_i} \text{ pulls down a $b_i$ } 
\end{align*}
So, if we set $b_i = 0$, the result will also be $0$, as $\langle x_i \rangle = 0$.\\
However, with more derivatives, the result can be non-zero, due to correlations between different $x_j$:
\begin{align*}
    \langle x_i x_j \rangle \text{ may be $\neq$ 0 }
\end{align*}    
But always, for an odd number of derivatives, due to the symmetry of the gaussian, we have:
\begin{align*}
    \langle x_i x_j x_k \rangle = 0
\end{align*}
So the expectation value of the product of different random variables, sampled from the same gaussian distribution centered on $0$, is only non-zero for an even number of variables. \\

(Reference: \q{Path integrals in Physics} vol. 1, Chaichian \& Demichev)\\

The contribution from every pair $(k_p, k_q)$  from the list of indices $K = \{k_1, k_2, \dots, k_l\}$ is associated with the covariance $(A^{-1})_{k_p, k_q}$. Then we arrive at Wick's theorem:
\begin{align*}
    \langle x_{k_1} x_{k_2 } \dots x_{k_l}  \rangle = \sum_{P \in \sigma(K)} A^{-1}_{k_{P_1} k_{P_2}} A^{-1}_{k_{P_3}k_{P_4}}\dots A^{-1}_{k_{P_{l-1}} k_{P_l}} = \sum_{P \in \sigma(K)} \langle x_{k_{P_1 }} x_{k_{P_2}} \dots \langle x_{k_{P_{l-1}}} x_{k_{P_l}} \rangle \rangle
\end{align*}
where $P$ is a permutation of indices $K$.  \\
This is true only for gaussians.\\


\textbf{Exercise}.
For a one variable gaussian distribution:
\begin{align*}
    \frac{1}{Z(A)} \exp\left(-\frac{a}{2} x^2 \right) 
\end{align*}
prove that:
\begin{align*}
    \langle x^2 \rangle &= \frac{1}{a} \\
    \langle x^4 \rangle &= \frac{3}{a^2} = 3(\langle x^2 \rangle)^2
\end{align*}
(Hint for $\langle x^4 \rangle$: $k_1 = 1 = k_2 = k_3 = k_4$).

\section{Steepest Descent Integrals}
It is possible to use gaussian integrals to solve a more general set of integrals, with the \textit{Steepest Descent Integrals} (or \textit{saddle point method}).\\

The idea is to convert a generic integral to a Gaussian one (with some approximation, depending on a controllable parameter).\\
We start with:
\begin{align*}
    I(\lambda) = \int d^nx e^{-F(\vec{x})/\lambda}
\end{align*}
As $\lambda \to 0$, it becomes important to study the integral behaviour around the minimum of $F(\vec{x})$, that is the maximum of $-F(\vec{x})$, which is called the \q{saddle point}. This is because this function is analytic, and in the complex plane there are saddle points at the real coordinate of the maximum.\\

We do this by changing variables:
\begin{align*}
    \vec{x} = \vec{x}_c + \sqrt{\lambda} \vec{y}
\end{align*}
where $\vec{x}_c$ is the saddle point, and $\vec{y}$ is a given variable.\\
Then we expand around $\lambda = 0$ and $\vec{x} = \vec{x}_c$, so that:
\begin{align*}
    \frac{1}{\lambda}F(\vec{x}) = \frac{1}{\lambda}F(\vec{x}_c) + \cancel{\frac{1}{\lambda}\sum_i \partial_{x_i} F(\vec{x}_c) y_i \sqrt{\lambda} }+ \frac{1}{\lambda}\frac{1}{2!} \sum_{ij} \partial^2_{x_i x_j} F(\vec{x}_c) y_i y_j \lambda + O(\lambda^{1/2})     
\end{align*}    
Note that, at the maximum, the first derivative is $0$.\\
Substituting in the integral (and transforming the differential) we get:
\begin{align*}
    I(\lambda) = \lambda^{n/2} \exp\left(-\frac{F(\vec{x}_c)}{\lambda} \right) \int d^n y \exp\left[-\frac{1}{2} \sum_{ij} \partial^2_{x_ix_j} F(\vec{x}_c) y_i y_j - R(y) \right]
\end{align*}

The idea is that the integrand of $I(\lambda)$ is \q{more and more similar} to a gaussian as $\lambda$ is lower.\\

Then, by taking the limit $\lambda \to 0$:
\begin{align*}
    I(\lambda) \underset{\lambda \to 0}{\approx} (2\pi \lambda)^{n/2} [\operatorname{det} \partial^2_{x_i x_i} F ]^{-1/2} \exp\left(-\frac{F(\vec{x}_c)}{\lambda} \right)
\end{align*} 

Often, in literature, the following expression can be found the case for $d=1$: 
\begin{align*}
    I(s) = \int g(z) e^{s f(z)} dz \underset{s \to \infty }{\approx}  \frac{(2\pi)^{1/2} g(z_c) e^{s f(z_c)}}{|s g''(z_c)|}
\end{align*}
with $f, g \in \mathbb{R}$, and $z_c$ is the maximum of $f$: $f(z_c) \geq f(z)$.\\
This formula is useful in physics: $s$ can model the system's size, and $s \to\infty$ is the limit for a large system.\\

\textbf{Example}. We can then derive the Stirling approximation. Recall that:
\begin{align*}
    s! = \int_0^\infty x^s e^{-x} dx
\end{align*} 
We then perform a change of variables:
\begin{align*}
    x = z s
\end{align*}
so that:
\begin{align*}
    s! = s^{s+1} \int_0^\infty e^{s(\ln z - z)} dz
\end{align*}
Then we look for the maximum of $f(z) = \ln z - z$:
\begin{align*}
    f'(z) &= \frac{d}{dz}(\ln z - z) = \frac{1}{z} - 1 \Rightarrow z_c = 1\\
    f''(z) &= -\frac{1}{z^2} \Rightarrow f''(z_c) = -1 
\end{align*} 
Using the Steepest Descent formula we get the Stirling approximation for a factorial:
\begin{align*}
    s! \approx \frac{\sqrt{2 \pi}}{|s|^{1/2}} s^{s+1} e^{-s} \approx \sqrt{2 \pi} s^{s+1/2} e^{-s}
\end{align*}
Note that the integral goes from $0$ to $\infty$. We can use the gaussian integral as the maximum is far from $0$, and so we can neglect the left side.\\

\textbf{Exercise}. Compute the Steepest Descent Approximation for the following integral:
\begin{align*}
    I(s) = \int_{-\infty}^{\infty} e^{sx - \cosh x} dx
\end{align*} 
Note that, for this peculiar case, the simple 1D formula does not work - and so one should proceed with the general method (full steps: find maximum, second derivative...).\\

\textbf{Exercise}. 
\begin{align*}
    I(\lambda) = \int_{0}^{\infty} \cos(x) \exp \left(-N \left[\left(x-\frac{\pi}{3} \right)^2 + \left(x-\frac{\pi}{3} \right)^4\right]\right)dx
\end{align*} 
Find also the limit:
\begin{align*}
    I(N)  \xrightarrow[N \to \infty]{}  \dots
\end{align*}

\end{document}
