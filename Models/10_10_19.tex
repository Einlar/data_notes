%&latex
%
\documentclass[../template.tex]{subfiles}
\begin{document}


\lesson{3}{10/10/19}
There will be 8 lectures by Prof. Baiesi - the first two about some mathematical tools, and then about scattered arguments, to show how to apply the theoretical physics framework to various topics.\\

\section{Moments and Generating Functions}
Consider a continuous function $f\colon \mathbb{R} \to \mathbb{R}$, ${x} \mapsto f(x)$. The $n$-th \textbf{moment} of $f$ about a point $c \in \mathbb{R}$ is defined as the integral:
\begin{align*}
    \mu_n = \int_{-\infty}^{\infty} (x-c)^n f(x) \dd{x}
\end{align*}


Moments provide a way to quantify, in a certain sense, the \textit{shape} of $f$. For example, if $f(x)$ is a linear density ($[\si{\kilo\g\per\m}]$), then the $0$-th moment is the total mass, the first one (with $c=0$) is the center of mass, and the second is the \textit{moment of inertia}.\\

Moments are especially useful if $f(x)$ is a probability density function (pdf), i.e. a non-negative normalized function. In this case the first moment about $0$ is the \textbf{mean}:
\begin{align*}
    \mu_1 &\equiv \int_{-\infty}^{\infty} x f(x) \dd{x} = \operatorname{E}[X] \equiv \mu; \qquad X \sim f
\end{align*}
where $X$ is a random variable sampled from $f$. Note that, if not specified, a moment is intended to be centered around $c = 0$ (it is a \textit{raw moment} or \textit{crude moment}).\\

The \textit{central second moment}, that is $\mu_2$ with $c = \mu$ is the \textbf{variance}:
\begin{align*}
    \int_{-\infty}^{\infty} (x- \mu)^2 f(x) \dd{x} \equiv \operatorname{E} [(X- \mu)^2] = \operatorname{Var}[X] 
\end{align*}  


A \textbf{moment-generating function} of a real-valued random variable is a certain function $f \colon \mathbb{R}^n \to \mathbb{R}$, $\bm{x} \mapsto f(\bm{x})$ that can be used to \textit{compute} the moment of the distribution where $X$ comes from.\\
More precisely, for a random variable $X$, the moment-generating function $M_X$ is defined as:
\begin{align*}
    M_X(t) \equiv \operatorname{E}[e^{tX}], \quad t \in \mathbb{R} 
\end{align*}
In fact, recall that:
\begin{align*}
    e^{tX} = 1 + tX + \frac{t^2 X^2}{2!} + \dots 
\end{align*}
Hence, as the \textit{expected value} is a linear operator: 
\begin{align*}
    M_X(t) &= \operatorname{E}[e^{tX}] = 1+ t \operatorname{E}[X] + \frac{t^2 \operatorname{E}[X^2]}{2!} + \dots =\\
    &= 1 + t \mu_1 + \frac{t^2 \mu_2}{2!} + \dots
\end{align*}
Note that the distribution's moments are the coefficients of the power series that defines $M_X(t)$.

\begin{expl}
    In fact, the more general definition of a \textbf{generating function} is that of a power-series with \q{hand-picked} coefficients $a_n$, such that by simply knowing the function one can compute $a_n$ in an iterative way.   
\end{expl}

To recover a certain $\mu_n$ we start by differentiating $M_X$ $n$ times with respect to $t$, such that the first $n-1$ terms vanish:
\begin{align*}
    \dv[n]{t} M_X(t) = \underbrace{\frac{n(n-1)\dots 1}{n!}}_{=1} \mu_n + \frac{(n+1)n\dots 2}{(n+1)!} t \mu_{n+1} + \dots   
\end{align*}    
Then, by setting $t=0$, all $\mu_r$ with $r>n$ vanish, leaving only the desired $\mu_n$:  
\begin{align*}
    \dv[n]{t} M_X(t) \big|_{t=0} = \mu_n
\end{align*}   

Finally, we note that a moment-generating function can be constructed even for a multi-dimensional vector $\bm{X} = (X_1, \dots, X_n)^T$ of random variables, by simply taking a scalar product in the exponential:
\begin{align*}
    M_{\bm{X}}(\bm{t}) \equiv \operatorname{E}\left(e^{\bm{t}^T \bm{X}}\right) \qquad \bm{t} \in \mathbb{R}^n
\end{align*} 

\section{Multivariate Gaussian}
Consider now a normal pdf in $d=1$:
\begin{align*}
    f(x;\mu,\sigma) = \frac{1}{\sqrt{2 \pi}\sigma} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2} \right) 
\end{align*} 
We denote a random variable sampled from $f(x;\mu,\sigma)$ as $X \sim \mathcal{N}(\mu,\sigma)$.\\
Suppose that we have multiple random variables $\{X_i\}_{i=1,\dots,n}$, each normally distributed ($X_i \sim \mathcal{N}(\mu_i, \sigma_i)$), with covariance matrix $\Sigma \in \mathbb{R}^{n\times n}$ defined as:
\begin{align*}
    \Sigma_{ij} = \operatorname{E}[(X_i- \mu_i)(X_j - \mu_j)] 
\end{align*}
Their joint pdf is given by a \textbf{multivariate normal distribution} :
\begin{align*}
    f(x_1, \dots, x_n; \bm{\mu}, \Sigma) = \frac{1}{\sqrt{(2\pi)^n \operatorname{det}(\Sigma) }} \exp \left(-\frac{1}{2}(\bm{x}- \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu}) \right) 
\end{align*}

\section{Moments and Gaussians}
We want now to compute the moment generating function for a multivariate gaussian, that is the value of the integral:
\begin{align}
    M_{\bm{X}}(\bm{t}) = \int_{\mathbb{R}^n} e^{\bm{t}\cdot \bm{x}} f(\bm{x};\bm{\mu},\Sigma) \dd[n]{x}
    \label{eqn:multivariate_moment}
\end{align} 

Let's start from the easiest case, and work our way out to the most general one.\\

Recall that the \textbf{gaussian integral}, i.e. the $0$-th moment of a normal univariate distribution is:
\begin{align*}
    \int_{-\infty}^{\infty} \exp\left(-\frac{a}{2}x^2 \right) \mathrm{dx} = \sqrt{\frac{2}{a} \pi } 
\end{align*}

\begin{expl}
    \textbf{Proof}. The integral as is can't be computed in terms of elementary functions. However, its square can be calculated:
    \begin{align*}
        \left(\int_{-\infty}^{\infty}\dd{x}  \exp\left(-\frac{a}{2}x^2 \right)\right)^2 =
        \int_{-\infty}^{\infty} \dd{x} \int_{-\infty}^{\infty} \dd{y} \exp \left(-\frac{2}{2} (x^2 + y^2) \right)
    \end{align*} 
    Transforming to polar coordinates:
    \begin{align*}
        = \int_0^{2\pi} \dd{\theta} \int_{0}^{\infty} \dd{r} \exp\left(-\frac{a}{2}r^2 \right) r = -\frac{2 \pi}{a} \exp\left(-\frac{a}{2}r^2 \right) \Big|_{0}^{\infty} = \frac{2 \pi}{a}  
    \end{align*}
    and we arrive at the desired result by simply taking the square root.
\end{expl}

Consider now the integral of the multivariate case, with $\bm{\mu} = \bm{0}$ (meaning we applied a translation from the general case): 
\begin{align*}
    Z(\Sigma) = \int_{\mathbb{R}^n} \dd[n]{\bm{x}}\exp\left(-\frac{1}{2}\bm{x}^T \Sigma^{-1} \bm{x} \right)
\end{align*}
Notice that the inverse of the covariance matrix $\Sigma^{-1} \equiv A$ is a symmetric positive-definite matrix, thus can be used to define a quadratic form:
\begin{align*}
    \mathbb{A}(\bm{x}) = \sum_{i,j=1}^n x_i A_{ij} x_j
\end{align*} 
The integral can be computed by applying a change of variables, \textit{rotating} $\bm{x}$ such that $A$ becomes diagonal:
\begin{align*}
    \bm{y} = O \bm{x}; \qquad O \in \bb{R}^{n\times n}; \quad O^T = O^{-1}, \> \operatorname{det}(O) = 1 
\end{align*}   
where $O$ is an orthogonal matrix, with a set of orthogonal eigenvectors of $A$ as columns, such that:
\begin{align*}
    O A O^{-1} = \operatorname{diag}(a_1, \dots, a_n) 
\end{align*}  
with $a_i$ being the eigenvalues of $A$.\\
Note that, as $\operatorname{det}(O) = 1$, the volume element in the integral does not change. So, by substituting:
\begin{align*}
    \bm{x} = O^{-1} \bm{y}; \qquad \bm{x}^T = \bm{y}^T (O^{-1})^T = \bm{y}^T O
\end{align*}
in the integral, we get:
\begin{align}\nonumber
    Z(A) &= \int_{\mathbb{R}^n} \dd[n]{\bm{y}} \exp\left(-\frac{1}{2} \bm{y}^T O A O^T \bm{y} \right) = \int_{\mathbb{R}^n} \dd[n]{\bm{y}} \exp\left(-\frac{1}{2} \sum_{i=1}^n a_i y_i^2 \right) =\\
    &= \prod_{i=1}^n \int_{\mathbb{R}} \dd{y_i} \exp\left(-\frac{1}{2} a_i y_i^2 \right) = (2\pi)^{n/2} \prod_{i=1}^n a_i^{-1/2} \underset{(a)}{=}  (2\pi)^{n/2} (\operatorname{det}(A))^{-1/2}
    \label{eqn:ZA}
\end{align} 
where in (a) we noted that the determinant of a matrix is the product of its eigenvalues.\\

We are now ready to consider the more general case of (\ref{eqn:multivariate_moment}), by simply adding a linear term in the exponential of $Z(A)$:  
\begin{align}
    Z(A, \bm{b}) \equiv \int_{-\infty}^{\infty} \dd[n]{\bm{x}} \exp\left(-\frac{1}{2} \mathbb{A}(\bm{x}) + \bm{b} \cdot \bm{x} \right) \qquad \bm{b} \cdot \bm{x} = \sum_{i=1}^n b_i x_i
    \label{eqn:ZAb-def}
\end{align}
To compute this integral, a trick is to translate the maximum of the exponential to the origin. So we start by differentiating:
\begin{align}
    \frac{\partial}{\partial x_i} \left(\frac{1}{2} \mathbb{A}(\bm{x}) - \bm{b} \cdot \bm{x} \right) \overset{!}{=}  0 \quad \forall i
    \label{eqn:derivative}
\end{align} 
Note that:
\begin{align*}
    \pdv{x_i} \mathbb{A}(\bm{x}) &= \pdv{x_i} \sum_{ab} x_a A_{ab} x_b = \sum_{ab} \delta_{ai} A_{ab} x_b + \sum_{ab} x_a A_{ab} \delta_{bi} =\\
    &= \sum_b A_{ib} x_b + \sum_{a} x_a A_{ai}
\intertext{By renaming the first summation variable to $a$, we get:}
    &= \sum_a (A_{ia} + A_{ai}) x_a \underset{(b)}{=} 2 \sum_a A_{ia} x_a = 2 A \bm{x}
\end{align*} 
where in (b) we used the fact that $A$ is symmetrical ($A_{ij} = A_{ji}$).\\
Substituting in (\ref{eqn:derivative}):
\begin{align*}
    \frac{1}{\cancel{2}} \cancel{2} \sum_j A_{ij} x_j = b_i \quad \forall i \underset{(c)}{\Leftrightarrow}  A^T \bm{x} = \bm{b} \underset{(d)}{\Leftrightarrow}  \bm{x^*} = A^{-1} \bm{b}
\end{align*} 
In (c) we noted that $b_i$ is the scalar product between the $i$-th column of $A$ and $\bm{x}$, leading to the transpose in the matrix notation. Of course, as $A = A^T$, in (d) we simply dropped the transpose.\\

We can now apply the coordinate change:
\begin{align*}
    \bm{x} = \bm{x^*} + \bm{y}
\end{align*}
Substituting in the exponential argument:
\begin{align} \nonumber
    -\frac{\mathbb{A}(\bm{x})}{2} + \bm{b} \cdot \bm{x} &= -\frac{1}{2} \bm{x}^T A \bm{x} + \bm{x}^T \bm{b} = -\frac{1}{2}(\bm{x^*} + \bm{y})^T A (\bm{x^*} + \bm{y}) + (\bm{x^*} + \bm{y})^T \bm{b} =\\
    &= -\frac{1}{2}\left [\bm{{x^*}}^T A \bm{x^*} + \bm{y}^T A \bm{y} + \cancel{\bm{{x^*}}^T A \bm{y}} + \cancel{\bm{y}^T A \bm{x^*}} \right] + \bm{{x^*}}^T A \bm{x^*} +\cancel{ \bm{y}^T A \bm{x^*}}
    \label{eqn:change-A}
\end{align}
Note, in fact, that $\bm{y}^T A \bm{x^*} = (\bm{{x^*}}^T A^T \bm{y})^T = (\bm{{x^*}}^T A \bm{y})^T$ because $A$ is symmetric, and then $(\bm{{x^*}}^T A \bm{y})^T = \bm{{x^*}}^T A \bm{y}$ because they are scalars.\\
Then:
\begin{align*}
    \bm{{x^*}}^T A \bm{x^*} = (A^{-1}\bm{b})^T A A^{-1} \bm{b} = \bm{b}^T (A^{-1})^T \bm{b} = \bm{b}^T A^{-1} \bm{b} = \bm{b}\cdot \bm{x^*}
\end{align*}
And substituting in (\ref{eqn:change-A}):
\begin{align*}
    -\frac{\mathbb{A}(\bm{x})}{2} + \bm{b}\cdot \bm{x} = -\frac{1}{2} \bm{y}^T A \bm{y} + \underbrace{\frac{1}{2} \bm{b}\cdot \bm{x^*}}_{\omega_2(\bm{b})} 
\end{align*}

To simplify notation, let's define:
\begin{align}
    w_2(\bm{b}) = \frac{1}{2} \sum_{i,j=1}^n b_i (A^{-1})_{ij} b_i = \frac{1}{2} \bm{b}\cdot \bm{x^*}  
    \label{eqn:w2}
\end{align}

As the change of variables involves only a translation by a constant value, the volume element in the integral does not change, leading to:
\begin{align}\nonumber
    Z(A, \bm{b}) &= \int_{-\infty}^{\infty} \dd[n]{\bm{y}} \exp\left(-\frac{\mathbb{A}(\bm{y})}{2}  + \omega_2(\bm{b})\right)
\intertext{Note that $\omega_2(\bm{b})$ is constant, thus can be extracted from the integral:}
    &= e^{\omega_2 (\bm{b})} \int_{-\infty}^{\infty} \dd[n]{\bm{y}}\exp\left(-\frac{\mathbb{A}(\bm{y})}{2} \right) \underset{(\ref{eqn:ZA})}{=} e^{\omega_2(\bm{b})} (2 \pi)^{n/2} (\operatorname{det}A )^{-1/2}
    \label{eqn:ZAb}
\end{align} 

\begin{expl}
    Another way to solve the integral for $Z(A,\bm{b})$ is by using the matrix equivalent of \q{completing the square}. We start by considering the argument of the exponential in (\ref{eqn:ZAb-def}):
    \begin{align*}
        -\frac{1}{2}(\bm{x}^T A \bm{x} -2\bm{b}^T \bm{x}) 
    \end{align*}
    $\bm{x}^T A \bm{x}$ has the role of the square, and $-2\bm{b}^T \bm{x}$ that of the double product. \\
    We can then sum and subtract a constant vector $\bm{c}$ in order to rewrite:
    \begin{align*}
        \bm{x}^T A \bm{x} - 2\bm{b}^T \bm{x} + \bm{c}-\bm{c} = \bm{y}^T A \bm{y} - \bm{c}
    \end{align*}  
    for some $\bm{y} \in \bb{R}^n$.\\
    Comparing to a generic square:
    \begin{align*}
        (\bm{a} + \bm{b})^TA(\bm{a} + \bm{b}) = \bm{a}^T A \bm{a} + \bm{b}^T A \bm{b} + 2\bm{a}^T A \bm{b}
    \end{align*}
    we note that $\bm{a} = \bm{x}$ and $\bm{b} = -A^{-1}\bm{b}$, leading to:
    \begin{align*}
        \bm{x}^T A \bm{x} - 2\bm{b}^T \bm{x} = (\bm{x} - A^{-1}\bm{b})^T A (\bm{x}-A^{-1}\bm{b})- \bm{b}^T A^{-1} \bm{b}
    \end{align*}  
    Defining $A^{-1}\bm{b} \equiv \bm{x^*}$ and $\bm{y} = \bm{x}-\bm{x^*}$ then leads to the same calculations as before.  
\end{expl}

\begin{exo}[Multivariate Gaussian Integral]
    Compute $Z(A)$ and $Z(A,\vec{b})$ with:
    \begin{align*}
        A = \left(\begin{array}{cc}
        3 & -1 \\ 
        -1 & 3
        \end{array}\right); \quad b = \left(\begin{array}{c}
        1 \\ 
        0
        \end{array}\right)
    \end{align*}   
    Note that $\operatorname{det}A = 8$, and:
    \begin{align*}
        A^{-1} = \frac{1}{8} \left(\begin{array}{cc}
        3 & 1 \\ 
        1 & 3
        \end{array}\right) 
    \end{align*} 
    So, by simply using (\ref{eqn:ZA}) and (\ref{eqn:ZAb}):
    \begin{align*}
        Z(A,0) &= \frac{(2\pi)^{2/2}}{\sqrt{8}} = \frac{\pi}{\sqrt{2}} \\
        \span \frac{1}{2} \left(\begin{array}{cc}
        1 & 0
        \end{array}\right) \frac{1}{8} \left(\begin{array}{cc}
        3 & 1 \\ 
        1 & 3
        \end{array}\right) \left(\begin{array}{c}
        1 \\ 
        0
        \end{array}\right) = \frac{3}{16} \\
        Z(A, \bm{b}) &= \frac{\pi}{\sqrt{2}} \exp\left(\frac{3}{16} \right)   
    \end{align*}
\end{exo}

\subsection{Gaussian expectation values}
The result in (\ref{eqn:ZAb}) is exactly what we need to compute the moment generating function for the multivariate normal (\ref{eqn:multivariate_moment}).\\

So, we can finally compute moments:   
\begin{align*}
    \langle x_{k_1} x_{k_2} \dots x_{k_l} \rangle \equiv \frac{1}{Z(A)} \int \dd[n]{\bm{x}} x_{k_1} x_{k_2} \dots x_{k_l} \exp\left(-\frac{1}{2} \mathbb{A}(\bm{x}) \right) 
\end{align*}
by simply deriving the generating function $Z(A, \bm{b})$ with respect to certain variables in $\bm{b}$. For example:
\begin{align*}
    \langle x_k \rangle = \frac{1}{Z(A)} \frac{\partial}{\partial b_k} Z(A,\vec{b}) = \frac{1}{Z(A)}  \int \dd[n]{\bm{x}}  x_k \exp\left(-\frac{\mathbb{A}(\bm{x})}{2} + \bm{b}^T \bm{x} \right) 
\end{align*} 
For the general case:
\begin{align*}
    \langle x_{k_1} x_{k_2 }\dots x_{k_l} \rangle &= (2\pi)^{-n/2}(\operatorname{det}A )^{-1/2} \left[\frac{\partial}{\partial b_{k_1 }} \frac{\partial}{\partial b_{k_2 }} \dots \frac{\partial}{\partial b_{k_l}} Z(A,\bm{b})   \right]_{\bm{b}=\bm{0}} =\\
    &= \frac{\partial}{\partial b_{k_1 }} \frac{\partial}{\partial b_{k_2 }} \dots \frac{\partial}{\partial b_{k_l}} e^{w_2(\bm{b})} \Big|_{\bm{b}=\bm{0}}
\end{align*}    
In physics, we say that $b_k$ is \q{coupled} to $x_k$, and that $Z(A,\bm{b})$ is used as \q{generating function} for $\bm{x}$.\\

\subsection{Wick's Theorem}
From the previous formula we know that:
\begin{align*}
    \frac{\partial}{\partial b_i} \text{ pulls down a $b_i$ } 
\end{align*}
Explicitly, recall that:
\begin{align*}
    \omega_2 (\bm{b}) = \frac{1}{2} \bm{b}^T A^{-1} \bm{b}
\end{align*}
and so:
\begin{align*}
    \pdv{b_i} e^{\omega_2(\bm{b})} = \frac{1}{2} e^{\omega_2{\bm{b}}} \pdv{b_i} \sum_{tk}b_t A^{-1}_{tk} b_k = e^{\omega_2 \bm{b}} \sum_k A_{ik}^{-1} b_k
\end{align*}
If we now set $\bm{b} = \bm{0}$, the result will be $0$, meaning that:
\begin{align*}
    \langle x_i \rangle = \pdv{b_i} \frac{Z(A,\bm{b})}{Z(A)} = 0 
\end{align*}  
This result is expected, as in $Z(A, \bm{b})$ all random variables are centered in $0$.\\
However, note that if we derive one more time, with respect to some $b_l$:
\begin{align*}
    \pdv{b_i} \pdv{b_l} e^{\omega_2(\bm{b})} = e^{\omega_2 \bm{b}} \sum_s A_{ls}^{-1} b_s \sum_k A_{ik}^{-1} b_k + e^{\omega_2 \bm{b}} A_{il}^{-1} 
\end{align*}   
And now, if we set $\bm{b} = \bm{0}$, the result may be $\neq 0$.\\
Note that if we derive one more time we return to the previous situation - and the result will be also $0$.\\    

In general, every moment of odd-order is $0$, due to the symmetry of the gaussian, we have:
\begin{align*}
    \langle x_i x_j x_k \rangle = 0
\end{align*}
So the expectation value of the product of different random variables, sampled from the same gaussian distribution centered on $0$, is only non-zero for an even number of variables. This result is known as the \textbf{Wick's theorem} (also known in literature as the \textbf{Isserlis theorem}).\\

By extending this argument, one can find a way to compute the even-order moments, leading to the following formula (which we will not prove):
\begin{align*}
    \langle x_{k_1} x_{k_2 } \dots x_{k_l}  \rangle = \sum_{P \in \sigma(K)} A^{-1}_{k_{P_1} k_{P_2}} A^{-1}_{k_{P_3}k_{P_4}}\dots A^{-1}_{k_{P_{l-1}} k_{P_l}} = \sum_{P \in \sigma(K)} \langle x_{k_{P_1 }} x_{k_{P_2}} \rangle \dots \langle x_{k_{P_{l-1}}} x_{k_{P_l}} \rangle
\end{align*}
where $(k_p, k_q)$ are a pair of indices from $K = \{k_1, \dots, k_l\}$, and $P$ is a permutation of $K$, so that $(k_{P_1}, k_{P_2})$ is the first pair of indices after the permutation $P$. The sum is over all the distinct ways of partitioning $l=2s$ variables in pairs to obtain distinct products of $s$ groups.\\
So, the total number of terms to be added will be $(2s)!/(2^s s!)$ - that is the total number of permutation of $2s$ elements, where the order within couples does not matter ($2^s$) and neither the order of the couples themselves ($s!$).\\
Note that:
\begin{align*}
    \frac{(2s)!}{2^s n!} = (2s-1)! = (2s-1)(2s-3)(2s-5)\dots 
\end{align*}
Where $!!$ denotes the \textit{double factorial}, not to be confused with the factorial of a factorial (which requires brackets: $(a!)!$).  

\begin{exo}[Wick's theorem]
Consider a univariate normal distribution:
\begin{align*}
    f(x) = \frac{1}{Z(A)} \exp\left(-\frac{a}{2} x^2 \right) 
\end{align*}
Show that:
\begin{align*}
    \langle x^2 \rangle &= \frac{1}{a} \\
    \langle x^4 \rangle &= \frac{3}{a^2} = 3(\langle x^2 \rangle)^2
\end{align*}

Here the $A$ matrix is just the scalar $a = \sigma^{-2}$. As the pdf is univariate, there is only one index possible $K = \{1\}$. As $(2-1)!! = 1!! = 1$, there is only one term in the summation, thus: 
\begin{align*}
    \langle x^2 \rangle = A^{-1}_{11} =\frac{1}{a} 
\end{align*}   
For the $4$-th order, however, we have more combinations: $(4-1)!! = 3!! = 3 \cdot 1 = 3$. Again, there is only one possible index, so all terms will be the same:
\begin{align*}
    \langle x^4 \rangle = A^{-1}_{11}A^{-1}_{11} + A_{11}^{-1} A_{11}^{-1} + A_{11}^{-1} A_{11}^{-1} = \frac{3}{a^2} = 3 (\langle x^2 \rangle)^2 
\end{align*}
\end{exo}
\section{Steepest Descent Integrals}
It is possible to use gaussian integrals to solve a more general set of integrals, thanks to the \textit{Steepest Descent approximation}.\\
We start with an integral of the form:
\begin{align}\label{eqn:steepest-form}
    I(\lambda) \equiv \int_S \dd[n]{x} \exp\left(-\frac{F(\bm{x})}{\lambda} \right)
\end{align}
where $\lambda$ is a small parameter (the approximation is more and more accurate as $\lambda \to 0$), $F(\bm{x})$ has a global minimum in $\bm{x_0} \in (a,b)$ and $S \subseteq \mathbb{R}^n$ is a sufficiently large region.\\
Note that, if $\lambda$ is lowered, the integral is dominated by the neighborhood of the minimum $\bm{x_0}$. In fact:
\begin{align*}
    h(\bm{x}) \equiv \exp\left(-\frac{F(\bm{x})}{\lambda}\right); \quad \frac{h(\bm{x_0})}{h(\bm{x})} = \exp\left(-\frac{1}{\lambda}(F(\bm{x_0}) - F(\bm{x})) \right) 
\end{align*}
As $F(\bm{x_0})-F(\bm{x}) < 0$, the ratio becomes exponentially higher if $\lambda \to 0$. Basically, for $\lambda \to 0$, the integrand function becomes \q{more and more similar to a gaussian}.\\ 

To compute the integral, then, we translate the coordinates about $\bm{x_0}$: 
\begin{align*}
    \bm{x} = \bm{x_0} + \sqrt{\lambda} \bm{y} \qquad \dd[n]{\bm{x}} = \lambda^{n/2} \dd[n]{\bm{y}}
\end{align*}
Then we perform a second order Taylor expansion about $\lambda = 0$ and $\bm{x} = \bm{x_0}$:  
\begin{align*}
    \frac{1}{\lambda}F(\bm{x}) = \frac{1}{\lambda}F(\bm{x_0}) + \cancel{\frac{1}{\lambda}\sum_i \partial_{x_i} F(\bm{x_0}) y_i \sqrt{\lambda} }+ \frac{1}{\cancel{\lambda}}\frac{1}{2!} \sum_{ij} \partial^2_{x_i x_j} F(\bm{x_0}) y_i y_j \cancel{\lambda} + O(\lambda^{1/2})     
\end{align*}    
where we cancelled the first derivative, as $\bm{x_0}$ is a stationary point for $F$.\\

Substituting back in the integral we get:
\begin{align*}
    I(\lambda) = \lambda^{n/2} \exp\left(-\frac{F(\bm{x_0})}{\lambda} \right) \int_{S'}\dd[n]{\bm{y}} \exp\left[-\frac{1}{2} \sum_{ij} \partial^2_{x_ix_j} F(\bm{x_0}) y_i y_j - R(\bm{y}) \right]
\end{align*}
This is a gaussian integral $Z(A)$, with $A$ being the Hessian of $F$ evaluated at the minimum $\bm{x_0}$ (or, equivalently, at the maximum of $-F(\bm{x})$).\\
Now, for $\lambda$ sufficiently small, we can ignore $R(\bm{y})$ and compute the integral with (\ref{eqn:ZA}), leading to the approximation:  
\begin{align}
    I(\lambda) \underset{\lambda \to 0}{\approx} (2\pi \lambda)^{n/2} [\operatorname{det} \partial^2_{x_i x_i} F(\bm{x_0})]^{-1/2} \exp\left(-\frac{F(\bm{x_0})}{\lambda} \right)
    \label{eqn:steepest-descent}
\end{align} 
Doing this, we implicitly integrated over the entire $\mathbb{R}^n$. This is fine because, for $\lambda \to 0$, the gaussian is \q{peaked} in a small region around $\bm{x_0}$, and vanishes exponentially moving further away.\\  

The Steepest Descent approximation generalizes Laplace's method for calculating integrals, which has a much simpler expression for the limited case of univariate integrals:
\begin{align}
    I(s) = \int g(z) e^{s f(z)} dz \underset{s \to \infty }{\approx}  \frac{(2\pi)^{1/2} g(z_c) e^{s f(z_c)}}{|s f''(z_c)|^{1/2}}
    \label{eqn:laplace-formula}
\end{align}
with $f, g \in \mathbb{R}$, and $z_c$ is the maximum of $f$, i.e. $f(z_c) \geq f(z)\> \forall z \in (a,b)$.\\
This formula is useful in physics: $s$ can model the system's size, and $s \to\infty$ is then the limit for a large system.\\

\begin{example}[Stirling approximation]
We can use the Steepest Descent approximation to derive the formula for the Stirling approximation of factorials.\\
Recall that a factorial is merely the $\Gamma$ function evaluated on $\mathbb{N}$:  
\begin{align*}
    s! = \int_0^\infty x^s e^{-x} dx
\end{align*} 
We then perform a change of variables:
\begin{align*}
    x = z s
\end{align*}
so that:
\begin{align*}
    s! = s^{s+1} \int_0^\infty e^{s(\ln z - z)} dz
\end{align*}
This is an integral in the form:
\begin{align*}
    \int \exp\left(-\frac{F(x)}{\lambda} \right)
\end{align*}
if we let $\lambda = 1/s$ and $F(z) = z-\ln z$. So we need to find the minimum of $F(z)$:
\begin{align*}
    F'(z) &= \dv{z}(z-\ln z) = 1-\frac{1}{z} \overset{!}{=}  0 \Rightarrow z_c =  1\\
    F''(z) &= \frac{1}{z^2} \Rightarrow F''(z_c) = 1 > 0 
\end{align*}   
We can now apply (\ref{eqn:steepest-descent}), leading to:
\begin{align*}
    s! \underset{s \to \infty}{\approx}  \left(\frac{2 \pi}{s} \right)^{1/2} (1)^{1/2} e^{-s} = \sqrt{2 \pi} s^{s+\frac{1}{2} } e^{-s}
\end{align*}
Note that the same result can be obtained by using the much simpler (\ref{eqn:laplace-formula}), with $g(z) \equiv 1$ and $f(z) = \ln z - z$.
\end{example} 
\clearpage
\begin{exo}[Steepest Descent Approximation] Compute the Steepest Descent Approximation for the following integral (for $s \to \infty$):
\begin{align*}
    I(s) = \int_{-\infty}^{\infty} e^{sx - \cosh x} dx
\end{align*} 
By collecting a $s$ in the exponential argument:
\begin{align*}
    I(s) = \int_{-\infty}^{\infty} \exp\left(s\left(x-\frac{\cosh x}{s} \right)\right)
\end{align*}
we can bring back to the form of (\ref{eqn:steepest-form}) with $F(x) = \cosh x/s -x$ and $\lambda = s^{-1}$.\\
We find the minimum of $F(x)$ by differentiating:
\begin{align*}
    F'(x) &= \frac{\sinh x}{s} -1 \overset{!}{=} 0 \Rightarrow x_0 = \sinh^{-1} s\\
    F''(x) &=  \frac{\cosh x}{s} \Rightarrow F''(x_0) = \frac{\cosh \sinh^{-1} s}{s} = \frac{\sqrt{1+s^2}}{s} > 0 
\end{align*}   
Finally, by applying (\ref{eqn:steepest-descent}) we obtain the result:
\begin{align*}
    I(s) &\underset{s \to \infty}{\approx} \sqrt{\frac{2 \pi}{s} } \sqrt{\frac{s}{\sqrt{1+s^2}} } \exp\left(\frac{\sqrt{1+s^2}}{s} - \sinh^{-1} s \right) =\\
    &= \frac{\sqrt{2\pi}}{(1+s^2)^{1/4}} \exp\left(\frac{\sqrt{1+s^2}}{s} - \sinh^{-1}s \right) 
\end{align*}

Note that, for this peculiar case, the simple 1D formula does not work (why?) - and so one should proceed with the general method (full steps: find maximum, second derivative...).
\end{exo}

\begin{exo}[Laplace's formula]
    Compute:
\begin{align*}
    I(N) = \int_{0}^{\infty} \cos(x) \exp \left(-N \left[\left(x-\frac{\pi}{3} \right)^2 + \left(x-\frac{\pi}{3} \right)^4\right]\right)dx
\end{align*} 
in the limit $N \to \infty$.\\

For this exercise we can use Laplace's formula (\ref{eqn:laplace-formula}) with:
\begin{align*}
    g(x) = \cos (x) \qquad f(x) = -\left[\left(x-\frac{\pi}{3} \right)^2 + \left(x-\frac{\pi}{3} \right)^4\right]
\end{align*} 
By looking at $f(x)$ one can see directly that it has a global maximum in $x_0 = \pi/3$. In fact:  
\begin{align*}
    f'(x) &= -\left[2\left(x-\frac{\pi}{3} \right) + 4\left(x-\frac{\pi}{3} \right)^3 \right] \overset{!}{=} 0 \Leftrightarrow x_0 = \frac{\pi}{3}\\
    f''(x) &= -\left[2+12\left(x-\frac{\pi}{3} \right)^2\right]  \Rightarrow f''(x_0) = -2 < 0
\end{align*} 
And so we arrive at:
\begin{align*}
    I(N) \underset{N \to \infty}{\approx}  \frac{(2\pi)^{1/2} \cos(\pi/3) e^{N \cdot 0}}{|N(-2)|^{1/2}} = \frac{1}{2} \sqrt{\frac{\pi}{N}}  
\end{align*}
\end{exo}
\end{document}
