%&latex
%
\documentclass[../template.tex]{subfiles}
\begin{document}

%\section{Introduction}
\lesson{14}{18/11/19}

\begin{comment}
We want to show that there are cases where the stochastic equation (or the resulting path integral) assume a particular form that is theoretically advantageous to be studied in an analytical way, leading to the \textit{Feynmann-Kac} formula, useful both in stochastic processes and in quantum mechanics (note that, in the latter, it needs to be generalized to complex numbers - which isn't rigorous, but still leads to exact results).

During last lecture, when discussing the Harmonic Oscillator in the overdamped limit, we wrote that:
\begin{align*}
    \dd{x} = -k x \dd{t} + \sqrt{2D} \dd{B} \qquad k = \frac{m\omega^2}{\gamma} 
\end{align*}
Then, recall that:
\begin{align*}
    W(x,t|x_0,0) = \exp\left(-\frac{x^2 - x_0^2}{4D}k + kt \right) \langle \exp\left(-\int_0^t V(x(\tau))\dd{\tau}\right) \delta(x(t)-x) \rangle_W 
\end{align*}
where the average is intended to be computed in the Wiener measure:
\begin{align*}
    \langle \cdots \rangle_W = \int \prod_{\tau = 0}^{t} \frac{\dd{x(\tau)}}{\sqrt{4 \pi D} \dd{\tau}} \exp\left[-\frac{1}{4D} \int_0^t \dot{x}^2 (\tau )\dd{\tau}\right] \qquad V(x) = \frac{k^2 x^2}{4D} 
\end{align*}
Integrals of this kind appear quite often when a particle moves in a 3D potential. Our goal is now to consider the more general case of a particle moving in a conservative force-field, and see how the average:
\begin{align*}
    \langle \exp\left(-\int_0^t V(x(\tau)) \dd{\tau} \right) \delta(x(t)-x)\rangle_W \equiv W_B(x,t)
\end{align*} 
will reappear, i.e. we will observe how general problems have a similar formulation.
Note that $V(x)$ is \textit{proportional} to the original harmonic potential:
\begin{align*}
    U(x) = \frac{1}{2} m \omega^2 x^2 
\end{align*} 
\end{comment}

\section{Particle in a conservative force-field}
In last section, we examined a particle of radius $a$ immersed in a harmonic potential $U(x) = m \omega^2 x^2/2$, moving through a medium with viscosity $\eta$ and subject to thermal fluctuations of amplitude proportional to $\sqrt{2D}$, so that its dynamics are described by the following \textit{stochastic differential equation}: 
\begin{align*}
    \dd{x} = -k x \dd{t} + \sqrt{2D} \dd{B}    \qquad k = \frac{m \omega^2}{\gamma}\qquad \gamma = 6 \pi \eta a 
\end{align*}
The solution, expressed as the \textit{transition probability} between any two given points, is a path integral:
\begin{align}\label{eqn:harmonic-tprob}
    W(x_t,t|x_0,0) = \exp\left(-\frac{x_t^2 - x_0^2}{4D}k + \frac{kt}{2}  \right) \hlc{Yellow}{\langle \exp\left(-\int_0^t V(x(\tau)) \dd{\tau}\right) \delta(x(t) - x_t) \rangle_W}  
\end{align}
with $V(x(\tau) = {k^2 x^2(\tau)}/({4D})$. The average is computed with the Wiener measure:
\begin{align*}
    \langle f(x(\tau)) \rangle_W \equiv \int_{\mathbb{R}^T} \left(\prod_{\tau=0^+}^t \frac{\dd{x(\tau)}}{\sqrt{4 \pi D \dd{\tau}}} \right) \exp\left(-\frac{1}{4D} \int_0^t \dot{x}^2 (\tau) \dd{\tau} \right) f(x(\tau))
\end{align*}
with $\mathbb{R}^T$ being the set of continuous functions $T \to \mathbb{R}$, and $T=[0,t]$.
\medskip

We want now to show that, in the more general case of a particle immersed in a generic potential $U(x)$, a path integral similar to the highlighted term in (\ref{eqn:harmonic-tprob}) will appear. Of course, the function $V(x(\tau))$ will be different, but it will be proportional to $U(x)$ - as it is evident in the harmonic case.

\medskip

So, let's consider a particle in a 3D space $\bm{r} = (x_1, x_2, x_3)^T$, immersed in a conservative force-field $\bm{F}(\bm{r}) = -\bm{\nabla} U(\bm{r})$ with potential $U(\bm{r})$, and subject to thermal noise. The Langevin equation becomes:
\begin{align}\label{eqn:langevin-d3}
    \dd{\bm{r}} = \bm{f}(\bm{r})  \dd{t} + \sqrt{2D} \dd{\bm{B}} \qquad \bm{f}(\bm{r}) = \frac{\bm{F}(\bm{r})}{\gamma} \quad \gamma= 6 \pi \eta a   
\end{align}     
with $\bm{B}= (B_1, B_2 , B_3 )^T$ being a $d=3$ vector with gaussian components:
\begin{align}
    \Delta B_\alpha \sim \frac{1}{\sqrt{2 \pi \Delta t} } \exp\left(-\frac{\Delta B^2_\alpha}{2 \Delta t} \right) \qquad \alpha =1,2,3
    \label{eqn:deltaba}
\end{align}
As different components are independent, the joint pdf for the vector $\bm{\Delta B}$ is just the product of the three terms in (\ref{eqn:deltaba}):
\begin{align*}
    \bm{\Delta B} \sim \frac{1}{(2 \pi \Delta t)^{3/2}} \exp\left(-\frac{\norm{\Delta \bm{B}}^2}{2 \Delta t} \right) 
\end{align*}  
As before, we introduce a time discretization $\{t_j\}_{j=0,\dots,n}$ with $t_0 \equiv 0$ and $t_n \equiv t$ fixed, so that (\ref{eqn:langevin-d3}) becomes:
\begin{align*}
    \bm{\Delta r_i} = \bm{r} (t_i) - \bm{r}(t_{i-1}) = \bm{f_{i-1}} \Delta t_i + \sqrt{2D} \bm{\Delta B_i} 
\end{align*}
where the force $\bm{f}(\bm{r})$ is evaluated at the left side $t_{i-1}$ of each discrete interval $[t_{i-1}, t_i]$, following Ito's prescription.

Then, starting from the joint pdf of the $\{\bm{\Delta B_i}\}$:
\begin{align*}
    \dd{P}(\Delta \bm{B}_1, \dots, \Delta \bm{B}_n) &= \prod_{i=1}^n \frac{\dd[3]{\bm{\Delta B_i}}}{(2 \pi \Delta t)^{3/2}} \exp\left(-\sum_{i=1}^n \frac{\norm{\bm{ \Delta B_i}}^2}{2 \Delta t_i} \right)
\end{align*}
we perform a change of variables by inverting (\ref{eqn:langevin-d3}):
\begin{align*}
    \bm{\Delta B_i} = \frac{\bm{\Delta r_i} - \bm{f_{i-1}} \Delta t_i}{\sqrt{2 D}} \Rightarrow \left|\pdv{\{{\Delta B_i^\alpha}\}}{\{\Delta r_j^\beta\}}  \right| = \left|\pdv{\{\Delta r_j^\beta\}}{\{{\Delta B_i^\alpha}\}} \right|^{-1} = \textcolor{Red}{(2D)^{3/2}}
\end{align*}
This leads to the joint pdf for the increments $\{\bm{\Delta r_i}\}$:
\begin{align}
    \dd{P}(\bm{\Delta r_1}, \dots, \bm{\Delta r_n}) &= \left(\prod_{i=1}^n \frac{\dd[3]{\bm{ \Delta r_i}}}{(\textcolor{Red}{4} \pi \textcolor{Red}{D} \Delta t_i)^{3/2}}\right) \exp\left[-\frac{1}{4D} \sum_{i=1}^n \frac{\norm{\bm{ \Delta r_i} - \bm{f_{i-1}} \Delta t_i}^2}{\Delta t_i}  \right] 
    \label{eqn:dP3}
\end{align}

\begin{comment}
(Note that, in the textbook, instead of $\bm{f}_{i-1}$ they are using the Stratonovich prescription $(\bm{f}_i + \bm{f}_{i-1})/2$, complicating the jacobian for a change of variables, while we are using Ito's. In the end, however, the final result will not depend on this choice - at least for this case).
\end{comment}

Expanding the square in the exponential:
\begin{align*}
    -\frac{1}{4D} \sum_{i=1}^n \left[\frac{\norm{\bm{\Delta r_i}}^2}{\Delta t_i} + \norm{\bm{f_{i-1}}}^2 \Delta t_i - 2 \bm{\Delta r_i} \cdot \bm{f}_{i-1} \right]
\end{align*}
allows to recognize the $d=3$ Wiener measure in (\ref{eqn:dP3}):
\begin{align} \nonumber
    \dd{P}(\{\bm{\Delta r_i}\}) &= \Bigg(\underbrace{\prod_{i=1}^n \frac{\dd[3]{\bm{\Delta r_i}}}{(4 \pi D \Delta t_i)^{3/2}} \exp\left[-\frac{1}{4D} \sum_{i=1}^n \frac{\norm{\bm{\Delta r_i}}^2}{\Delta t_i}  \right] }_{\dd[3]{_W \bm{r}}}\Bigg) \cdot\\
    &\quad \> \cdot \label{eqn:dP-int3}
    \exp\Bigg(-\frac{1}{4D} \underbrace{\sum_{i=1}^n \norm{\bm{f_{i-1}}}^2 \Delta t_i}_{\displaystyle\int_0^t \norm{\bm{f}(\bm{r}(\tau))}^2 \dd{\tau} }  + \frac{1}{2D} \underbrace{ \sum_{i=1}^n \bm{f_{i-1}} \cdot \bm{\Delta r_i}}_{\displaystyle \int_0^t \bm{f}(\bm{r}(\tau)) \cdot \dd{\bm{r}(\tau)}}  \Bigg)
\end{align}

Let's focus on the \textit{stochastic integral} (the one in $\dd{\bm{r}(\tau)}$). For this we need to generalize to $d=3$ the integration formula we found in the previous section.

Consider a multi-variable scalar function $h(\bm{r}) \colon \mathbb{R}^3 \to \mathbb{R}$, $\bm{r} \mapsto h(\bm{r})$. As before, we start from the difference:
\begin{align} \nonumber
    h(\bm{r}_n) - h(\bm{r}_0) &= h(\bm{r}_n) - h(\bm{r}_{n-1}) + h(\bm{r}_{n-1}) -h(\bm{r}_{n-2}) + \dots + h(\bm{r}_1) - h(\bm{r}_0) =\\
    &= \sum_{i=1}^n (h(\bm{r}_i) - h(\bm{r}_{i-1})) = \sum_{i=1}^n \Delta h_i \label{eqn:increments3} 
\end{align}
In the discretization, $\bm{r}_i = \bm{r}_{i-1} + \bm{\Delta x}$, with $\bm{\Delta x} = (\Delta x_i^1, \Delta x_i^2, \Delta x_i^3)$. Each differential $\Delta h_i$ is then:
\begin{align}\nonumber
    \Delta h_i &= h(\bm{r}_i) - h(\bm{r}_{i-1}) \equiv h_i - h_{i-1} =\\
    \nonumber&\underset{(a)}{=} \cancel{h(\bm{r}_{i-1})} + \sum_{\alpha =1}^3 \left[\pdv{x^\alpha}h(\bm{r}_{i-1})\right] \Delta x_i^\alpha + \hlc{Yellow}{\frac{1}{2}} \sum_{\alpha, \beta = 1}^3 \left[\pdv[2]{}{x^\alpha}{x^\beta}h(\bm{r}_{i-1})\right] \hlc{Yellow}{\Delta x_i^\alpha \Delta x_i^\beta }+ \dots - \cancel{h(\bm{r}_{i-1})} =\\
    &\underset{(b)}{=}  \sum_{\alpha =1}^3 \left[\pdv{x^\alpha}h(\bm{r}_{i-1})\right] \Delta x_i^\alpha + \hlc{Yellow}{D} \sum_{\alpha, \beta = 1}^3 \left[\pdv[2]{}{x^\alpha}{x^\beta}h(\bm{r}_{i-1})\right] \hlc{Yellow}{\Delta t}_i 
    \label{eqn:increments4}
\end{align}
where in (a) we expanded the first term in Taylor series about $\bm{r}_{i-1}$, and in (b) we used Ito's rules, and in particular the fact that:
\begin{align*}
    \Delta x_i^\alpha \Delta x_i^\beta = \Delta t_i 2 D \delta_{\alpha \beta}
\end{align*}
Substituting (\ref{eqn:increments4}) back in (\ref{eqn:increments3}) leads to:
\begin{align*}
    h(\bm{r}_n) - h(\bm{r}_0) = \sum_{i=1}^n \Delta h_i = \sum_{i=1}^n\sum_{\alpha = 1}^3  \pdv{x^\alpha} h_{i-1} \Delta x_i^\alpha + D \sum_{\alpha =1}^3 \pdv[2]{{x^\alpha}} h_{i-1} \Delta t_i
 \end{align*}
 and then, in the continuum limit:
 \begin{align*}
     h(\bm{r}(t))- h(\bm{r}(0)) = \int_0^t \bm{\nabla} h(\bm{r}) \cdot \dd[3]{\bm{r}} + D \int_0^t \nabla^2 h(\bm{r}(\tau)) \dd{\tau}
 \end{align*}
 Rearranging we arrive at the desired formula for integration:
 \begin{align}
     \int_0^t \bm{\nabla} h(\bm{r}) \cdot \dd[3]{\bm{r}} = h(\bm{r}(t)) - h(\bm{r}(0)) - D\int_0^t \nabla^2 h(\bm{r}(\tau)) \dd{\tau}
     \label{eqn:multivariate-integration}
 \end{align}

 Thanks to (\ref{eqn:multivariate-integration}) we can solve the stochastic integral in (\ref{eqn:dP-int3}):
 \begin{align*}
     \int_0^t \bm{f}(\bm{r}(\tau)) \cdot \dd[3]{\bm{r}(\tau)}
 \end{align*}
 Inserting $\bm{f}(\bm{r}) = -\bm{\nabla} U(\bm{r})/\gamma$ and applying the formula leads to:
 \begin{align*}
     \int_0^t \bm{f}(\bm{r}(\tau)) \cdot \dd[3]{\bm{r}(\tau)} = -\frac{1}{\gamma} \int_0^t \bm{\nabla} U(\bm{r}(\tau)) \cdot \dd[3]{\bm{r}(\tau)}  = -\frac{1}{\gamma} \left[U(\bm{r}(t)) - U(\bm{r}(0)) - D \int_0^t \nabla^2 U(\bm{r}(\tau)) \dd{\tau}\right] 
 \end{align*} 
 Substituting back in (\ref{eqn:dP-int3}):
\begin{align} \nonumber
    \dd{P} &= \dd[3]{_W \bm{r}} \exp\left(-\frac{1}{4D} \int_0^t \norm{\bm{f}}^2 \dd{\tau} + \frac{1}{2D} \left[-\frac{1}{\gamma} [U(\bm{r}(t)) - U(\bm{r}(0))] + \frac{D}{\gamma} \int_0^t \nabla^2 U(\bm{r}(\tau)) \dd{\tau} \right]  \right) =\\
    &= \dd[3]{_W \bm{r}} \exp \Bigg(-\frac{1}{4D} \int_0^t \dd{\tau} \underbrace{\left[\norm{\bm{f}}^2 - \frac{2D}{\gamma} \nabla^2 U \right]}_{V(\bm{r})}  \Bigg) \exp\left(-\frac{1}{2 D \gamma} [U(\bm{r}(t)) - U(\bm{r}(0))] \right)
    \label{eqn:dP-potential}
\end{align}
where:
 \begin{align*}
     V= \bm{f}^2 - \frac{2D}{\gamma}  \nabla^2 U = \bm{f}^2 - 2D \bm{\nabla} \cdot \bm{f}
 \end{align*}
 
Using the just found measure $\dd{P}$ we can compute path integrals, and in particular \textit{transition probabilities}: 
 \begin{align*}
     W(\bm{r},t | \bm{r}_0, 0) &= \int_{\mathbb{R}^T} \dd{P} \delta(\bm{r}(t) - \bm{r}) \equiv \langle \delta(\bm{r}(t)- \bm{r}(0)) \rangle_W = \\
     &= \int_{\mathbb{R}^T} \dd[3]{_W \bm{r}} \exp\left(-\frac{1}{4D} \int_0^t V(\bm{r}(\tau)) \dd{\tau} \right) \delta(\bm{r}(t) - \bm{r}) \exp\left(-\frac{1}{2D \gamma} (U(\bm{r}) - U(\bm{r}_0))\right) =\\
     &= \langle \exp\left(-\frac{1}{4D} \int_0^t V(\bm{r}(\tau)) \dd{\tau} \right)  \delta (\bm{r}(t) - \bm{r})\rangle_W \exp\left(-\frac{1}{2D \gamma} (U(\bm{r})- U(\bm{r}_0))\right)
 \end{align*}
 This expression is indeed similar to that derived in the specific case of the harmonic oscillator (\ref{eqn:harmonic-tprob}), meaning that the techniques we used to evaluate previous path integrals can be useful in much more general cases.

 \medskip

 This observation has indeed a deeper meaning, as we found a way to describe the dynamics of conservative systems with a path integral. We already know that the behaviour of these systems can be also described with partial differential equations (e.g. the Fokker-Planck equation). So, there should be a link between path integrals and PDEs, that will be explored in the next section.

\section{Feynman-Kac formula}
It is possible to use the machinery of stochastic processes and path integrals to solve certain partial differential equations, which - as we will see - are of fundamental importance in Quantum Mechanics.

In this regard, a very important result is offered by the \textbf{Feynman-Kac formula}. The main idea is to use a Brownian process to \textit{simulate many paths}, and express the solution of the differential equation as the \textit{average} of a certain functional computed over all these paths. 

More precisely, consider the following \textit{partial differential equation} (Bloch's equation): 
\begin{align}
    \partial_t W_B(x,t) = D \partial_x^2 W_B(x,t) - V(x) W_B(x,t) \qquad D \in \mathbb{R}, V\colon \mathbb{R} \to \mathbb{R}  
    \label{eqn:bloch}
\end{align}
The \textbf{Feynman-Kac} formula states that the function $W_B(x,t)$ that solves (\ref{eqn:bloch}) can be found by computing a Wiener path integral: 
\begin{align}
    W_B(x,t) \equiv \langle \exp\left(-\int_0^t V(x(\tau)) \dd{\tau}\right) \delta(x(t) - x) \rangle_W
    \label{eqn:Wb}
\end{align}

Note that this result can be generalized to more dimensions - but we will limit ourselves to the $d=1$ case for simplicity.

\medskip

\textbf{Proof}. We now show that (\ref{eqn:Wb}) indeed satisfies (\ref{eqn:bloch}). As usual, we start by defining a time discretization, $\{t_i\}_{i=0,\dots,n+1}$, so that $t_0 \equiv 0$ and $t_{n+1} \equiv \bar{t}$ is the instant at which we wish to evaluate the solution $W_B(x,t)$. Then (\ref{eqn:Wb}) at that instant will be obtained by the \textit{continuum limit} of the discretized average $\psi_{n+1}(x)$:
\begin{align}
    W_B(x, \bar{t}) &= \lim_{n \to \infty} \psi_{n+1}(x) \nonumber\\
    \psi_{n+1}(x) &= \int_{\mathbb{R}^{n+1}} \left(\prod_{i=1}^{n+1} \frac{\dd{x_i}}{\sqrt{4 \pi D \Delta t_i} }\right) \exp\left(-\sum_{i=1}^{n+1} \frac{(x_i - x_{i-1})^2}{4 D \Delta t_i} - \sum_{i=1}^{n+1} \Delta t_i V(x_i)\right) \delta(x_{n+1}-x)
     \label{eqn:psix}
\end{align} 
Note that $\psi_{n+1}(x)$ is the average of a functional over \textit{all paths} that arrive in $x$ at the instant $\bar{t}$, making exactly $n+1$ steps from their starting point $0$. In the following, the intuition is to see these paths as \textit{being generated}, i.e. evolving \textit{step} after \textit{step} from $0$ to $x$. For example, suppose we want to approximate (\ref{eqn:psix}). We would start by choosing an \textit{ensemble} of paths arriving to $x$ after $n+1$ timesteps, compute the functional on each of them, and average the results.
However, we could also do it in another  - a bit stranger - way. Consider the same ensemble of paths we already (supposedly) generated. From each of them, remove the last step. We now have a set of paths that arrive \textit{close} to $x$, and will arrive exactly there if we let another timestep pass. However, we decide to compute the functional on each of these paths and then average the results, before letting them arrive at their destination. So, in a certain sense, we will estimate the value of the functional \q{a timestep in the past}. Of course, we can repeat this process, removing more and more timesteps at every iteration. At the end, we will have a sequence of numbers detailing the \q{evolution} of the functional from the start to the end. Turns out that the \textit{rule} for such an evolution is exactly (\ref{eqn:bloch}). So, to prove Feynman-Kac, we just have to find that rule - meaning how to relate $\psi_{n+1}$ to its \q{past} $\psi_n$ (in the continuum limit $n \to\infty$).

This is just an informal intuition, that will only be useful as a guide for the rest of the proof. So, let's go on.

\medskip

For simplicity, we choose the time discretization as \textbf{uniform}, so that $\Delta t_i \equiv \epsilon$ $\forall i$, and:
\begin{align*}
    t_{n+1} = (n+1) \epsilon \equiv \bar{t} \Rightarrow \epsilon = \frac{\bar{t}}{n+1}   
\end{align*}  
We then rewrite (\ref{eqn:psix}), highlight the last term (the one with the $x_{n+1}$) and integrate to remove the $\delta$:
\begin{align} \nonumber
    \psi_{n+1}(x) &= \int_{\mathbb{R}^n} \left(\prod_{i=1}^{n} \frac{\dd{x_i}}{\sqrt{4 \pi D \epsilon}} \right) \exp\left(-\sum_{i=1}^n \frac{(x_i - x_{i-1})^2}{4 D \epsilon} - \sum_{i=1}^{n} \epsilon V(x_i)\right) \cdot \\ \nonumber
    &\quad \> \cdot \int_{\mathbb{R}} \frac{\dd{x_{n+1}}}{\sqrt{4 \pi D \epsilon }} \exp\left(-\frac{(x_{n+1} - x_n)^2}{4 D \epsilon} - \epsilon V(x_{n+1})   \right) \delta(x_{n+1} - x) =\\ \nonumber
    &= \int_{\mathbb{R}^n} \left(\prod_{i=1}^{n} \frac{\dd{x_i}}{\sqrt{4 \pi D \epsilon}} \right) \exp\left(-\sum_{i=1}^n \frac{(x_i - x_{i-1})^2}{4 D \epsilon} - \sum_{i=1}^{n} \epsilon V(x_i)\right) \cdot \\ 
    &\quad \> \cdot \frac{1}{\sqrt{4 \pi D \epsilon }} \exp\left(-\frac{(x - x_n)^2}{4 D \epsilon} - \epsilon V(x)   \right) \label{eqn:psix1}
\end{align}
Now, with some algebra, we recognize in these integrals a term $\psi_n(x_n)$, indicating the expected value of the functional over all paths reaching $x_n$ (which is close to the end-point $x$) at timestep $t_n = t - \epsilon$. We start by rearranging, putting the integration over $\dd{x_n}$ at the front:
\begin{align} \nonumber
    (\ref{eqn:psix1}) &= \int_{\mathbb{R}} \frac{\dd{x_n}}{\sqrt{4 \pi D \epsilon} } \exp\left(-\frac{(x- x_n)^2}{4 D \epsilon} - \epsilon V(x) \right) \cdot \\
    &\quad \> \cdot \frac{1}{\sqrt{4 \pi D \epsilon}}  \int_{\mathbb{R}^{n-1}} \left(\prod_{i=1}^{n-1} \frac{\dd{x_i}}{\sqrt{4 \pi D \epsilon}} \right) \exp\left(-\sum_{i=1}^{\textcolor{Red}{n}} \frac{(x_i - x_{i-1})^2}{4 D \epsilon}  - \epsilon\sum_{i=1}^{\textcolor{Red}{n}} V(x_i) \right)
    \label{eqn:psix2}
\end{align}
Now we \textit{change} all the $x_i$ in the second line to $y_i$, and then add a $\delta$ (with its integral) to \textit{connect} $y_n$ to $x_n$, which appears in the integral in the first line. In this way we will highlight the desired $\psi_n(x_n)$:
\begin{align}
    (\ref{eqn:psix2}) \nonumber
 &= \int_{\mathbb{R}} \frac{\dd{x_n}}{\sqrt{4 \pi D \epsilon} } \exp\left(-\frac{(x- x_n)^2}{4 D \epsilon} - \epsilon V(x) \right) \cdot \\
    &\quad \> \cdot \underbrace{\int_{\mathbb{R}^{n}} \left(\prod_{i=1}^{\textcolor{Red}{n}} \frac{\dd{y_i}}{\sqrt{4 \pi D \epsilon}} \right) \exp\left(-\sum_{i=1}^{\textcolor{Red}{n}} \frac{(y_i - y_{i-1})^2}{4 D \epsilon}  - \epsilon\sum_{i=1}^{\textcolor{Red}{n}} V(y_i) \right) \delta(x_n - y_n)}_{\psi_n(x_n)} 
    \label{eqn:psix3}
\end{align} 
And so:
\begin{align}
    \psi_{n+1}(x) = e^{-\epsilon V(x)} \int_{\mathbb{R}} \frac{\dd{x_n}}{\sqrt{4 \pi D \epsilon }} \exp\left(-\frac{(x - x_n)^2}{4 D \epsilon} \right) \psi_n(x_n) 
    \label{eqn:psix4}
\end{align}
This is relation between $\psi_{n+1}(x)$ and its \q{past} $\psi_n(x)$ that we were searching for. Now, to retrieve (\ref{eqn:bloch}), all that's left is to put (\ref{eqn:psix4}) in differential form. 

We start by simplifying the integral, making it similar to a gaussian with a change of variables:
\begin{align*}
    - \frac{(x- x_n)^2}{4 D \epsilon} \overset{!}{=} - \frac{z^2}{2} \Rightarrow z = - \frac{x - x_n}{\sqrt{2 D \epsilon}}; \quad x_n = x + \sqrt{2 D \epsilon}z; \quad \dd{x_n} = \sqrt{2 D \epsilon} \dd{z}  
\end{align*}
which leads to:
\begin{align}
    \psi_{n+1}(x) = e^{-\epsilon V(x)} \int_{-\infty}^{+\infty} \frac{\dd{z}}{\sqrt{2 \pi}} \exp\left(-\frac{z^2}{2} \right) \psi_n(x_n + z \sqrt{2 D \epsilon})
    \label{eqn:psix5}
\end{align}
As $n \to \infty$, $z \to 0$. We will not prove this, but note that:
\begin{align*}
    z = - \frac{x-x_n}{\epsilon} \frac{\sqrt{\epsilon}}{\sqrt{2 D}}  
\end{align*}
where the first factor is a \textit{velocity}, which must be physically limited, and so $n \to \infty \Rightarrow \epsilon \to 0 \Rightarrow z \to 0$.

This means that we can expand $\psi_n$ in Taylor series about $x$:
\begin{align*}
    \psi_n(x_n + z \sqrt{2 D \epsilon}) = \psi_n(x) + z \sqrt{2 D \epsilon} \psi_n'(x) + z^2 D \epsilon\psi_n''(x) + O(z^3 \epsilon^{3/2})  
\end{align*}
Substituting back in (\ref{eqn:psix5}):
\begin{align} \nonumber
    \psi_{n+1}(x) &= e^{-\epsilon V(x)} \Bigg[ \psi_n(x) \underbrace{\int_{\mathbb{R}} \frac{\dd{z}}{\sqrt{2 \pi}} \exp\left(-\frac{z^2}{2} \right)}_{1} + \sqrt{2 D \epsilon} \psi_n'(x) \underbrace{\int_{\mathbb{R}} \frac{\dd{z}}{\sqrt{2 \pi}} z\exp\left(-\frac{z^2}{2} \right)}_{0} + \\
    &\quad  \>
    + D \epsilon \psi_n''(x) \underbrace{\int_{\mathbb{R}} \frac{\dd{z}}{\sqrt{2 \pi}} z^2 \exp\left(-\frac{z^2}{2} \right)}_{1}
    + O(\epsilon^2) \Bigg] \label{eqn:psix6}
\end{align}
as the integrand is just a standard gaussian ($\mu= 0$, $\sigma = 1$). Note how the error term is of order $e^2$, as the first non-null integral in the series will be that with $z^4$:
\begin{align*}
    \int_{\mathbb{R}} \frac{\dd{z}}{\sqrt{2 \pi}} z^k \frac{({2 D \epsilon})^{k/2}}{k!} \psi_n^{(k)} = \begin{cases}
        0 & \text{$k$ odd}\\
        1^k (k-1)!! & \text{$k$ even}
    \end{cases} 
\end{align*}
Expanding also the $e^{-\epsilon V(x)}$ term:
\begin{align*}
    e^{-\epsilon V(x)} = 1 - \epsilon V(x) + \frac{\epsilon^2 V^2(x)}{2} + O(\epsilon^3) 
\end{align*}
Finally, substituting back in (\ref{eqn:psix6}), expanding the product and ignoring all terms of order $2$ or higher in $\epsilon$:
\begin{align*}
    \psi_{n+1}(x) = \psi_n(x) + D \epsilon \psi_n''(x) - \epsilon V(x) \psi_n(x) + O(\epsilon^2)
\end{align*}
Rearranging:
\begin{align*}
    \frac{\psi_{n+1} - \psi_n}{\epsilon} = D \psi_n'' - V \psi_n 
\end{align*}

And when $\epsilon \to 0$ the first term becomes a time derivative, leading to Bloch's equation, and proving Feynman-Kac formula:
\begin{align*}
    \partial_t W_B(x,t) = D \partial_x^2 W_B(x,t) - V(x) W_B(x,t)
\end{align*} 

\subsection{Application to Quantum Mechanics}
It is possible to map the Schr\"odinger equation to the Bloch equation (\ref{eqn:bloch}), and then use Feynman-Kac formula to solve it.

Recall the time-dependent Schr\"odinger equation for a particle immersed in a $d=1$ potential $v(x)$ and described by a wavefunction $\psi(x,t)$:
    \begin{align*}
        i\hbar \partial_t \psi(x,t) = -\frac{\hbar^2}{2m} \partial_x^2 \psi(x,t) + v(x) \psi(x,t) 
    \end{align*}
This is already similar to (\ref{eqn:bloch}), except for the presence of a complex coefficient $i$. We can remove it with a change of variable $t \mapsto it$, leading to:
\begin{align*}
    i \hbar (i) \pdv{t} \psi(x,it) = - \hbar \pdv{t} \psi(x,it) = - \frac{\hbar^2}{2m} \partial_x^2 \psi(x,it) + v(x) \psi(x,it) 
\end{align*}
Defining $\psi(x,it) \equiv \hat{\psi}(x,t)$ and multiplying both sides by $-\hbar^{-1}$ leads to:
\begin{align*}
    \pdv{t} \hat{\psi}(x,t) = \frac{\hbar}{2 m} \pdv[2]{x} \hat{\psi}(x,t)  - \frac{v(x)}{\hbar} \hat{\psi}(x,t) 
\end{align*}
which has the form of Bloch's equation:
\begin{align*}
    \pdv{t} \hat{\psi}(x,t) = D \pdv[2]{x} \hat{\psi}(x,t) - V(x) \hat{\psi}(x,t) \qquad D = \frac{\hbar}{2m}; \quad V(x) = \frac{v(x)}{\hbar} 
\end{align*}

\section{Variational methods}
Consider a particle subject to an external conservative force $\bm{F}(\bm{r}) = -\bm{\nabla} U(\bm{r})$, moving through a viscous medium and subject to thermal noise. The probability density for a path $x(\tau)$ can be derived from (\ref{eqn:dP-potential}), after \q{dividing by the volume element} and taking the limit $n \to \infty$:
\begin{align}\nonumber
    \Omega[\bm{r}(\tau)] &\equiv \frac{\dd{P}}{\dd{V}} =\\ \nonumber
    &= \exp \Bigg(-\frac{1}{4D}\int_0^t \dd{\tau} \dot{\bm{r}}^2(\tau)  + \int_0^t \dd{\tau} \underbrace{\left(-\frac{1}{4D} \right)\left[\norm{\bm{f}}^2 - \frac{2D}{\gamma} \nabla^2 U \right]}_{V(\bm{r})} -\frac{1}{2 D \gamma} [U(\bm{r}(t)) - U(\bm{r}(0))] \Bigg) =\\
    &=\exp \Bigg(-\frac{1}{4D}\int_0^t \dd{\tau} \dot{\bm{r}}^2(\tau)  -\frac{1}{4D} \int_0^t \dd{\tau} \underbrace{\left[\norm{\frac{\bm{F}}{\gamma} }^2 + \frac{2D}{\gamma} \bm{\nabla}\cdot \bm{F} \right]}_{V(\bm{r})} +\frac{1}{2 D \gamma} \int_{\bm{r}(0)}^{\bm{r}(t)} \dd[3]{\bm{r}} \cdot \bm{F}(\bm{r}) \Bigg)
    \label{eqn:omega1}
\end{align}
with $\bm{f}(\bm{r}) = \bm{F}/\gamma$, and $\gamma = 6 \pi \eta a$ ($\eta$ being the medium viscosity, and $a$ the particle radius). If we change variables in the last integral:
\begin{align*}
    \int_{\bm{r}(0)}^{\bm{r}(t)} \dd[3]{\bm{r}} \cdot \bm{F}(\bm{r}) = \int_{0}^t \dd{\tau} \bm{F}(\bm{r}) \cdot \dv{\bm{r}(\tau)}{\tau}
\end{align*}
we can rewrite (\ref{eqn:omega1}) as a single integral:
\begin{align}\label{eqn:omega2}
    \Omega[\bm{r}(\tau)] = \exp\left(-\frac{1}{4D} \int_0^t \dd{\tau} L(\bm{r}(\tau)) \right)
\end{align}
with the function $L\colon \mathbb{R} \to \mathbb{R}$, $\bm{r}(\tau) \mapsto L(\bm{r}(\tau))$ defined as:
\begin{align}\nonumber
    L(\bm{r}(\tau)) &\equiv \bm{r}^2(\tau) + \norm{\frac{\bm{F}(\bm{r}(\tau))}{\gamma}}^2 + \frac{2D}{\gamma} \bm{\nabla}\cdot \bm{F}(\bm{r}(\tau)) - \frac{2}{\gamma} \bm{F}(\bm{r}(\tau)) \cdot \dot{\bm{r}}(\tau) =\\
    &= \norm{\dot{\bm{r}}(\tau) - \frac{\bm{F}(\bm{r}(\tau))}{\gamma} }^2 + \frac{2D}{\gamma} \bm{\nabla}\cdot \bm{F}(\bm{r}(\tau)) \label{eqn:Lagrangian}
\end{align}

\begin{expl} \textbf{Classical path}.

Consider the \textbf{classical limit} $D \to 0$. Then the $\bm{\nabla}\cdot \bm{F}$ term in (\ref{eqn:Lagrangian}) vanishes, and as $-1/(4D) \to \infty$, there will be only \textit{one path} with \textit{non-zero} probability, i.e. the one $\bm{r_c}(\tau)$ for which the functional vanishes:
\begin{align*}
    \int_0^t \dd{\tau} L(\bm{r_c}(\tau)) = 0 \Rightarrow \int_0^t  \dd{\tau}  \norm{\dot{\bm{r}}_c(\tau) - \frac{\bm{F}(\bm{r}_c(\tau))}{\gamma} }^2 = 0
\end{align*}
We can then compute that path. As the integrand is a non-negative function, for its integral to be $0$ it must be $0$ $\forall t$, leading to:
\begin{align*}
    \dv{\bm{r_c}}{\tau} = \frac{\bm{F}(\bm{r_c})}{\gamma} 
\end{align*}
which is just the equation of motion from classical mechanics.
\end{expl}

\medskip

Let's now use the form (\ref{eqn:omega2}) to compute path integrals. For example,
 consider a transition probability:
\begin{align}\label{eqn:tproba}
    W(\bm{r}_t, t|\bm{r}_0, 0) = \int_{\mathcal{C}\{\bm{r}_0,0;\bm{r}_t,t\}} \dd{\bm{r}(\tau)} \underbrace{\exp\left(-\frac{1}{4D} \int_0^t \dd{\tau} L(\bm{r}(\tau)) \right) }_{\Omega[\bm{r}(\tau)]}
\end{align}
Let's define the functional:
\begin{align*}
    S[\bm{r}(\tau)] = \int_0^t \dd{\tau} L(\bm{r}(\tau))
\end{align*}
Given the form of (\ref{eqn:tproba}), the path $\bm{r_c}(\tau)$ that \textit{minimizes} $S[\bm{r}(\tau)]$ will give the greatest contribution to the path integral. The parameter $D$ \textit{modulates} the relative contributions of paths. If $D \to 0$, $\bm{r}_c(\tau)$ will be the only contributing path, but if $D \gg 1$, many different paths will have a significant contribution. Suppose that $\bm{r_c}(\tau)$ is indeed important, meaning that $D$ is sufficiently small (more precisely, that $S[\bm{r}_c(\tau)])/D \gg 1$). Then, we write any generic path $\bm{x}(\tau)$ as the most important one $\bm{x_c}(\tau)$ plus a \q{deviation} $\bm{y}(\tau)$:
\begin{align*}
    \bm{x}(\tau) = \bm{x_c}(\tau) + \underbrace{(\bm{x}(\tau) - \bm{x_c}(\tau))}_{\bm{y}(\tau)} 
\end{align*}
Note that as the end-points of every path are fixed, $\bm{y}(0) = \bm{y}(t) = 0$. Then, we expand in series the functional:
\begin{align*}
    S[\bm{x}(\tau)] = S[\bm{x_c}(\tau) + y(\tau)] = S[\bm{x_c}] + \delta S[\bm{x_c}, \bm{y}] + \frac{1}{2!} \delta^2 S[\bm{x_c},\bm{y}] + \dots 
\end{align*}
where the $\delta$ terms are the \textit{variations} of the functional\footnote{See \url{www2.math.uconn.edu/~gordina/NelsonAaronHonorsThesis2012.pdf} for a refresher}. For example, the first variation $\delta S[\bm{x_c}, \bm{y}]$ is given by, measures how much $S$ \textit{varies} to first order when changing $\bm{y}(\tau)$. As $\bm{x_c}$ is a minimum of $S$, it is also a \textit{stationary point}, meaning that paths close to $\bm{x_c}$ do not change the value $S[\bm{x}_c]$ to first order. Applying the definition of the first variation, this leads to the Euler-Lagrange equations for determining $\bm{x_c}$:
\begin{align*}
    \delta S[\bm{x_c}, \bm{y}] = \pdv{L(\bm{r}(\tau))}{x_i} - \dv{\tau} \pdv{L(\bm{r}(\tau))}{\dot{x}_i} \overset{!}{=}  0 \qquad i=1,2,3
\end{align*}
Then, note how all other terms of the series involve integrals of $\bm{y}(\tau)$, which do not depend on $x$ - as $\bm{y}(\tau)$ starts from $0$ and returns to $0$ at $t$. So:
\begin{align*}
    S[\bm{x}(\tau)] = S[\bm{x_c}] + \underbrace{\frac{1}{2!} \delta^2 S[\bm{x_c},\bm{y}] + \dots}_{h(t)} 
\end{align*} 
Substituting back in (\ref{eqn:tproba}):
\begin{align*}
    W(\bm{r}_t, t|\bm{r}_0, 0) = \underbrace{-\frac{1}{4D} \exp(h(t))}_{\Phi(t)} \exp(-\frac{1}{4D} S[\bm{x_c}]) = \Phi(t) \exp\left(-\frac{1}{4D} \int_0^t \dd{\tau} L[\bm{r_c}(\tau)] \right)
\end{align*}
The function $\Phi(t)$ is called \textit{fluctuation factor}, and its computation is not trivial in the general case. However, if we are dealing with transition probabilities, we can use the \textit{normalization condition} to find it:
\begin{align*}
    \int_{\mathbb{R}^3} \dd[3]{\bm{r}} W(\bm{r},t|\bm{r},0) \equiv 1
\end{align*} 

\begin{example}[Simple integral with variational methods]
    An example will hopefully clarify the essence of the variational method. Let's start with a already known integral, in the $d=1$ case:
    \begin{align} \label{eqn:Wsolita}
        W(x,t|x_0,0) &= \int_{\mathbb{R}^T} \left(\prod_{\tau = 0^+}^t \frac{\dd{x(\tau)}}{\sqrt{4 \pi D \dd{\tau}}} \right) \exp\left(-\frac{1}{4D} \int_0^t \dot{x}^2(\tau) \dd{\tau} \right) \delta(x-x(t)) =\\\nonumber
        &= \frac{1}{\sqrt{4 \pi D t}} \exp\left(-\frac{(x-x_0)^2}{4 D t} \right) 
    \end{align}
    Let's compute it again, this time using variations.
    In this case we are interested in the functional:
    \begin{align}\label{eqn:S1}
        S[x(\tau)] = \int_0^t \dot{x}^2(\tau) \dd{\tau}
    \end{align}
    To minimize it, we solve the Euler-Lagrange equations:
    \begin{align*}
        \dv{\tau} \pdv{S(x_c)}{\dot{x}} - \pdv{S(x_c)}{x} = 0 \Rightarrow 0 - 2 \ddot{x_c} = 0 \Rightarrow \ddot{x}_c(\tau) = 0 
    \end{align*}
    Integrating two times:
    \begin{align*}
        \dot{x}_c(\tau) = a \Rightarrow x_c(\tau) = a \tau + b
    \end{align*}
    The boundary conditions are path's two extrema:
    \begin{align*}
        x_c(0) = b \overset{!}{=} 0; \quad x_c(t) = at + x_0 \overset{!}{=} x \Rightarrow a= \frac{x-x_0}{t} 
    \end{align*}
    leading to:
    \begin{align*}
        x_c(\tau) = x_0 + \frac{x-x_0}{t}\tau 
    \end{align*}
    So the path minimizing $S$ is just the straight line joining $x_0$ to $x$. We can now express any path $x(\tau)$ as a \textit{deviation} from the $x_c(\tau)$:
    \begin{align}
        x(\tau) = x_c(\tau) + y(\tau) \qquad y(0) = y(t) = 0 \label{eqn:xdef}
    \end{align}
    This is a change of variables for (\ref{eqn:Wsolita}), from $x(\tau)$ to $y(\tau)$ ($x_c(\tau)$ is a \textit{fixed} path). As this is just a translation, $\dd{x(\tau)} = \dd{y(\tau)}$ and the path integral becomes:
    \begin{align} \nonumber
        W(x,t|x_0,0) &= \int_{\mathbb{R}^T} \left(\prod_{\tau=0^+}^t \frac{\dd{y(\tau)}}{\sqrt{4 \pi D \dd{\tau} }} \right) \delta(y(t) - 0) S[x_c(\tau) + y(\tau)] =\\ \label{eqn:Wsolita1}
        &= \int_{\mathcal{C}\{0,0;0,t\}} \left(\prod_{\tau=0^+}^t \frac{\dd{y(\tau)}}{\sqrt{4 \pi D \dd{\tau} }} \right) S[x_c(\tau)+y(\tau)]
    \end{align}
    To compute $S$, first we differentiate (\ref{eqn:xdef}):
    \begin{align*}
        \dot{x}(\tau) = \dot{x}_c(\tau) + \dot{y}(\tau)
    \end{align*}
    and substitute in (\ref{eqn:S1}), leading to:
    \begin{align*}
        S[x(\tau)] = \int_0^t \dd{\tau} (\dot{x}_c + \dot{y})^2 = \int_0^t \dot{x}_c^2(\tau) \dd{\tau} + 2\int_0^t \dot{x}_c(\tau) \dot{y}(\tau)\dd{\tau} + \int_0^t \dot{y}^2(\tau) \dd{\tau}
    \end{align*}
    Note that the middle term vanishes. We can see it by integrating by parts:
    \begin{align*}
        \int_0^t \dot{x}_c(\tau) \dot{y}(\tau)\dd{\tau} = \dot{x}_c(\tau) y(\tau)\Big|_0^t - \int_0^t \ddot{x}_c(\tau) y(\tau) \dd{\tau} = 0
    \end{align*}
    as $y(0) = y(t) = 0$ and $\ddot{x}_c(\tau) \equiv 0$. Going back to (\ref{eqn:Wsolita1}):
    \begin{align*}
        W(x,t|x_0,0) &= \int_{\mathbb{R}^T} \left(\prod_{\tau=0^+}^t \frac{\dd{y(\tau)}}{\sqrt{4 \pi D \dd{\tau} }} \right) \delta(y(t) - 0) \exp\left(-\frac{1}{4D} \left[\int_0^t \dot{x}_c^2(\tau) \dd{\tau} + \int_0^t \dot{y}^2(\tau) \dd{\tau}\right] \right)
    \intertext{As $x_c(\tau)$ is fixed, we can bring it outside the integral:}
        &= \exp\left(-\int_0^t \dot{x}_c^2(\tau) \dd{\tau}\right)  \underbrace{\int_{\mathbb{R}^T} \left(\prod_{\tau=0^+}^t \frac{\dd{y(\tau)}}{\sqrt{4 \pi D \dd{\tau} }} \right) \delta(y(t) - 0) \exp \left(-\frac{1}{4D}  \int_0^t \dot{y}^2(\tau) \dd{\tau} \right)}_{\Phi(t)} 
    \end{align*}
    We recognize the remaining path integral as a function $\Phi(t)$ of time only, and finally:
    \begin{align*}
        W(x,t|x_0,0) &= \Phi(t) \int_0^t \dd{\tau} \dot{x}_c^2(\tau) = \Phi(t) \exp\left[-\frac{1}{4D} \left(\frac{x^2-x_0^2}{t^2} \right)\underbrace{\int_0^t \dd{\tau}}_{t}\right]  =\\
        &= \Phi(t) \exp\left(-\frac{(x-x_0)^2}{4Dt} \right) 
    \end{align*}
    To find the remaining $\Phi(t)$ we can now use the normalization condition:
    \begin{align*}
        \int_{-\infty}^{+\infty} \dd{x} W(x,t|x_0,0) \overset{!}{=}  1
    \end{align*}
    In this case, this is just a gaussian integral:
    \begin{align*}
        \int_{\mathbb{R}} \dd{x} \Phi(t) \exp\left(-\frac{(x-x_0)^2}{4 D t} \right) = \Phi(t) \sqrt{4 \pi D t} \overset{!}{=} 1 \Rightarrow \Phi(t) = \frac{1}{\sqrt{4 \pi D t}} 
    \end{align*}
    And so we retrieve the correct result:
    \begin{align*}
        W(x,t|x_0,0) = \frac{1}{\sqrt{4 \pi D t}} \exp\left(-\frac{(x-x_0)^2}{4Dt} \right)
    \end{align*}
\end{example}

\begin{expl}
\textbf{Gaussian integrals}. There is another, more specific, way to interpret the results we discussed in this section. Instead of working in the continuum, we could use a discretization, and see path integrals as (\ref{eqn:Wsolita}) as integrals of a highly dimensional gaussian. For example, in the case just examined, we have:
\begin{align*}
    W(x,t|x_0,0) &= ``\lim_{n \to\infty}" I_N\\
    I_n &= \int_{\mathbb{R}^n} \left(\prod_{i=1}^n \frac{\dd{x_i}}{\sqrt{4 \pi D \Delta t_i}} \right) \exp\left(-\sum_{i=1}^n \frac{(x_i - x_{i-1})^2}{4D \Delta t_i } \right) \delta (x-x_n)
\end{align*} 
Performing the integration over the $\dd{x_n}$ we can remove the $\delta$, leaving only a multivariate gaussian:
\begin{align*}
    I_n = \int_{\mathbb{R}^{n-1}} \frac{1}{\sqrt{4 \pi D \Delta t_i }} \left(\prod_{i=1}^n \frac{\dd{x_i}}{\sqrt{4 \pi D \Delta t_i}} \right) \exp\left(-\sum_{i=1}^n \frac{(x_i - x_{i-1})^2}{4 D \Delta t_i} \right) \Big|_{x_n = x}
\end{align*}
This is a gaussian in the form of:
\begin{align}
    \int_{\mathbb{R}^n} \dd[n]{\bm{x}} \exp\left(-\frac{1}{2} \bm{x}^T A \bm{x} + \bm{b}^T \bm{x} \right) \label{eqn:gausintstrano}
\end{align}
Note that removing the $\delta$ inserts a linear term in the exponential, here highlighted:
\begin{align*}
    \sum_{i=1}^{n} \frac{(x_i - x_{i-1})^2}{4 D \Delta t_i}\Big|_{x = x_n} = \sum_{i=1}^{n-1} \frac{(x_i - x_{i-1})^2}{4 D \Delta t_i} +  \frac{x_n^2 + \hlc{Yellow}{2x_n x_{n-1}} + x_{n-1}^2}{4 D \Delta t_n} 
\end{align*}
and so $\bm{b} \neq \bm{0}$.

Recall that to solve (\ref{eqn:gausintstrano}) we proceeded with a change of variables, $\bm{x} = \bm{x_c} + \bm{y}$, where $\bm{x_c}$ is the \textit{minimum} of the gaussian (see 10/10 notes). This leads to a result that is proportional to the exponential evaluated at $\bm{x_c}$:
\begin{align*}
    \int_{\mathbb{R}^n} \dd[n]{\bm{x}} \exp\left(-\frac{1}{2} \bm{x}^T A \bm{x} + \bm{b}^T \bm{x}  \right) &= \frac{(2 \pi)^{n/2}}{\sqrt{\operatorname{det}(A) }}\exp\left(\frac{1}{2} \bm{b}^T A^{-1} \bm{b} \right) =\\
    &= \frac{(2 \pi)^{n/2}}{\sqrt{\operatorname{det}(A) }} \exp\left(\operatorname{Stat}_{\bm{x}} \left[-\frac{1}{2} \bm{x}^T A \bm{x} + \bm{b} \cdot \bm{x} \right] \right) \\ \span
    \operatorname{Stat}_{\bm{x}} F(\bm{x}) = F(\bm{x}_c); \qquad \bm{x_c} \text{ such that } \pdv{F(\bm{x})}{x_i} \Big|_{\bm{x} = \bm{x}_c} = 0 \quad \forall i = 1,\dots, n
\end{align*}
So, in the discrete case, the same \textit{variational result} just derives from choosing the best set of coordinates to describe the multivariate gaussian. 

\end{expl}





 
\end{document}
