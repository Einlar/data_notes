%&latex
%
\documentclass[../template.tex]{subfiles}
\begin{document}


\chapter{MoTP Exercises 2019/20}
\begin{comment}
\section{Stochastic Processes and Path Integrals}

\begin{exo}[Stirling's approximation]
    Use the $\Gamma$ function definition:
    \begin{align}
        \Gamma(n) \equiv \int_0^\infty x^{n-1} e^{-x} \dd{x} \quad n > 0\qquad \Gamma(n+1) = n! \label{eqn:gamma-func}
    \end{align}
    together with the saddle point approximation to derive the result used in chapter $2$ of Lecture Notes:
    \begin{align}
        \ln n! = n\ln n - n + \frac{1}{2} \ln(2 \pi n) + O\left(\frac{1}{n} \right) \label{eqn:stirling}
    \end{align} 

    \medskip

    \textbf{Solution}. To use the saddle-point approximation we need to rewrite (\ref{eqn:gamma-func}) in the following form:
    \begin{align}
        I(\lambda) = \int_S \dd{x} \exp\left(-\frac{F(x)}{\lambda} \right)
    \end{align}
    So that:
    \begin{align}
        I(\lambda) \underset{\lambda \to 0}{\approx}  \sqrt{2 \pi \lambda} \left(\pdv{F}{x} (x) \Big|_{x=x_0}\right)^{-1/2} \exp\left(-\frac{F(x_0)}{\lambda} \right)
        \label{eqn:saddle-formula}
    \end{align}
    where $x_0$ is a global minimum of $F(x)$.  

    First, we evaluate $\Gamma$ at $n+1$, and express the integrand as a single exponential:
    \begin{align*}
        \Gamma(n+1) = n! = \int_0^{+\infty} \dd{x} x^n e^{-x}  = \int_{0}^{+\infty} \dd{x} e^{-x + n \log x} 
    \end{align*}
    We want to collect a $n$ in the exponential, and then define $\lambda = 1/n$, so that the saddle-point approximation $\lambda \to 0$ corresponds to the case of a large factorial $n \to \infty$. To do this, we perform a change of variables $x \mapsto s$, so that $x = n s$, with $\dd{x} = n \dd{s}$:
    \begin{align*}
        \Gamma(n+1) &= \int_0^{+\infty} \dd{s} n \exp(-ns + n \log(ns))= \\
        &= n^{n+1} \int_0^{+\infty} \dd{s} \exp(n[\log s - s])
    \end{align*}
    In the last step we split the logarithm $n\log(ns) = n\log n + n\log s = n^n + n\log s$, extracted from the integral all terms not depending on $s$, and then collected the $n$ as desired. Now, letting $\lambda = 1/n$ we have:
    \begin{align*}
        = n^{n+1} \int_0^{+\infty} \dd{s} \exp\left(\frac{\log s - s}{\lambda} \right)
    \end{align*}
    which is in the desired form (\ref{eqn:saddle}).

    So, we compute the minimum of $F(s) = \log s - s$:
    \begin{align*}
        F'(s) &= \dv{s} (s - \log s) = 1 - \frac{1}{s} \overset{!}{=} 0 \Rightarrow s_0 = 1\\
        F''(s) &= \frac{1}{s^2} \Rightarrow F''(s_0) = 1 > 0 
    \end{align*}
    And applying formula (\ref{eqn:saddle-formula}):
    \begin{align*}
        n! \underset{n \to \infty}{\approx} \sqrt{\frac{2\pi}{n} } \cdot 1 \cdot e^{-n} = \sqrt{2 \pi} n^{n+\frac{1}{2}} e^{-n}  
    \end{align*}
    Finally, taking the logarithm leads to the result (\ref{eqn:stirling}):
    \begin{align*}
        \log n! \underset{n \to \infty}{\approx} n \log n -n +\frac{1}{2} \log (2 \pi n) 
    \end{align*}
\end{exo}

\begin{exo}[Random walk tends to a Gaussian]
    Implement a numerical simulation to explicitly show how the solution of the ME for a 1-dimensional random walk with $p_\pm = 1/2$ tends to the Gaussian. 
\end{exo}

\begin{exo}[Non symmetrical motion]
    Write the analogous of:
    \begin{align}
        W(x,t+\epsilon) = \frac{1}{2}[W(x-l, t) + W(x+l,t)] 
        \label{eqn:ME}
    \end{align}
    in the LN for the case with $p_+ = 1-p_- \neq p_-$ and determine:
    \begin{enumerate}
        \item How they depend on $l$ and $\epsilon$ in order to have a meaningful continuum limit
        \item The resulting continuum equation and how to map it in the diffusion equation:
        \begin{align*}
            \partial_t W(x,t) = D \partial_x^2 W(x,t)
        \end{align*}  
    \end{enumerate} 

    \medskip

    \textbf{Solution}. Consider a Brownian particle moving on a uniform lattice $\{x_i = i \cdot l\}_{i \in \mathbb{N}}$, making exactly one \textit{step} at each \textit{discrete instant} $\{t_n = n \cdot \epsilon\}_{n \in \mathbb{N}}$, with $l, \epsilon \in \mathbb{R}$ fixed. Denoting with $p_+$ the probability of a \textit{step to the right}, and with $p_-$ that of a \textit{step to the left}, the Master Equation for the particle becomes:
    \begin{align} \label{eqn:ME2}
        W(x,t+\epsilon) = p_+ W(x-l,t) + p_- W(x+l,t)
    \end{align}  
    
    \begin{enumerate}
        \item We already derived (see 7/10) the expected position $n$ at timestep $n$ in that case:
        \begin{align}
            \langle x \rangle_{t_n} = n l (p_+ - p_-) = t\frac{l}{\epsilon} (p_+ - p_-) \label{eqn:pref-motion}
        \end{align}
        Intuitively, an unbalance $p_+ \neq p_-$ will result in a \textit{preferred motion} proportional to that unbalance. Thus we can rewrite (\ref{eqn:pref-motion}) as: 
        \begin{align*}
            \langle x \rangle_{t_n} = vt \qquad v = \frac{l}{\epsilon}(p_+ - p_-) 
        \end{align*}
        $v$ is the \textit{physical} parameter that needs to be fixed when performing the continuum limit. So, as $p_+ - p_- = 2p_+ -1$ by normalization, we can find the desired relation between $p_+$ and $v$:
        \begin{align*}
            (2p_+ - 1) \frac{l}{\epsilon} \equiv v \Rightarrow p_+ = \frac{1}{2} \left[\frac{v \epsilon}{l} + 1 \right]  
        \end{align*}
        As before, we also need to fix $l^2/(2\epsilon) \equiv D$.
        \item Expanding each term of (\ref{eqn:ME2}) in a Taylor series we get:
        \begin{align*}
            \cancel{W(x,t)} + \epsilon \dot{W}(x,t) + \frac{\epsilon^2}{2} \ddot{W}(x,t) + O(\epsilon^3) &= p_+ \left[\cancel{W(x,t)} + l W'(x,t) + \frac{1}{2} l^2 W''(x,t) + O(l^3) \right]\\
            &\> + p_- \left[\cancel{W(x,t)} - lW'(x,t) + \frac{1}{2} l^2 W''(x,t) + O(l^3)  \right]      
        \end{align*}
        Using the normalization $p_+ + p_- = 1$ and dividing by $\epsilon$ leads to:
        \begin{align*}
            \dot{W}(x,t) + \frac{\epsilon}{2} \ddot{W}(x,t) + O(\epsilon^2) = (p_+-p_-) \frac{l}{\epsilon} W'(x,t) + \frac{l^2}{2 \epsilon} W''(x,t) + O\left(\frac{l^3}{\epsilon} \right)   
        \end{align*}
        In the continuum limit $l, \epsilon \to 0$, with fixed $v$ and $D$, we get the diffusion equation:
        \begin{align*}
            \dot{W}(x,t) = v W'(x,t) + D W''(x,t)
        \end{align*}
        which leads back to the usual diffusion equation if we set $v = 0$. Note that $p_+ = p_- \Rightarrow v = 0$, as it should be. 
    \end{enumerate}
\end{exo}

\begin{exo}[Multiple steps at once]
    Write the analogous of:
    \begin{align*}
        W(x,t+\epsilon) = \frac{1}{2}[W(x-l),t + W(x+l,t)] 
    \end{align*}
    for the case where the probability to make a step of length $sl \in \{\pm nl \colon n \in \mathbb{Z} \land n > 0\}$ is:
    \begin{align*}
        p(s) = \frac{1}{Z} \exp\left(-|s| \alpha\right) 
    \end{align*}
    where $\alpha$ is some fixed constant. Determine:
    \begin{enumerate}
        \item the normalization constant $Z$
        \item what is the condition to have a meaningful continuum limit, discussing why the neglected terms do not contribute to such limit
        \item which equation you get in the continuum limit 
    \end{enumerate}
\end{exo}

\begin{exo}[Expected values]
    Use equation:
    \begin{align*}
        W(x,t) \equiv W(x,t|x_0, t_0) = \frac{1}{\sqrt{4 \pi D t}} \exp\left(-\frac{(x-x_0)^2}{4 D (t-t_0)} \right) \qquad t\geq t_0  
    \end{align*}
    to determine $\langle x \rangle_t$, $\langle x^2 \rangle_t$ and $\operatorname{Var}_t(x)$.   
\end{exo}

\begin{exo}[Diffusion with boundaries]
    Consider the diffusion equation:
    \begin{align*}
        \partial_t W(x,t) = D \partial_x^2 W(x,t)
    \end{align*}
    in the domain $[0,\infty)$ instead of $(-\infty,\infty)$. To do that one needs the \textit{boundary condition} (bc) that $W(x,t)$ has to satisfy at $0$. Determine the bc for the following two cases and for each of them solve the diffusion equation with the initial condition $W(x,t=0) = \delta(x-x_0 )$ with $x_0 > 0$.
    \begin{enumerate}
        \item \textit{Case of reflecting bc}: when the particle arrives at the origin it bounces back and remains in the domain. How is the flux of particles at $0$?
        \item \textit{Case of absorbing bc}: when the particle arrives at the origin it is removed from the system (captured by a trap acting like a filter!) What is $W(x=0, t)$ at all time $t$? Notice that in this case we do not expect that the probability is conserved, i.e. we have instead a \textit{Survival Probability}:
        \begin{align*}
            \mathcal{P}(t) \equiv \int_0^\infty W(x,t) \dd{x}    
        \end{align*}      
        that decreases with $t$. Calculate it and determine its behavior in the two regimes $t \ll x_0^2/D$ and $t \gg x_0^2/D$. Why $x_0^2/D$ is a relevant time scale?\\
        (Hint: use the fact that $e^{\pm ikx}$ are eigenfunctions of $\partial_x^2$ corresponding to the same eigenvalue and choose an appropriate linear combination of them so to satisfy the bc for the two cases. Be aware to ensure that the eigenfunctions so determined are orthonormal. Use also the fact that $\int_{\mathbb{R}} e^{iqx} \dd{x} = \delta(q)$  )
    \end{enumerate}


\end{exo}

\end{comment}

\section{SDE}
\begin{exo}[Change of variables in $\lambda$ prescription]
Generalize the results for the change of variable formula for Ito integrals to the case of a generic $\lambda$-prescription and re-derive the result of problem 4.9.\\

\textbf{Solution}. The main idea is to start from a Stochastic Differential Equation in $\lambda$-prescription, convert it to an equivalent formulation using the Ito prescription, apply Ito's formula for changing variables, and then go back to the former prescription.

First, two SDEs have the same solution $x(t)$ if, for any realization $B(t)$ of the Brownian noise, their solutions (which can now be found by \textit{normal} integration) coincide.

So, consider the usual SDE in Ito's prescription:
\begin{align*}
    \dd{x(t)} = a(x(t),t) \dd{t} + b(x(t),t) \dd{B(t)}
\end{align*}
which has solution:
\begin{align*}
    x(t) = x(t_0) + \int_{t_0}^t a(x(\tau),\tau) \dd{\tau} + \int_{t_0}^t b(x(\tau), \tau) \dd{B(\tau)}
\end{align*}
where the stochastic integral is formally defined as:
\begin{align*}
    \int_{t_0}^t b(x(\tau),\tau) \dd{B(\tau)} \equiv \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^n b(x_{i-1}, t_i) \Delta B_i \qquad \Delta B_i = B_i - B_{i-1}
\end{align*}
We now consider a  SDE in the $\lambda$ prescription:
\begin{align*}
    \dd{y(t)} = \alpha(y(t),t) \dd{t} + \beta(y(t),t) \dd{B(t)} \big|_{\lambda}
\end{align*}
which has solution:
\begin{align*}
    y(t) = \int_{t_0}^t \alpha(y(\tau), \tau) \dd{\tau} + \int_{t_0}^t \beta(y(\tau), \tau) \dd{\tau} \Big|_{\lambda}
\end{align*}
Now, however, the stochastic integral has a different definition:
\begin{align*}
    \int_{t_0}^t \beta(y(\tau), \tau) \dd{B(\tau)} \Big|_{\lambda} \equiv \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^n \beta\Big( (1-\lambda) x_{i-1} + \lambda x_i, t_{i-1}\Big) \Delta B_i
\end{align*}
We now impose that $y(t) = x(t)$ for every $t$, and search the mapping $a,b \mapsto \alpha, \beta$ that establishes a correspondence between an Ito SDE and a generic $\lambda$ prescription SDE.\\

Let's focus on the argument of the $\lambda$-integral, and expand it about the left extremum of the discretization:
\begin{align} \nonumber
    \beta(y_{i-1} - \lambda y_{i-1} + \lambda y_i, t_{i-1}) &= \beta(y_{i-1} + \lambda(y_i - y_{i-1}), t_{i-1}) =\\
    &= \beta(y_{i-1}, t_{i-1}) + \partial_x \beta(y_{i-1}, t_{i-1}) \lambda (y_i - y_{i-1}) \label{eqn:beta1}
\end{align}
As the paths are the same, $y_i = x_i$, and the increments $\Delta y_i$ follow the rule:
\begin{align*}
    \Delta y_i = a(x_{i-1}, t_{i-1}) \Delta t_i + b(x_{i-1}, y_{i-1}) \Delta B_i
\end{align*}
Leading to:
\begin{align*}
    (\ref{eqn:beta1}) = \beta_{i-1} + \beta_{i-1}' \lambda [a_{i-1}\Delta t_i + b_{i-1} \Delta B_i]
\end{align*}
Substituting inside the integral we get:
\begin{align*}
    \int_{t_0}^t \beta(y(\tau), \tau) \dd{B(\tau)} \Big|_{\lambda} &= \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^{n} \Big[\beta_{i-1} \Delta B_i + \lambda \beta'_{i-1} \Delta B_i [a_{i-1} \Delta t_i + b_{i-1} \Delta B_{i}] \Big] =\\
    &=\overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^{n}\Big[
    \beta_{i-1} \Delta B_i + \lambda \beta_{i-1}' b_{i-1} \Delta B_i^2 + O(\Delta B_i \Delta t_i)    
    \Big]
\end{align*}
We already proved that:
\begin{align*}
    \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^n G_{i-1} \Delta B_i^2 = \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^n G_{i-1} \Delta t_i
\end{align*}
And so we can use this result, setting $G_{i-1} = \lambda \beta'_{i-1} b_{i-1}$, justifying the usual $\dd{B}^2 = \dd{t}$ rule. So, this leads to:
\begin{align} \nonumber 
    \int_{t_0}^t \beta(y(\tau), \tau) \dd{B(\tau)} \Big|_{\lambda} &= \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^{n} \Big[\beta_{i-1} \Delta B_i + \lambda \beta_{i-1}' b_{i-1} \Delta t_i \Big] = \\
    &=\int_{t_0}^t \beta(y(\tau), \tau) \dd{B(\tau)} + \lambda \int_{t_0}^t b(y(\tau), \tau) \pdv{x} \beta(y(\tau), \tau) \dd{\tau}
    \label{eqn:formula}
\end{align}
where the last two integrals are Ito integrals. We have now found a way to evaluate a $\lambda$-integral using an Ito integral (provided the paths are generated by a Ito SDE). We can find the explicit conversion rules by equating the solutions:
\begin{align*}
    x(t) &= x(t_0) + \int_{t_0}^t a(x(\tau), \tau) \dd{\tau} + \int_{t_0}^t b(x(\tau), \tau) \dd{B(\tau)} =\\
    &\overset{!}{=} x(t_0) + \int_{t_0}^t \alpha(x(\tau), \tau) \dd{\tau} + \int_{t_0}^t \beta(x(\tau), \tau) \dd{B(\tau)} =\\
    &\underset{(\ref{eqn:formula})}{=} x(t_0) + \int_{t_0}^t \alpha(x(\tau), \tau) \dd{\tau} + \int_{t_0}^t \beta(x(\tau), \tau) \dd{B(\tau)} + \lambda \int_{t_0}^t b(x(\tau), \tau) \partial_x \beta(x(\tau), \tau) \dd{\tau}
\end{align*} 
leading to:
\begin{align*}
    \begin{cases}
        \alpha + \lambda b \partial_x \beta = a\\
        b = \beta
    \end{cases} \Rightarrow \begin{cases}
        \alpha = a - \lambda b \partial_x \beta\\
        \beta = b
    \end{cases}
\end{align*}
where $(\alpha, \beta)$ are the coefficients in the $\lambda$ SDE, and $(a,b)$ the ones in the equivalent Ito SDE.

Consider now a $\lambda$ SDE:
\begin{align*}
    \dd{x} = \alpha \dd{t} + \beta \dd{B(t)}
\end{align*}
The equivalent Ito SDE is:
\begin{align}\label{eqn:newdx}
    \dd{x} = (\alpha + \lambda \beta \partial_x \beta) \dd{t} + \beta \dd{B(t)}
\end{align}
Squaring:
\begin{align*}
    \dd{x}^2 = \beta^2 \dd{t}
\end{align*}
Where we used $\dd{B}^2 = \dd{t}$ (as this is an Ito SDE), and ignored higher order terms. It is clear that $\dd{x}^n = 0$ with $n > 0$.

Let $y=y(x)$ be a change of variables. The new differential will be:
\begin{align*}
    \dd{y} = \dv{y}{x} \dd{x} + \frac{1}{2} \dv[2]{y}{x} \dd{x}^2 + \dots
\end{align*}
and we can ignore the higher order terms, as they will be $O(\dd{t})$. Substituting in (\ref{eqn:newdx}) we get:
\begin{align*}
    \dd{y} &= \dv{y}{x} (\alpha + \lambda \beta \partial_x \beta) \dd{t} + \dv{y}{x} \beta \dd{B} + \frac{1}{2} \dv[2]{y}{x} \beta^2 \dd{t}  =\\
    &= \left[\dv{y}{x}\left(\alpha+ \lambda \beta \partial_x \beta \right) + \frac{1}{2} \dv[2]{y}{x} \beta^2 \right]\dd{t} + \dv{y}{x} \beta \dd{B}
\end{align*}
To complete the change of variables, we need to express everything in terms of $y$ - in particular the derivatives. One trick is to use the inverse function theorem:
\begin{align*}
    \dv{y}{x} = \left(\dv{x}{y}\right)^{-1} = \frac{1}{x'(y)} 
\end{align*}
For the second derivative, note that, if $f$ and $g$ are the inverse of each other:
\begin{align*}
    g \circ f = \operatorname{id} &\Rightarrow g(f(x)) = x \underset{\rm d/dx}{\Rightarrow} g'(f(x)) f'(x) = 1\\
     &\underset{\rm d/dx}{\Rightarrow} g''(f(x)) [f'(x)]^2 + g'(f(x)) f''(x) = 0 \\&\underset{y=f(x)}{\Rightarrow}  g''(y) = - \frac{g'(y) f''(x)}{f'(x)^2} = - \frac{f''(x)}{[f'(x)]^3}  
\end{align*}
And in our case:
\begin{align*}
    \dv[2]{y}{x} = -\frac{x''(y)}{[x'(y)]^3} 
\end{align*}
One last thing:
\begin{align*}
    \pdv{x} \beta = \dv{y}{x} \pdv{y} \beta = \frac{1}{x'(y)} \partial_y \beta 
\end{align*}

This leads to:
\begin{align*}
    \dd{y} = \underbrace{\left[\frac{\alpha}{x'(y)} + \frac{\lambda \beta \partial_y \beta}{[x'(y)]^2} - \frac{1}{2} \beta^2 \frac{x''(y)}{[x'(y)]^3}    \right]}_{a}  \dd{t} + \underbrace{\frac{\beta}{x'(y)}}_{b}  \dd{B} 
\end{align*}

We can finally map this back to a $\lambda$ SDE and find the change of variable rule for that case. Applying the substitutions:
\begin{align*}
    \dd{y} = \tilde{\alpha} \dd{t} + \tilde{\beta} \dd{B} \qquad \begin{cases}
        \tilde{\alpha} &= a-\lambda b \partial_x b\\
    \tilde{\beta} &= b
    \end{cases}
\end{align*}
We arrive to:
\begin{align*}
    \tilde{\alpha} &= \frac{\alpha}{x'(y)} + \frac{\lambda \beta \partial_y \beta}{[x'(y)]^2} - \frac{1}{2} \beta^2 \frac{x''(y)}{[x'(y)]^3} - \lambda \frac{\beta}{x'(y)} \partial_y \frac{\beta}{x'(y)}  =\\
    &= \frac{\alpha}{x'(y)} + \cancel{\frac{\lambda \beta \partial_y \beta}{[x'(y)]^2}} - \frac{1}{2} \beta^2 \frac{x''(y)}{[x'(y)]^3} - \lambda \frac{\beta}{x'(y)}\left[\frac{\cancel{\partial_y \beta x'(y)} - x''(y) \beta}{[x'(y)]^2} \right] =\\
    &= \frac{\alpha}{x'(y)} +\frac{2\lambda - 1}{2} \frac{\beta^2 x''(y)}{[x'(y)]^3}\\
    \tilde{\beta} &= \frac{\beta}{x'(y)}\\
    \dd{y} &= \left[\frac{\alpha}{x'(y)} +\frac{2\lambda - 1}{2} \frac{\beta^2 x''(y)}{[x'(y)]^3}\right] \dd{t} + \frac{\beta}{x'(y)} \dd{B} 
\end{align*}
Let's bring this result to the usual notation:
\begin{align*}
    \alpha = f(x(\tau), \tau) \qquad \beta = g(x(\tau),\tau) \qquad \begin{dcases}
        y = h(x(\tau))\\
        \frac{1}{x'(y)} = \dv{h}{x} = h'(x(\tau))\\
        -\frac{x''(y)}{[x'(y)]^3} = \dv[2]{h}{x} = h''(x(\tau)) 
    \end{dcases}
\end{align*}
leading to:
\begin{align*}
    \dd{h(x(\tau))} &= \left(f(x(\tau),\tau) h'(x(\tau)) + \frac{1-2 \lambda}{2} h''(x(\tau)) g(x(\tau), \tau)^2\right) \dd{\tau} +\\
    &\quad \> + g(x(\tau), \tau) h'(x(\tau)) \dd{B(\tau)} \Big|_\lambda
\end{align*}
which is the formula for changing variables in the $\lambda$ prescription. Let's rearrange to isolate the $\dd{B}$ term:
\begin{align*}
    gh' \dd{B} = \dd{h} - \left(f h' + \frac{1-2\lambda}{2} h'' g^2\right) \dd{\tau}
\end{align*}
Then we integrate, leading to the formula:
\begin{align*}
    \int_{t_0}^t h'(x(\tau)) g(x(\tau), \tau) &= h(x(t))-h(x(t_0)) - \int_{t_0}^t h'(x(\tau))f(x(\tau), \tau) \dd{\tau}\\
    &\quad \> -\frac{1-2 \lambda}{2}\int_{t_{0}}^t  h''(x(\tau)) g(x(\tau), \tau)^2 \dd{\tau} 
\end{align*}
Finally, set $g(x(\tau), \tau) \equiv 1$, and $h'(x(\tau)) = B(\tau)$, so that:
\begin{align*}
    h = \frac{B^2}{2} \qquad h'' = 1 
\end{align*}
Substituting in the formula we can compute the desired integral:
\begin{align*}
    \int_{t_0}^t B(\tau) \dd{B(\tau)} = \frac{B^2(t)-B^2(t_0)}{2} + \frac{2 \lambda - 1}{2} (t-t_0)  
\end{align*}





\end{exo}



\listoftheorems

\end{document}
