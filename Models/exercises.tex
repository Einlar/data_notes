%%&latex
%
\documentclass[../template.tex]{subfiles}
\usepackage{amsbsy}

\begin{document}

\newgeometry{top=2cm, bottom=2cm, left=2cm, right=2cm}
\title{ \normalsize \textsc{Models of Theoretical Physics}
                \\ [2.0cm]
                \HRule{0.5pt} \\
                \LARGE \textbf{{Maritan's Exercises}}
                \HRule{2pt} \\ [0.5cm]
                \normalsize \today \vspace*{5\baselineskip}}

\date{}

\author{
    Francesco Manzali, 1234428\\
    Master's degree in Physics of Data \\ 
    UniversitÃ  degli Studi di Padova}

\maketitle
\restoregeometry
\tableofcontents
\newpage

\setcounter{chapter}{1}
\chapter{Stochastic Processes and Path Integrals}

\begin{exo}[Stirling's approximation]
    Use the $\Gamma$ function definition:
    \begin{align}
        \Gamma(n) \equiv \int_0^\infty x^{n-1} e^{-x} \dd{x} \quad n > 0\qquad \Gamma(n+1) = n! \label{eqn:gamma-func}
    \end{align}
    together with the saddle point approximation to derive the result used in chapter $2$ of Lecture Notes:
    \begin{align}
        \ln n! = n\ln n - n + \frac{1}{2} \ln(2 \pi n) + O\left(\frac{1}{n} \right) \label{eqn:stirling}
    \end{align} 

    \medskip

    \textbf{Solution}. To use the saddle-point approximation we need to rewrite (\ref{eqn:gamma-func}) in the following form:
    \begin{align}
        I(\lambda) = \int_S \dd{x} \exp\left(-\frac{F(x)}{\lambda} \right)
    \end{align}
    So that:
    \begin{align}
        I(\lambda) \underset{\lambda \to 0}{\approx}  \sqrt{2 \pi \lambda} \left(\pdv{F}{x} (x) \Big|_{x=x_0}\right)^{-1/2} \exp\left(-\frac{F(x_0)}{\lambda} \right)
        \label{eqn:saddle-formula}
    \end{align}
    where $x_0$ is a global minimum of $F(x)$.  

    First, we evaluate $\Gamma$ at $n+1$, and express the integrand as a single exponential:
    \begin{align*}
        \Gamma(n+1) = n! = \int_0^{+\infty} \dd{x} x^n e^{-x}  = \int_{0}^{+\infty} \dd{x} e^{-x + n \log x} 
    \end{align*}
    We want to collect a $n$ in the exponential, and then define $\lambda = 1/n$, so that the saddle-point approximation $\lambda \to 0$ corresponds to the case of a large factorial $n \to \infty$. To do this, we perform a change of variables $x \mapsto s$, so that $x = n s$, with $\dd{x} = n \dd{s}$:
    \begin{align*}
        \Gamma(n+1) &= \int_0^{+\infty} \dd{s} n \exp(-ns + n \log(ns))= \\
        &= n^{n+1} \int_0^{+\infty} \dd{s} \exp(n[\log s - s])
    \end{align*}
    In the last step we split the logarithm $n\log(ns) = n\log n + n\log s = n^n + n\log s$, extracted from the integral all terms not depending on $s$, and then collected the $n$ as desired. Now, letting $\lambda = 1/n$ we have:
    \begin{align*}
        = n^{n+1} \int_0^{+\infty} \dd{s} \exp\left(\frac{\log s - s}{\lambda} \right)
    \end{align*}
    which is in the desired form for (\ref{eqn:saddle-formula}).

    So, we compute the minimum of $F(s) = \log s - s$:
    \begin{align*}
        F'(s) &= \dv{s} (s - \log s) = 1 - \frac{1}{s} \overset{!}{=} 0 \Rightarrow s_0 = 1\\
        F''(s) &= \frac{1}{s^2} \Rightarrow F''(s_0) = 1 > 0 
    \end{align*}
    And applying formula (\ref{eqn:saddle-formula}):
    \begin{align*}
        n! \underset{n \to \infty}{\approx} \sqrt{\frac{2\pi}{n} } \cdot 1 \cdot e^{-n} = \sqrt{2 \pi} n^{n+\frac{1}{2}} e^{-n}  
    \end{align*}
    Finally, taking the logarithm leads to the result (\ref{eqn:stirling}):
    \begin{align*}
        \log n! \underset{n \to \infty}{\approx} n \log n -n +\frac{1}{2} \log (2 \pi n) 
    \end{align*}
\end{exo}

\begin{exo}[Random walk tends to a Gaussian]
    Implement a numerical simulation to explicitly show how the solution of the ME for a 1-dimensional random walk with $p_\pm = 1/2$ tends to the Gaussian. 
\end{exo}

\begin{exo}[Non symmetrical motion]
    Write the analogous of:
    \begin{align}
        W(x,t+\epsilon) = \frac{1}{2}[W(x-l, t) + W(x+l,t)] 
        \label{eqn:ME}
    \end{align}
    in the LN for the case with $p_+ = 1-p_- \neq p_-$ and determine:
    \begin{enumerate}
        \item How they depend on $l$ and $\epsilon$ in order to have a meaningful continuum limit
        \item The resulting continuum equation and how to map it in the diffusion equation:
        \begin{align*}
            \partial_t W(x,t) = D \partial_x^2 W(x,t)
        \end{align*}  
    \end{enumerate} 

    \medskip

    \textbf{Solution}. Consider a Brownian particle moving on a uniform lattice $\{x_i = i \cdot l\}_{i \in \mathbb{N}}$, making exactly one \textit{step} at each \textit{discrete instant} $\{t_n = n \cdot \epsilon\}_{n \in \mathbb{N}}$, with $l, \epsilon \in \mathbb{R}$ fixed. Denoting with $p_+$ the probability of a \textit{step to the right}, and with $p_-$ that of a \textit{step to the left}, the Master Equation for the particle becomes:
    \begin{align} \label{eqn:ME2}
        W(x,t+\epsilon) = p_+ W(x-l,t) + p_- W(x+l,t)
    \end{align}  
    
    \begin{enumerate}
        \item We already derived (see notes of 7/10) the expected position $n$ at timestep $n$ in that case:
        \begin{align}
            \langle x \rangle_{t_n} = n l (p_+ - p_-) = t\frac{l}{\epsilon} (p_+ - p_-) \label{eqn:pref-motion}
        \end{align}
        Intuitively, an unbalance $p_+ \neq p_-$ will result in a \textit{preferred motion} proportional to that unbalance. Thus we can rewrite (\ref{eqn:pref-motion}) as: 
        \begin{align*}
            \langle x \rangle_{t_n} = vt \qquad v = \frac{l}{\epsilon}(p_+ - p_-) 
        \end{align*}
        $v$ is the \textit{physical} parameter that needs to be fixed when performing the continuum limit. So, as $p_+ - p_- = 2p_+ -1$ by normalization, we can find the desired relation between $p_+$ and $v$:
        \begin{align*}
            (2p_+ - 1) \frac{l}{\epsilon} \equiv v \Rightarrow p_+ = \frac{1}{2} \left[\frac{v \epsilon}{l} + 1 \right]  
        \end{align*}
        As before, we also need to fix $l^2/(2\epsilon) \equiv D$.
        \item Expanding each term of (\ref{eqn:ME2}) in a Taylor series we get:
        \begin{align*}
            \cancel{W(x,t)} + \epsilon \dot{W}(x,t) + \frac{\epsilon^2}{2} \ddot{W}(x,t) + O(\epsilon^3) = \span\\
            &= p_+ \left[\cancel{W(x,t)} + l W'(x,t) + \frac{1}{2} l^2 W''(x,t) + O(l^3) \right]\\
            &\> + p_- \left[\cancel{W(x,t)} - lW'(x,t) + \frac{1}{2} l^2 W''(x,t) + O(l^3)  \right]      
        \end{align*}
        Using the normalization $p_+ + p_- = 1$ and dividing by $\epsilon$ leads to:
        \begin{align*}
            \dot{W}(x,t) + \frac{\epsilon}{2} \ddot{W}(x,t) + O(\epsilon^2) = (p_+-p_-) \frac{l}{\epsilon} W'(x,t) + \frac{l^2}{2 \epsilon} W''(x,t) + O\left(\frac{l^3}{\epsilon} \right)   
        \end{align*}
        In the continuum limit $l, \epsilon \to 0$, with fixed $v$ and $D$, we get the diffusion equation:
        \begin{align*}
            \dot{W}(x,t) = v W'(x,t) + D W''(x,t)
        \end{align*}
        which leads back to the usual diffusion equation if we set $v = 0$. Note that $p_+ = p_- \Rightarrow v = 0$, as it should be. 
    \end{enumerate}
\end{exo}

\begin{exo}[Multiple steps at once]
    Write the analogous of:
    \begin{align*}
        W(x,t+\epsilon) = \frac{1}{2}[W(x-l,t) + W(x+l,t)] 
    \end{align*}
    for the case where the probability to make a step of length $sl \in \{\pm nl \colon n \in \mathbb{Z} \land n > 0\}$ is:
    \begin{align*}
        p(s) = \frac{1}{Z} \exp\left(-|s| \alpha\right) 
    \end{align*}
    where $\alpha$ is some fixed constant. Determine:
    \begin{enumerate}
        \item the normalization constant $Z$
        \item what is the condition to have a meaningful continuum limit, discussing why the neglected terms do not contribute to such limit
        \item which equation you get in the continuum limit 
    \end{enumerate}

    \medskip

    \textbf{Solution}. Consider a uniform lattice $\{x_i = i \cdot l\}_{i \in \mathbb{N}}$, and a time discretization $\{t_n = n \cdot \epsilon\}_{n \in \mathbb{N}}$. Let $W(x_i, t_n)$ be the probability that a particle is in $x_i$ at time $t_n$.

    At each timestep, the particle can make \textit{jumps} of size $sl$, with $s \in \mathbb{N} \setminus \{0\}$, with probability $p(s)$. So, to get the probability of finding it at $x_i$ at the next timestep $t_{n+1}$ we sum over all possible jump sizes:
    \begin{align} \nonumber
        W(x_i, t_{n+1}) &= \sum_{s=1}^{+\infty} p(s) W(x_i-sl, t_n) + \sum_{s=1}^{+\infty} p(s) W(x_i+sl,t_n) =\\
        &= \frac{1}{Z} \sum_{s=1}^{+\infty} e^{-s \alpha} \left[ W(x_i-sl, t_n) +  W(x_i+sl,t_n) \right] \label{eqn:ME-multiple}
    \end{align} 
    The first sum is relative to jumps \textit{to the right}, while the latter is for jumps \textit{to the left}. Note that the particle always moves.

    \begin{enumerate}
        \item For the normalization, as the jumps can be in both directions, we have two sums:
        \begin{align*}
            Z &\overset{!}{=}  \sum_{s = 1}^{+\infty} e^{-|s| \alpha} + \sum_{s=-1}^{-\infty} e^{-|s| \alpha} = 2 \sum_{s=1}^{+\infty} e^{-s \alpha} =\\
            &= 2\left(\sum_{s=\textcolor{Red}{0}}^{+\infty} e^{-s \alpha} \textcolor{Red}{- 1}\right) \underset{(a)}{=} 2 \left(\frac{1}{1-e^{-\alpha}}  - 1\right) = \frac{2 e^{-\alpha}}{1 - e^{-\alpha}} 
        \end{align*}
        where in (a) we used the limit of a geometric series:
        \begin{align*}
            \sum_{n = 0}^{+\infty} r^{n} = \frac{1}{1-r} \qquad |r| < 1 
        \end{align*}
        \item We start from the master equation (\ref{eqn:ME-multiple}) and Taylor expand:
        \begin{align} \nonumber
            W(x,t) + \epsilon \dot{W}(x,t) + O(\epsilon^2) = \span \\ \nonumber &= \frac{1}{Z} \sum_{s=1}^{+\infty} e^{-s \alpha}\Big[W(x,t) -\cancel{sl W'(x,t)} + \frac{1}{2} (sl)^2 W''(x,t) + O(l^3) \\ \nonumber
            &\phantom{\> = \frac{1}{Z} \sum_{s=1}^{+\infty} e^{-s \alpha} \Big[}W(x,t) + \cancel{sl W'(x,t)} +\frac{1}{2}(sl)^2 W''(x,t) + O(l^3) \Big] =\\
            &= \frac{2}{Z} W(x,t) \underbrace{\sum_{s=1}^{+\infty} e^{-s \alpha}}_{Z/2}  + \frac{l^2}{\hlc{Yellow}{Z} } W''(x,t) \sum_{s=1}^{+\infty} s^2 e^{-s \alpha} \label{eqn:continuum-multiple}
        \end{align}
        Note that we can neglect the sum of the infinite $O(l^3)$, because they are weighted by a decreasing exponential $e^{-s \alpha}$, meaning that they quickly vanish for $s \to \infty$.

        To evaluate the second sum we start from:
        \begin{align*}
            \sum_{s=1}^{+\infty} e^{-s \alpha} = \frac{Z}{2} 
        \end{align*}
        Differentiating with respect to $\alpha$ two times we arrive to the desired formula:
        \begin{align*}
            \underset{\dd{\alpha}}{\to}  -\sum_{s=1}^{+\infty} s e^{-s \alpha} \underset{\dd{\alpha}}{\to} \sum_{s=1}^{+\infty} s^2 e^{-s \alpha} = \dv[2]{\alpha} \frac{Z}{2} = \frac{e^\alpha (1+e^\alpha)}{(e^\alpha- 1 )^3}   
        \end{align*}
        Substituting back in (\ref{eqn:continuum-multiple}):
        \begin{align*}
            \bcancel{W(x,t)} + \epsilon \dot{W}(x,t) + O(\epsilon^2) =\span \\ &= W(x,t) + \frac{l^2 \hlc{Yellow}{(e^{\alpha}-1)}}{\hlc{Yellow}{2}}  W''(x,t)\frac{e^\alpha (1+e^\alpha)}{(e^\alpha- 1 )^3} + O(l^3)=\\
            &= \bcancel{W(x,t)} + \frac{l^2}{2 } W''(x,t) \frac{e^\alpha (1+ e^{\alpha})}{(e^\alpha -1)^2} + O(l^3)
        \end{align*}
        Dividing by $\epsilon$:
        \begin{align*}
            \dot{W}(x,t) + O(\epsilon) = \frac{l^2}{2 \epsilon} W''(x,t) \frac{e^\alpha (1+ e^{\alpha})}{(e^\alpha -1)^2} + O(l^3 \epsilon^{-1})
        \end{align*}
        So to get the correct continuum limit for $\epsilon, l \to 0$ we need to fix $l^2/(2 \epsilon) \equiv D$.
        \item The final continuum limit is given by:
        \begin{align*}
            \dot{W}(x,t) = D  \frac{e^\alpha (1+ e^{\alpha})}{(e^\alpha -1)^2} W''(x,t)
        \end{align*}
    \end{enumerate}
    
\end{exo}

\begin{exo}[Expected values]
    Use equation:
    \begin{align*}
        W(x,t) \equiv W(x,t|x_0, t_0) = \frac{1}{\sqrt{4 \pi D t}} \exp\left(-\frac{(x-x_0)^2}{4 D (t-t_0)} \right) \qquad t\geq t_0  
    \end{align*}
    to determine $\langle x \rangle_t$, $\langle x^2 \rangle_t$ and $\operatorname{Var}_t(x)$.   
    
    \medskip

    \textbf{Solution}. $W(x,t)$ is a gaussian with mean $x_0$ and standard deviation $\sqrt{2D(t-t_0)}$. So from standard gaussian integrals we immediately know that:
    \begin{align*}
        \langle x \rangle_t = x_0; \qquad \operatorname{Var}_t(x) = \sigma^2 = 2D(t-t_0) 
    \end{align*}
    Then:
    \begin{align*}
        \operatorname{Var}(x) = \langle x^2 \rangle  - \langle x \rangle^2 \Rightarrow \langle x^2 \rangle = \operatorname{Var}(x) + \langle x \rangle^2 = 2D(t-t_0) + x_0^2
    \end{align*}
\end{exo}

\begin{exo}[Diffusion with boundaries]
    Consider the diffusion equation:
    \begin{align*}
        \partial_t W(x,t) = D \partial_x^2 W(x,t)
    \end{align*}
    in the domain $[0,\infty)$ instead of $(-\infty,\infty)$. To do that one needs the \textit{boundary condition} (bc) that $W(x,t)$ has to satisfy at $0$. Determine the bc for the following two cases and for each of them solve the diffusion equation with the initial condition $W(x,t=0) = \delta(x-x_0 )$ with $x_0 > 0$.
    \begin{enumerate}
        \item \textit{Case of reflecting bc}: when the particle arrives at the origin it bounces back and remains in the domain. How is the flux of particles at $0$?
        \item \textit{Case of absorbing bc}: when the particle arrives at the origin it is removed from the system (captured by a trap acting like a filter!) What is $W(x=0, t)$ at all time $t$? Notice that in this case we do not expect that the probability is conserved, i.e. we have instead a \textit{Survival Probability}:
        \begin{align*}
            \mathcal{P}(t) \equiv \int_0^\infty W(x,t) \dd{x}    
        \end{align*}      
        that decreases with $t$. Calculate it and determine its behavior in the two regimes $t \ll x_0^2/D$ and $t \gg x_0^2/D$. Why $x_0^2/D$ is a relevant time scale?\\
        (Hint: use the fact that $e^{\pm ikx}$ are eigenfunctions of $\partial_x^2$ corresponding to the same eigenvalue and choose an appropriate linear combination of them so to satisfy the bc for the two cases. Be aware to ensure that the eigenfunctions so determined are orthonormal. Use also the fact that $\int_{\mathbb{R}} e^{iqx} \dd{x} = \delta(q)$  )
    \end{enumerate}
    
    \medskip

    \textbf{Solution}. The idea is to extend this equation to the \textit{entire real line} (a case that we already tackled) by exploiting \textit{symmetries}.   

    So, consider the diffusion equation on $\mathbb{R}$:
    \begin{align*}
        \partial_t \tilde{W}(x,t) = D\partial_x^2 \tilde{W}(x,t) \qquad x \in \mathbb{R}
    \end{align*}
    We note that if $\tilde{W}(x,0)$ is even/odd, then $\tilde{W}(x,t)$ is even/odd $\forall t$, that is the diffusion \textit{preserves} the initial symmetry.
    
    This can be seen directly. For example, suppose $\tilde{W}(x,0)$ is odd. Then the solution at time $t$ is:
    \begin{align*}
        \tilde{W}(x,t) = \int_{\mathbb{R}} \dd{x_0} W(x,t|x_0,0) W(x_0,0)
    \end{align*}
    Let's evaluate $\tilde{W}(-x,t)$:
    \begin{align*}
        \tilde{W}(-x,t) = \int_{-\infty}^{+\infty} \dd{x_0} \frac{1}{\sqrt{4 \pi Dt}}  \exp\left(-\frac{(-x -x_0)^2}{4Dt} \right) \tilde{W}(x_0,0)
    \end{align*}
    Changing variables to $x_0' = -x_0$:
    \begin{align*}
        \tilde{W}(-x,t) = -\int_{+\infty}^{-\infty} \dd{x_0'} \frac{1}{\sqrt{4 \pi Dt}}  \exp\left(-\frac{(-x +x_0')^2}{4Dt} \right) \tilde{W}(-x_0',0)
    \end{align*}
    Then inverting the integration path, renaming $x_0' \to x_0$ and using the $W(x_0,0) = -W(-x_0,0)$ leads to:
    \begin{align*}
        \tilde{W}(-x,t) = -\int_{-\infty}^{+\infty} \dd{x_0} \frac{1}{\sqrt{4 \pi Dt}} \exp\left(-\frac{(x-x_0)^2}{4Dt} \right) \tilde{W}(x_0,0) = - \tilde{W}(x,t)
    \end{align*} 
    
    Assuming $\tilde{W}(x,0)$ even, with similar calculations it is possible to see that also $\tilde{W}(x,t)$ is even.

    \begin{expl}
        \textbf{Alternative method}. We could prove the same \textit{symmetry preserving property} without knowing the explicit solution formula. Just start from the diffusion equation:
        \begin{align} \label{eqn:d1}
            \partial_t W(x,t) = D\partial_x^2 W(x,t)
        \end{align}  
        Note that it is \textit{linear}, meaning that if $W_1(x,t)$ and $W_2(x,t)$ are solutions, then also $a W_1(x,t) + bW_2(x,t)$ is a solution.

        So, let $W(x,t)$ be \textit{any} solution. We can construct an \textit{odd} solution as follows:
        \begin{align*}
            W_{\mathrm{odd}}(x,t) = \frac{W(x,t) - W(-x,t)}{2} 
        \end{align*}  
        To prove that this is indeed a solution, we need to show that $W(-x,t)$ is a solution, and the rest follows from linearity. So, by direct substitution:
        \begin{align*}
            \partial_t W(-x,t) = D \partial_x^2 [W(-x,t)] = D \partial_x[-W'(-x,t)] = D W''(-x,t)
        \end{align*}
        This is possible because the $x$-derivative is of \textit{even} order. 
    \end{expl}

    \begin{expl}
        Now, by \textit{uniqueness} of solutions, the same initial condition cannot \textit{evolve} to different solutions. So, as $W_{\mathrm{odd}}(x,t)$ is odd by construction $\forall t$, we have showed that any solution of (\ref{eqn:d1}) that starts \textit{odd} will remain \textit{odd}.  
    \end{expl}

    We can finally tackle the half-line problem. Note that any solution $\tilde{W}(x,t)$ on the full line, when evaluated on $x>0$, will be a solution on the half-line (because the differential equation remains the same). The only thing that needs to be checked is the boundary condition - and that's why we needed the excursus on symmetry. We note that:

    \begin{itemize}
        \item \textbf{Absorbing boundary}. Any $\tilde{W}(x,t)$ \textbf{odd} satisfies the absorbing boundary condition $\tilde{W}(0,t) = 0$ (if the particle disappears upon reaching $x=0$, then the probability of finding it there must be always $0$).
        \item \textbf{Reflecting boundary}. Recall the definition of flux:
        \begin{align*}
            \partial_t W(x,t) \equiv - \pdv{x} J(x,t)
        \end{align*}
        Comparing with (\ref{eqn:d1}) we have:
        \begin{align*}
            J(x,t) = -D \pdv{x}W(x,t)
        \end{align*}
        So the reflective boundary condition becomes:
        \begin{align*}
            J(0,t) \overset{!}{=} 0 \Rightarrow \pdv{x} W(0,t) = 0
        \end{align*}
        Now note that this condition is automatically satisfied by any solution $\tilde{W}(x,t)$ on the whole line that is \textbf{even}. In fact:
        \begin{align*}
            \pdv{x} \tilde{W}(0,t) = \lim_{\Delta x \to 0} \frac{\tilde{W}(\Delta x,t) - \tilde{W}(0,t)}{\Delta t} = \lim_{\Delta x \to 0} \frac{\tilde{W}(0,t) - \tilde{W}(-\Delta x,t)}{\Delta x}  
        \end{align*} 
        And inserting $\tilde{W}(-\Delta x,t) = \tilde{W}(\Delta x,t)$ by symmetry:
        \begin{align*}
            \pdv{x} \tilde{W}(0,t) =  \underbrace{\lim_{\Delta x \to 0} \frac{\tilde{W}(\Delta x,t) - \tilde{W}(0,t)}{\Delta t} }_{a}= \underbrace{\lim_{\Delta x \to 0} \frac{\tilde{W}(0,t) - \tilde{W}(\hlc{Yellow}{\Delta x},t)}{\Delta x} }_{-a}= 0
        \end{align*}
        As $a=-a$ only if $a=0$.
    \end{itemize}
    Summarizing, by choosing the initial condition $\tilde{W}(x,0)$ with the right symmetry on the whole line, we get \q{for free} the solution on the half-line with an absorbing/reflective boundary at $x=0$. Let's do just that.

    \begin{itemize}
        \item \textbf{Absorbing boundary}. We need to \textit{reflect} the half-line initial condition so to have an \textit{odd} initial condition on the full line. So:
        \begin{align*}
            \tilde{W}(x,0) = \delta(x-x_0)-\delta(x+x_0)
        \end{align*}   
        The full solution is then:
        \begin{align*}
            \tilde{W}(x,t) &= \int_\mathbb{R} \frac{\dd{x'}}{\sqrt{4 \pi Dt}} \exp\left(-\frac{(x-x')^2}{4Dt} \right) \tilde{W}(x',0) =\\
            &= \int_{\mathbb{R}} \frac{\dd{x'}}{\sqrt{4 \pi D t}} \exp\left(-\frac{(x-x')^2}{4Dt} \right) (\delta(x'-x_0) - \delta(x+x_0)) =\\
            &=\frac{1}{\sqrt{4 \pi D t}} \left[\exp\left(-\frac{(x-x_0)^2}{4Dt} \right) - \exp\left(-\frac{(x+x_0)^2}{4Dt} \right)\right] \qquad x>0
        \end{align*}
        The survival probability $\mathcal{P}(t)$ is the integral over the half-line:
        \begin{align}\nonumber
            \mathcal{P}(t) &= \int_{\mathbb{R}^+} \tilde{W}(x,t) =\frac{1}{\sqrt{4 \pi D t}} \int_0^{+\infty} \dd{x} \left[\exp\left(-\frac{(x-x_0)^2}{4Dt} \right) - \exp\left(-\frac{(x+x_0)^2}{4Dt} \right)\right] =\\ \nonumber
            &\underset{(a)}{=\frac{1}{\sqrt{4 \pi D t}}}  \int_{-x_0}^{+\infty} \dd{y} \exp\left(-\frac{y^2}{4Dt} \right) -\frac{1}{\sqrt{4 \pi D t}} \int_{x_0}^{+\infty} \dd{y} \exp\left(-\frac{y^2}{4Dt} \right) =\\ \label{eqn:Psurv}
            &=\frac{1}{\sqrt{4 \pi D t}} \int_{-x_0}^{+x_0} \dd{y} \exp\left(-\frac{y^2}{4Dt} \right) \underset{(b)}{=} \frac{1}{\sqrt{\pi}} \int_{-x_0/\sqrt{4Dt}}^{x_0/\sqrt{4Dt}} \dd{t} e^{-t^2}   \equiv \operatorname{erf}\left(\frac{x_0}{\sqrt{4Dt}} \right) 
        \end{align}
        where in (a) we changed variables $y=x-x_0$ (first integral) and $y=x+x_0$ (second integral). The in (b) we performed another change of variable $t=y/\sqrt{4 Dt}$. The $\operatorname{erf}(x)$ is the gaussian \textit{error function} (used mainly in statistics), defined as the integral:
        \begin{align*}
            \operatorname{erf}(x) = \frac{1}{\sqrt{\pi}}\int_{-x}^{+x} e^{-t^2} \dd{t}  
        \end{align*}  
        which has no analytical form (except for $x=\infty$) and must be computed numerically.

        Looking at (\ref{eqn:Psurv}) we see that $\mathcal{P}(t)$ is, \textit{graphically}, the area under a center slice - with width $2x_0/\sqrt{4 D t}$ - of the unit gaussian.
        
        As $D$ has units of $[\si{\m^2\per\s}]$, $x_0^2/D$ has units of time, and so can be interpreted as the \textit{scale} of the \textit{first arrival time} at $x=0$ from $x_0$ for a diffusing particle. In fact:
        \begin{itemize}
            \item For $t \ll x_0^2/D$, or equivalently $x_0^2/(Dt) \gg 1$, $\mathcal{P}(t) \to 1$, as the integral in (\ref{eqn:Psurv}) is over most of the \textit{support} of the gaussian. Physically, this means that the particle \textit{did not have enough time} to reach the absorbing boundary $x=0$ from its starting position at $x_0$.
            \item  For $t \gg x_0^2/D$, i.e. $x_0^2/(Dt) \ll 1$, $\mathcal{P}(t) \to 0$, as the integral limits in (\ref{eqn:Psurv}) are almost the same. Physically, given a \textit{sufficiently long time}, a particle will reach $x=0$ through random motion, independently of its starting position. 
        \end{itemize}
        

        
        
        
        \item \textbf{Reflective boundary}. This time we need an \textit{even} extension:
        \begin{align*}
            \tilde{W}(x,0) = \delta(x-x_0) + \delta(x+x_0)
        \end{align*}  
        Leading to:
        \begin{align*}
            \tilde{W}(x,t) &= \int_\mathbb{R} \frac{\dd{x'}}{\sqrt{4 \pi Dt}} \exp\left(-\frac{(x-x')^2}{4Dt} \right) \tilde{W}(x',0) =\\
            &= \int_{\mathbb{R}} \frac{\dd{x'}}{\sqrt{4 \pi D t}} \exp\left(-\frac{(x-x')^2}{4Dt} \right) (\delta(x'-x_0) + \delta(x+x_0)) =\\
            &=\frac{1}{\sqrt{4 \pi D t}} \left[\exp\left(-\frac{(x-x_0)^2}{4Dt} \right) + \exp\left(-\frac{(x+x_0)^2}{4Dt} \right)\right] \qquad x>0
        \end{align*}
    \end{itemize}
    
\end{exo}

\begin{comment}
The idea is to rewrite $W(x,t)$ as a (infinite) linear combination of eigenfunctions $\varphi_k(x)$ of the operator $\partial_x^2$, with coefficients $c_k$ that can depend on time:
    \begin{align} \label{eqn:eigen-decomp}
        W(x,t) = \int_{\mathbb{R}} c_k (t)\varphi_k(x) \dd{k} \qquad \partial_x^2 \varphi_k(x) = \lambda_k \varphi_k(x)
    \end{align}
    So that the second derivative can be computed as follows:
    \begin{align*}
        \partial_x^2 W(x,t) = \int_{\mathbb{R}} c_k (t)\partial_x^2 \varphi_k(x) \dd{k} = \int_{\mathbb{R}} c_k(t) \lambda_k \varphi_k(x) \dd{k}
    \end{align*}
    And substituting back in the differential equation:
    \begin{align*}
        \partial_t \int_{\mathbb{R}} \dd{k} c_k (t)\varphi_k(x) = D\int_\mathbb{R} \dd{k} \lambda_k c_k(t) \varphi_k(x)
    \end{align*}
    As the $\varphi_k(x)$ are orthonormal, the two sums are equal if and only if each term is equal, and so:
    \begin{align*}
        \dot{c}_k (t)= D \lambda_k c_k(t)
    \end{align*}
    This is a differential equation that can be solved, for example by separation of variables.

    \medskip
    
    To find the $\varphi_k(x)$ we solve the eigenvalue equation:
    \begin{align}
        \partial_x^2 \varphi_k(x) = \lambda_k \varphi_k(x) \label{eqn:eigenvalue-eq}
    \end{align}
    If we let $\lambda_k = -k^2$, (\ref{eqn:eigenvalue-eq}) becomes the harmonic oscillator ODE, and so:
    \begin{align*}
        \varphi_k(x) = A e^{ikx} + B e^{-ikx} \qquad A,B \in \mathbb{C}
    \end{align*}
    We discover that $e^{\pm ikx}$ are both eigenfunctions of $\partial_x^2$ corresponding to the same eigenvalue, and so it is any linear combination of them. As $\partial_x^2$ is a \textit{hermitian} operator, $\varphi_k(x)$ and $\varphi_{k'}(x)$ with $k \neq k'$ must be orthogonal, as they are eigenfunctions of different eigenvalue. 

    Let's consider now the boundary conditions.
    Substituting in (\ref{eqn:eigen-decomp}) and setting $W(0,t) = 0$ (\textbf{absorbing boundary}):
    \begin{align*}
        0 \overset{!}{=}  W(0,t) = \int_{\mathbb{R}} \dd{k} c_k(t) [A+B]
    \end{align*}
    So the boundary condition is automatically satisfied by setting $A = -B$. For example, let $A=1/(2i)$ and $B=-1/(2i)$, so that:
    \begin{align*}
        \varphi_k(x) = \frac{e^{ikx} - e^{-ikx}}{2i} = \sin(kx)
    \end{align*}
\end{comment}
\begin{exo}
    For a Brownian motion $x(s)$, $0\leq s \leq t$, with diffusion coefficient $D$ and initial condition $x(0) = 0$, show that:
    \begin{align*}
        \operatorname{Prob}\left(\sup_{0 \leq s \leq t} x(s) \geq a \right) = \frac{2}{\sqrt{4 \pi D t }} \int_a^{\infty} \exp\left(-\frac{z^2}{4 Dt } \right) \dd{z} = \operatorname{erfc}\left(\frac{a}{\sqrt{4 D t}} \right) 
    \end{align*}  
    (Hint: some of the results of previous exercises are useful to derive the above result.)

    \medskip

    \textbf{Solution}. We wish to compute the probability that a Brownian path starting in $0$ at $t=0$ arrives \textit{past} $x=a$ in a time window $[0,t]$. In other words, generate $N=100$ paths in $[0,t]$, and count how many of them go further thatn $x=a$. We are interested in the fraction of \q{successes} as $N \to \infty$. 
    
    \medskip

    Note that it does not matter the final point of each path, but just if it reaches $x=a$ at least one time or not. So, let $x=a$ be an absorbing boundary, and let's study the Brownian motion in the half-line $(-\infty,a)$. As the particle is free, there is no preferred direction, and by symmetry we can \textit{reverse} the interval, studying $x \in (-a,+\infty)$ with absorbing boundary at $x=a$. Formally:
    \begin{align*}
        \operatorname{Prob}\left(\sup_{0 \leq s \leq t} x(s) \geq a \Big| x(0) = 0\right) = \operatorname{Prob}\left(\inf_{0 \leq s \leq t} x(s)\leq -a\Big| x(0) = 0\right) \equiv p
    \end{align*}
    Finally, the system is translationally invariant: the only \textit{physical} parameter that matters for the final probability is the distance between the starting position ($0$) and the absorbing boundary ($-a$). So we can consider the particle starting at $a$ with an absorbing boundary at $0$, retrieving the case $x \in (0,\infty)$ that we studied in the last exercise.
    
    The probability of \textit{crossing} the boundary is the probability of \textit{not surviving}:
    \begin{align*}
        p = \operatorname{Prob}\left(\inf_{0 \leq s \leq t}x(s) \leq 0\Big|x(0) = a\right)  = 1-\mathcal{P}(t) &\underset{(\ref{eqn:Psurv})}{=} 1-\operatorname{erf}\left(\frac{a}{\sqrt{4 D t}} \right)\\
        & \equiv \operatorname{erfc}\left(\frac{a}{\sqrt{4Dt}} \right) 
    \end{align*}  
    Where $\operatorname{erfc}$ is the \textit{complementary error function}, defined as:
    \begin{align*}
        \operatorname{erfc}(x) = 1-\operatorname{erf}(x)  
    \end{align*}  
\end{exo}

\begin{exo}
    Solve the diffusion equation:
    \begin{align} \label{eqn:diff-d}
        \pdv{t} W(\bm{x},t) = D \nabla^2 W(\bm{x},t) \qquad \bm{x} \in \mathbb{R}^d
    \end{align}
    \begin{enumerate}
        \item Determine the propagator $W(\bm{x},t|\bm{x_0},t_0)$
        \item The averages $\langle \bm{x} \rangle$ and $\langle \norm{\bm{x}}^2 \rangle$
        \item The general solution for a generic initial condition $W(\bm{x_0},t_0)$
    \end{enumerate}
    
    \medskip
 
    \textbf{Solution}. The procedure is the same as in the $d=1$ case. So we start with a $d$-dimensional Fourier transform to convert the problem to a first order ODE that can be solved with separation of variables.

    \begin{align} \label{eqn:inv-t}
        W(\bm{x},t) &= \mathcal{F}^{-1}[\tilde{W}(\bm{k},t)](\bm{x},t) = \int_{\mathbb{R}^d} \dd[d]{\bm{k}} \frac{1}{(2 \pi)^d}  e^{i\bm{x} \cdot \bm{k}} \underbrace{\tilde{W}(\bm{k},t)}_{c(\bm{k},t)} =\\ \nonumber
        &= \int_{\mathbb{R}^d} \dd[d]{\bm{k}} \ket{c_{\bm{k}}} \langle c_{\bm{k}}|W(\bm{x},t) \rangle\\
        c(\bm{k},t) &= \mathcal{F}[W(\bm{x},t)](\bm{k},t) = \int_{\mathbb{R}^d} \dd[d]{\bm{x}} e^{-i\bm{x} \cdot \bm{k}} W(\bm{x},t) = \langle c_{\bm{k}}|W(\bm{x},t) \rangle \label{eqn:ck}
    \end{align}
    We can rewrite (\ref{eqn:diff-d}) in terms of the Fourier coefficients by differentiating $c(\bm{k},t)$ (\ref{eqn:ck}) with respect to $t$, and then substituting in (\ref{eqn:diff-d}):
    \begin{align*}
        \dot{c}(\bm{k},t) &= \int_{\mathbb{R}^d} \dd[d]{\bm{x}} e^{-i\bm{x} \cdot \bm{k}} \dot{W}(\bm{x},t) \underset{(\ref{eqn:diff-d})}{=} \int_{\mathbb{R}^d} \dd[d]{\bm{x}} e^{-i\bm{x} \cdot \bm{k}} D \nabla^2 W(\bm{x},t) = \\
        &\underset{(a)}{=}  -D \norm{\bm{k}}^2\underbrace{ \int_{\mathbb{R}^d} \dd[k]{\bm{x}} e^{-i\bm{x} \cdot \bm{k}} W(\bm{x},t)}_{c(\bm{k},t)}
    \end{align*}
    where in (a) we integrated by parts two times over each dimension. This is d Explicitly, for $d=2$:
    \begin{align*}
        D \int_{\mathbb{R}^2} \dd[2]\bm{x} e^{-i x_1 k_1 - ix_2 k_2} \left(\pdv[2]{x_1} W(\bm{x},t) + \pdv[2]{x_2} W(\bm{x},t)\right)
    \end{align*}
    Consider for example the first term:
    \begin{align*}
        D\int_{\mathbb{R}} \dd{x_2} e^{-ix_2 k_2} \int_{\mathbb{R}} \dd{x_1} e^{-i x_1 k_1} \pdv[2]{x_1} W(\bm{x},t) =\span \\
        &=D \int_\mathbb{R} \dd{x_2} e^{-ix_2 k_2} \int_{\mathbb{R}} \dd{x_1} \left(\pdv[2]{x_1} e^{-ix_1 k_1}\right) W(\bm{x},t) =\\
        &= -Dk_1^2 \int_{\mathbb{R}^2} \dd[2]{\bm{x}} e^{-i\bm{x} \cdot \bm{k}} W(\bm{x},t) 
    \end{align*}
    The same reasoning extends to $d$ dimensions, and explains why we can perform partial integration as in $d=1$ case.

    \medskip

    So we arrived at:
    \begin{align*}
        \dot{c}(\bm{k},t) = -D\norm{\bm{k}}^2 c(\bm{k},t)
    \end{align*}
    Which can be solved by separation of variables:
    \begin{align*}
        \frac{\dd{c(\bm{k},t)}}{c(\bm{k},t)}  = -D \norm{\bm{k}}^2 \dd{t} \Rightarrow c(\bm{k},t) = C e^{-D \norm{\bm{k}}^2 t}
    \end{align*}
    where the constant $C$ of integration can be found by imposing the initial condition:
    \begin{align*}
        c(\bm{k},t_0) \overset{!}{=} C e^{-D \norm{\bm{k}}^2 t_0} \Rightarrow c(\bm{k},t) = c(\bm{k},t_0) e^{-D \norm{\bm{k}}^2 (t-t_0)}
    \end{align*}
    To retrieve $W(\bm{x},t)$ we perform the inverse Fourier transform, according to (\ref{eqn:inv-t}):
    \begin{align*}
        W(\bm{x},t) &= \frac{1}{(2 \pi)^d} \int_{\mathbb{R}^d} \dd[d]{\bm{k} e^{i\bm{k}\cdot \bm{x}}} c(\bm{k},t) =\\
        &=\frac{1}{(2 \pi)^d} \int_{\mathbb{R}^d} \dd[d]{\bm{k} e^{i\bm{k}\cdot \bm{x}}} c(\bm{k},t_0) e^{-D \norm{\bm{k}}^2 (t-t_0)}
    \end{align*}
    We then need to transform back also the initial condition:
    \begin{align*}
        c(\bm{k},t_0) = \int_{\mathbb{R}^d} \dd[d]{\bm{y}} e^{-i\bm{y} \cdot \bm{k}} W(\bm{y},t_0)
    \end{align*}
    Substituting it above:
    \begin{align*}
        W(\bm{x},t) &= \frac{1}{(2 \pi)^d} \int_{\mathbb{R}^d} \dd[d]{\bm{k}\int_{\mathbb{R}^d} \dd[d]{\bm{y}}  e^{i\bm{k}\cdot \bm{x}}} e^{-i\bm{y} \cdot \bm{k}} W(\bm{y},t_0) e^{-D \norm{\bm{k}}^2 (t-t_0)} =\\
        &= \frac{1}{(2 \pi)^d} \int_{\mathbb{R}^d} \dd[d]{\bm{k}\int_{\mathbb{R}^d} \dd[d]{\bm{y}}  } \exp\left(-\underbrace{D (t-t_0) }_{a}\norm{\bm{k}}^2+ \underbrace{i (\bm{x} - \bm{y})}_{\bm{b}} \cdot\bm{k} \right) W(\bm{y},t_0)
    \end{align*}
    Note that the exponential is a gaussian integral in $\bm{k}$, with all dimensions $k_i$ independent from each other. So, for every $i = 1,\dots,d$:
    \begin{align*}
        \frac{1}{2 \pi} \int_{\mathbb{R}} \dd{k_i} \exp\left(-a k_i^2 + b_i k_i\right) = \frac{1}{2 \pi} \sqrt{\frac{\pi}{a} } \exp\left(\frac{b^2_i}{4a} \right) = \span \\
        &= \frac{1}{\sqrt{4 \pi D (t-t_0)}} \exp\left(-\frac{(x_i-y_i)^2}{4 D (t-t_0)} \right)
    \end{align*}
    And multiplying all together:
    \begin{align*}
        W(\bm{x},t) = \frac{1}{[4 \pi D(t-t_0)]^{d/2}} \int_{\mathbb{R}^d} \dd[d]{\bm{y}} \exp\left(-\frac{\norm{\bm{x} - \bm{y}}^2}{4 D (t-t_0)} \right) W(\bm{y},t_0)
    \end{align*}
    This is the general solution for any initial $W(\bm{y},t_0)$.

    \begin{enumerate}
        \item For the propagator $W(\bm{x},t|\bm{x_0},t_0)$, choose $W(\bm{y},t_0) = \delta^d(\bm{x}-\bm{x_0})$, leading to:
        \begin{align} \nonumber
            W(\bm{x},t|\bm{x_0},t_0) &=     \frac{1}{[4 \pi D(t-t_0)]^{d/2}} \int_{\mathbb{R}^d} \dd[d]{\bm{y}} \exp\left(-\frac{\norm{\bm{x} - \bm{y}}^2}{4 D (t-t_0)} \right) \delta^d(\bm{y} -\bm{x_0}) =\\ \label{eqn:prop1}
            &= \frac{1}{[4 \pi D(t-t_0)]^{d/2}} \exp\left(-\frac{\norm{\bm{x} - \bm{x_0}}^2}{4 D (t-t_0)} \right)
        \end{align}
        \item Note that (\ref{eqn:prop1}) is a multivariate gaussian with mean $\bm{x_0}$ and diagonal covariance matrix will all entries equal to $2 D(t-t_0)$. So, immediately:
        \begin{align*}
            \langle \bm{x} \rangle_t &= \bm{x_0}\\
            \langle x_i x_j \rangle_t &= 2D (t-t_0) \delta_{ij}
        \end{align*}
        Meaning that:
        \begin{align*}
            \langle \norm{\bm{x}}^2 \rangle_t = \langle \sum_{i=1}^d x_i^2\rangle = 2D(t-t_0)d
        \end{align*}
            \end{enumerate}
    
\end{exo}

\begin{exo}
    Deduce the analogous of the following equation:
    \begin{align*}
        W(x,t|0,0) = \lambda W(\lambda x, \lambda^2 t|0,0)
    \end{align*}
    for the $d$-dimensional case of the previous exercise.

    \medskip

    \textbf{Solution}. Starting from the diffusion equation:
    \begin{align*}
        \partial_t W(\bm{x},t) = D \nabla^2 W(\bm{x},t)
    \end{align*}
    Note that if $W(\bm{x},t)$ is a solution, then also $W(\lambda\bm{x}, \lambda^2 t)$ is a solution. In fact, let $\bm{x'} = \lambda \bm{x}$ and $t' = \lambda^2 t$. Then:
    \begin{align*}
        \partial_t W(\bm{x'},t') = \partial_{t'} W(\bm{x'}, t') \underbrace{\dv{t'}{t}}_{\lambda^2} = D \nabla^2_{\bm{x}} W(\bm{x'},t') = D (\nabla_{\bm{x'}}^2 W(\bm{x'},t))\underbrace{ \nabla^2_{\bm{x}} \bm{x'}}_{\lambda^2} 
    \end{align*}
    And so:
    \begin{align*}
        \partial_{t'} W(\bm{x'},t') = D \nabla_{\bm{x'}}^2 W(\bm{x'},t')
    \end{align*}
    which is verified because $W(\bm{x},t)$ is a solution by hypothesis.

    \medskip

    However, interpreting $W(\bm{x},t)$ as a probability distribution, we need to enforce the normalization (this makes no difference in the solution, because by linearity of the PDE, if $f(\bm{x},t)$ is a solution, also $a f(\bm{x},t)$ with $a \in \mathbb{R}$ is a solution). So:
    \begin{align*}
        1 &\overset{!}{=}  A \int_{\mathbb{R}^d} \dd[d]{\bm{x}} W(\lambda \bm{x}, \lambda^t|0,0) = \frac{A}{[4 \pi D \lambda^2 t]^{d/2}}  \int_{\mathbb{R}^d} \dd[d]{\bm{x}} \exp\left(-\frac{\norm{\lambda \bm{x}}^2}{4 Dt} \right) =\\
        &= \frac{A}{[4 \pi D \lambda^2 t]^{d/2}}  \left(\frac{4 \pi D t}{\lambda^2} \right)^{d/2} = \frac{A}{\lambda^d} \Rightarrow A = \lambda^d 
    \end{align*}

    And so the correct relation is:
    \begin{align*}
        W(\bm{x},t|0,0) = \lambda^d W(\lambda \bm{x}, \lambda^2 t|0,0)
    \end{align*}
\end{exo}

\begin{exo}
    Prove by a direct calculation that the propagator:
    \begin{align}
        W(x,t|x_0,t_0) = \frac{1}{\sqrt{4 \pi D (t-t_0)}}  \exp\left(-\frac{(x - x_0)^2}{4D (t-t_0)} \right) \qquad t> t_0 \label{eqn:propagator1}
    \end{align}
    satisfies the ESCK relation:
    \begin{align} \label{eqn:esck}
        W(x,t|x_0,t_0) = \int_{\mathbb{R}} \dd{x'} W(x,t|x',t') W(x',t'|x_0,t_0) \qquad t_0 < t' < t  
    \end{align}
    \medskip

    \textbf{Solution}. Substituting (\ref{eqn:propagator1}) in (\ref{eqn:esck}) leads to:
    \begin{align*}
        W(x,t|x_0,t_0) &= \underbrace{\frac{1}{\sqrt{4 \pi D (t-t')}} \frac{1}{\sqrt{4 \pi D(t'-t_0) }}}_{\mathcal{N}} \cdot \\
        &\quad \> \cdot \int_{\mathbb{R}} \dd{x'}    \exp\left(-\frac{(x'-x_0)^2}{4 D (t'-t_0)} -\frac{(x-x')^2}{4D(t-t')} \right)
    \end{align*}
    To make notation easier, let $t-t' \equiv \Delta t$ and $t'-t_0 \equiv \Delta t'$, with $\Delta t + \Delta t' = t-t_0$. Merging the fractions:
    \begin{align*}
        W(x,t|x_0,t_0) = \mathcal{N} \int_{\mathbb{R}} \dd{x'} \exp\left(-\frac{\Delta t (x'-x_0)^2 + \Delta t'(x-x')^2}{4 D \Delta t \Delta t'} \right) = \span\\
        &= \mathcal{N} \int_{\mathbb{R}} \dd{x'} \exp\left(-\frac{(x')^2[\Delta t + \Delta t'] + x'[-2 \Delta t x_0 - 2 \Delta t' x ] + \Delta t x_0^2 + \Delta t' x^2}{4 D \Delta t \Delta t'} \right) =\\
        &= \mathcal{N} \int_{\mathbb{R}} \dd{x'} \exp\left(-ax^2 + bx + c\right) = \mathcal{N} \sqrt{\frac{\pi}{a} } \exp\left(-\frac{b^2}{4a} + c \right)
    \end{align*}
    with:
    \begin{align*}
        a = \frac{t-t_0}{4 D \Delta t \Delta t'}; \quad b = 2\frac{x_0 \Delta t + x \Delta t'}{4D \Delta t \Delta t'}; \qquad c = -\frac{\Delta t x_0^2 + \Delta t' x^2 }{4D \Delta t \Delta t'}   
    \end{align*}
    The normalization term is:
    \begin{align*}
        \mathcal{N} \sqrt{\frac{\pi}{a} } = \frac{1}{\sqrt{4 \pi D(t-t_0) }} 
    \end{align*}
    While the exponential argument becomes:
    \begin{align*}
        -\frac{4 [x_0 \Delta t + x \Delta t']^2}{(4D \Delta t \Delta t' )^2} \frac{4D \Delta t \Delta t'}{4 (t-t_0)}  - \frac{\Delta t x_0^2 + \Delta t' x^2}{4D \Delta t \Delta t'} = \span \\
        &= -\frac{x_0^2 \Delta t^2 + x^2 (\Delta t')^2 + 2x x_0 \Delta t \Delta t' - \overbrace{(t-t_0)}^{\Delta t+ \Delta t'}[\Delta t x_0^2 + \Delta t' x^2]  }{4D \Delta t \Delta t' (t-t_0)} =\\
        &=-\frac{2x x_0 \cancel{\Delta t \Delta t' }- \cancel{\Delta t \Delta t' }x^2 - \cancel{\Delta t' \Delta t} x_0^2  }{4D \cancel{\Delta t \Delta t'} (t-t_0)} = - \frac{(x-x_0)^2}{4D (t-t_0)} 
    \end{align*}
    And so the complete solution is indeed:
    \begin{align*}
        W(x,t|x_0,t_0) = \frac{1}{\sqrt{4 \pi D (t-t_0)}}  \exp\left(-\frac{(x - x_0)^2}{4D (t-t_0)} \right) \qquad 
    \end{align*}
\end{exo}

\begin{exo}
    Compute the expected value for the functional:
    \begin{align}
        F\left(\int_0^t a(\tau) x(\tau) \dd{\tau }\right) \label{eqn:func1}
    \end{align}
    with $a(\tau) = \delta(\tau - t')$ for $0<t'<t$ and $F(z) = \delta (z-x)$. Is this a known result?

    \medskip

    \textbf{Solution}. Substituting $a(\tau) = \delta(\tau - t')$ and $F(z) = \delta(z-x)$ in (\ref{eqn:func1}) we get:
    \begin{align*}
        F\left(\int_0^t a(\tau) x(\tau) \dd{\tau}\right) &= \delta\left(\int_0^t \delta(\tau - t') x(\tau) \dd{\tau} -x\right) = \qquad 0<t'<t\\
        &= \delta(x(t') - x)
    \end{align*}
    So the expected value is just the transition probability after $t'$, which is a known result:
    \begin{align*}
        W(x,t'|0,0) = \langle \delta(x - x(t')) \rangle_W = \frac{1}{\sqrt{4 \pi D t'}} \exp\left(-\frac{x^2}{4D t'} \right) 
    \end{align*}

    Let's confirm this by doing the full calculation. We already evaluated $\langle F \rangle$ in the general case (with $D=1/4$), obtaining:

    \begin{align} \label{eqn:formula-func}
        \langle F\left(\int_0^t a(\tau) x(\tau) \dd{\tau}\right) \rangle_W &= \sqrt{\frac{1}{\pi R(t)} } \int_{\mathbb{R}} \dd{z} F(z) \exp\left(-\frac{z^2}{R(t)} \right);\\ \nonumber
         \quad R(t) &\equiv \int_0^t \dd{\tau} \left(\int_\tau^t a(s) \dd{s}\right)^2
    \end{align}
    Substituting the values for $a$ and $F$ we get:
    \begin{align*}
        R(t) = \int_0^t \dd{\tau} \left(\int_\tau^t \delta(s-t') \dd{s}\right)^2 \qquad 0<t'<t
    \end{align*}
    Note that:
    \begin{align*}
        \int_\tau^t \delta(s-t')\dd{s} = \begin{cases}
            1 & \tau < t' < t\\
            0 & \text{otherwise}
        \end{cases}
    \end{align*}
    So it is convenient to split the \textit{outer} integral at $t'$:
    \begin{align*}
        R(t) &= \int_0^{t'} \dd{\tau} \underbrace{\left(\int_\tau^t \delta(s-t') \dd{s}\right)^2}_{0<\tau<t'}  + \int_{t'}^t \dd{\tau} \underbrace{\left(\int_\tau^t \delta(s-t') \dd{s}\right)^2}_{t'<\tau<t} =\\
        &=\int_0^{t'} \dd{\tau} 1^2 + \int_{t'}^t \dd{\tau} 0^2 = t'
    \end{align*} 
    And substituting back in (\ref{eqn:formula-func}):
    \begin{align*}
        \langle \delta(x-x(t')) \rangle_W &= \frac{1}{\sqrt{\pi t'}}  \int_{\mathbb{R}} \dd{z} \delta(z-x) \exp\left(-\frac{z^2}{t'} \right) =\\
        &= \frac{1}{\sqrt{ \pi t' }}  \exp\left(-\frac{x^2}{t'} \right)
    \end{align*} 
    To retrieve $D$ we substitute $t' \to 4 D t'$, leading to the same result as before:
    \begin{align*}
        \langle \delta(x-x(t')) \rangle_W = \frac{1}{\sqrt{4 \pi D t'}}  \exp\left(-\frac{x^2}{4 D t'} \right)
    \end{align*}
\end{exo}

\begin{exo}
    Using the Wiener measure explain what the following average means:
    \begin{align*}
       C= \langle \delta(x_1 - x(t_1)) \delta(x_2 - x(t_2)) \cdots \delta(x_n - x(t_n)) \rangle_W \quad 0<t_1 <t_2 < \cdots < t_n < t
    \end{align*}

    \medskip

    \textbf{Solution}. $C$ represents the probability for a path starting in $x=0$ at $t=0$ to pass trough all $x_i$ at the consecutive instants $t_i$.

    To see this, we expand the average using the Wiener measure:
    \begin{align*}
        C = \int_{\mathcal{C}\{0,0;t\}} \dd{_Wx(\tau)} \prod_{i=1}^n\delta(x_i-x(t_i)) 
    \end{align*} 
    where the integral is over all the paths with fixed starting point $(0,0)$ reaching any point $\in \mathbb{R}$ at $t$. Each delta \q{fixes} a point in a path, in the sense that it nullifies the integral for every path that does not traverse $x_i$ at time $t_i$. So we can rewrite $C$ as a product of transition probabilities:
    \begin{align*}
        C&=\int_{\mathcal{C}\{0,0;x_1,t_1\}} \dd{_Wx(\tau)} \int_{\mathcal{C}\{x_1,t_1;x_2,t_2\}} \dd{_Wx(\tau)} \cdots \int_{\mathcal{C}\{x_{n-1},t_{n-1};x_n,t_n\}}\dd{_Wx(\tau)} =\\
        &= W(x_1,t_1|x_0,t_0) W(x_2,t_2|x_1,t_1) \cdots W(x_{n-1},t_{n-1}|x_n,t_n)
    \end{align*}
    Which is, in fact, the probability of a path traversing all these points.
\end{exo}

\begin{exo}
    Determine the following average:
    \begin{align*}
        J(x) = \langle \exp\left(-ik^2 \int_0^t  \dd{\tau} x^2(\tau)\delta(x-x(t))\right) \rangle_W
    \end{align*}
    by using the results from the Gelfand-Yoglom method with the initial condition $x(0)=0$. Determine also the normalization:
    \begin{align*}
       \mathcal{N} = \int_{\mathbb{R}} J(x) \dd{x}
    \end{align*}

    \medskip

    \textbf{Solution}. We already computed the average of the exponential functional with fixed endpoint:
    \begin{align*}
        \langle \exp\left(-\int_0^t P(\tau) x^2(\tau) \dd{\tau}\right) \delta(x-x(t)) \rangle_W = \frac{1}{\sqrt{\pi \tilde{D}(0)}} \exp\left(-x^2 \frac{D(0)}{\tilde{D}(0)} \right)
    \end{align*}
    where $D(t)$ and $\tilde{D}(t)$ are solutions of the following differential equations:
    \begin{align*}
        \tilde{D}''(\tau) &= P(\tau) \tilde{D}(\tau) \qquad \begin{dcases}
            \tilde{D}(t) = 0\\
            \dv{\tilde{D}(\tau)}{\tau}\Big|_{\tau = t} = -1
        \end{dcases}\\
        D''(\tau) &= P(\tau) D(\tau) \qquad \begin{dcases}
            {D}(t) = 1\\
            \dv{{D}(\tau)}{\tau}\Big|_{\tau = t} = 0
        \end{dcases}
    \end{align*}

    In this case $P(\tau) = ik^2$ and so we need to solve:
    \begin{align}
        D''(\tau) = ik^2 D(\tau) \label{eqn:diff-eq1}
    \end{align}
    with the two different sets of initial conditions. Note that this equation is really similar to that of the harmonic repulsor, and so we search a solution as a linear combination of exponentials:
    \begin{align*}
        D(\tau) = A e^{\alpha k \tau } + B e^{-\alpha k \tau}
    \end{align*}
    Substituting in (\ref{eqn:diff-eq1}) leads to:
    \begin{align*}
        \alpha^2 k^2 (A e^{\alpha k \tau} + B e^{-\alpha k \tau}) = ik^2(A e^{\alpha k \tau} + B e^{-\alpha k \tau})
    \end{align*}
    And so $\alpha^2 = i \Rightarrow \alpha = \sqrt{i} = \exp(i \pi/4)$.

    Now it's just a matter of applying the two sets of initial conditions. Instead of re-doing all the computations, recall the solutions we got for the case $P(\tau) = k^2$:
    \begin{align*}
        \tilde{D}(\tau) = \frac{1}{k} \sinh(k(t-\tau)) \qquad D(\tau) = \cosh(k(t-\tau))
    \end{align*}
    We can get the ones for the case $P(\tau) = ik^2$ by substituting $k \to \alpha k$:
    \begin{align*}
        \tilde{D}(\tau) = \frac{1}{\alpha k} \sinh(\alpha k(t- \tau)) \qquad D(\tau) = \cosh(\alpha k(t-\tau)) 
    \end{align*}

    And substituting back we arrive at the desired solution:
    \begin{align*}
        J(x) = \sqrt{\frac{\alpha k}{\pi \sinh(\alpha k t)}} \exp\left(-\alpha k x^2 \coth (\alpha k t) \right)
    \end{align*}

    All that's left is to compute the normalization, which is just a gaussian integral:
    \begin{align*}
        \mathcal{N}&=\sqrt{\frac{\alpha k}{\pi \sinh(\alpha k t)}} \int_{\mathbb{R}} \dd{x}    \exp(-\alpha k \coth(\alpha k t) x^2) = \sqrt{\frac{\alpha k}{\pi \sinh(\alpha k t)}} \sqrt{\frac{\pi}{\alpha k \coth(\alpha k t) } } =\\
        &= \frac{1}{\sqrt{\sinh(\alpha k t )}} \sqrt{\frac{\sinh(\alpha k t)}{\cosh(\alpha k t )} } = \frac{1}{\sqrt{\cosh (\alpha k t)}} 
    \end{align*}
\end{exo}

\begin{exo}
    Determine the average:
    \begin{align}
        K(a,k) = \langle \exp\left(-\int_0^t [a[\dot{x}(\tau)]^2 + ik \dot{x}(\tau)]\dd{\tau}\right) \rangle_W  \label{eqn:Kdef}
    \end{align}
    where $a$ and $k$ are arbitrary (real) constants. How this result can be used to determine $\langle \delta(x-x(t)) \rangle_W$?

    \medskip

    \textbf{Solution}. Let's introduce a discretization $\{t_j\}_{j=1,\dots,n}$, and the usual notation $f_i \equiv f(t_i)$ and $\Delta f = f_{i} - f_{i-1}$. Let also $D=1/4$ to simplify notation.

    For a fixed $a, k \in \mathbb{R}$, $K(a,k)$ is the \textit{continuum} limit of the discretized integral:
    \begin{align*}
        K(a,k) &= \lim_{n \to \infty} K_n(a,k)\\
        K_n(a,k) &= \int_{\mathbb{R}^n} \left(\prod_{i=1}^n \frac{\dd{x_i}}{\sqrt{\pi \Delta t_i}} \right) \exp \left(-\sum_{i=1}^n \frac{(x_i - x_{i-1})^2}{\Delta t_i} \right) \cdot \\
        &\quad \> \cdot \exp\left(-\sum_{i=1}^n \left[ \frac{a(x_i - x_{i-1})^2}{\Delta t_i^2}  + \frac{ik \Delta x_i}{\Delta t_i} \right] \Delta t_i\right)
    \end{align*} 
    Note that:
    \begin{align*}
        \sum_{i=1}^n \Delta x_i = x_n - x_0 = x_n
    \end{align*}
    where we assume $x_0 = 0$ for simplicity. This leads to:
    \begin{align*}
        K_n(a,k) &= \int_{\mathbb{R}^n} \left(\prod_{i=1}^n \frac{\dd{x_i}}{\sqrt{\pi \Delta t_i}} \right) \exp \left(-\sum_{i=1}^n \frac{(x_i - x_{i-1})^2}{\Delta t_i} \right) \cdot\\
        &\quad \> \cdot \exp \left(-\sum_{i=1}^n \frac{a (x_i - x_{i-1})^2}{\Delta t_i} + \hlc{Yellow}{i k x_n}\right)
    \end{align*}
    This \textit{is} a multivariate gaussian integral. We can \textit{decouple} all the $n$ gaussian with a change of variables, that is suggested by the most immediate simplification $(x_i - x_{i-1})^2 = y_i^2$:
    \begin{align*}
        x_i - x_{i-1} = y_i
    \end{align*} 
    Note that:
    \begin{align*}
        \sum_{j=1}^i y_j = y_1+\dots + y_i = \cancel{x_1}-\underbrace{x_0 }_{0}+ \bcancel{x_2}-\cancel{x_1 }+ \dots + x_i - \cancel{x_{i-1}} = x_i \quad 1 \leq i \leq n
    \end{align*}
    This allows to compute the jacobian:
    \begin{align*}
        \operatorname{det}\left|\pdv{\{x_i\}}{\{y_j\}} \right| = \operatorname{det} \left|\left(\begin{array}{cccc}
        1 & 0 & \cdots & 0 \\ 
        1 & \ddots & \ddots & \vdots \\ 
        \vdots & \ddots & \ddots & 0 \\ 
        1 & \cdots & 1 & 1
        \end{array}\right) \right|_{n\times n} = 1
    \end{align*}
    Leading to:
    \begin{align*}
        K_n(a,k) &= \int_{\mathbb{R}^n}  \left(\prod_{i=1}^n \frac{\dd{y_i}}{\sqrt{\pi \Delta t_i}} \right) \exp\left(-\sum_{i=1}^n \frac{y_i^2 (1+a)}{\Delta t_i}  + \hlc{Yellow}{ik \sum_{i=1}^n y_i}\right)
    \end{align*}
    Now the gaussians are decoupled, and we can move the product outside the integral:
    \begin{align}
        K_n(a,k) &= \prod_{i=1}^n \int_{\mathbb{R}} \frac{\dd{y_i}}{\sqrt{\pi \Delta t_i}} \exp\Bigg(-\underbrace{\frac{(1+a)}{\Delta t_i} }_{A}y_i^2 + \underbrace{ik}_{b}y_i \Bigg) \label{eqn:orig} \\ \nonumber
        &= \prod_{i=1}^n \sqrt{\frac{\pi}{A} } \exp\left(\frac{b^2}{4A} \right) = \prod_{i=1}^n \frac{1}{\sqrt{1+a}}  \exp\left(-\frac{k^2}{4(1+a)}\Delta t_i\right) =\\ \nonumber
        &= \left(\frac{1}{\sqrt{1+a}} \right)^n \exp\left(-\frac{k^2}{4(1+a) } \sum_{i=1}^n \Delta t_i \right) =\\ \label{eqn:final}
        &= \left(\frac{1}{\sqrt{1+a}} \right)^n \exp\left(-\frac{k^2 t}{4(1+a)} \right)
    \end{align}
    Taking the continuum limit $n \to \infty$ leads to different results depending on the value of $a$. 
    
    Note that (\ref{eqn:final}) works only for $a > -1$, and for the other cases we need to resort to a previous step (\ref{eqn:orig}). If $a < -1$, the integral diverges, and if $a = -1$ it is a product of $\delta(k)$. We omit an extended discussion of these degenerate cases.


    For $a > -1$, by inspecting (\ref{eqn:final}) we arrive to:
    \begin{align*}
        K(a,k) = \begin{cases}
            \infty & -1 < a < 0\\
            \exp\left(-\frac{k^2 t}{4} \right) & a = 0\\
            0 & a > 0
        \end{cases}
    \end{align*}

    We can use (\ref{eqn:final}) to evaluate $\langle \delta(x-x(t)) \rangle_W$. Recall the Dirac $\delta$ definition:
    \begin{align*}
        \delta(x) = \mathbb{F}^{-1}[1](x) = \frac{1}{2 \pi} \int_{\mathbb{R}} e^{ikx} \dd{k}  
    \end{align*}
    Then:
    \begin{align}
        \langle \delta(x-x(t)) \rangle_W = \langle \frac{1}{2\pi} \int_{\mathbb{R}} \dd{k} e^{ik(x-x(t))}  \rangle \label{eqn:delta1}
    \end{align}
    Compare this last expression with the starting integral (\ref{eqn:Kdef}). Note that, if we set $a=0$, we get:
    \begin{align*}
        K(0,k) = \langle- \int_0^t ik \dot{x}(\tau) \dd{\tau} \rangle = \langle \exp(-ik x(\tau)) \rangle \overset{(\ref{eqn:final})}{=} \exp\left(-\frac{k^2 t}{4} \right)
    \end{align*}
    Substituting in (\ref{eqn:delta1}) and exchanging the integral on $\dd{k}$ with the average (thanks to linearity):
    \begin{align*}
        \langle \delta(x-x(t)) \rangle_W &= \frac{1}{2 \pi} \int_{\mathbb{R}} \dd{k} e^{ikx} \underbrace{\langle e^{-ikx(t)} \rangle_W}_{K(0,k)} = \frac{1}{2 \pi} \int_{\mathbb{R}} \dd{k} \exp\Bigg(-\underbrace{\frac{t}{4}}_{A} k^2 +\underbrace{ix}_{b}k\Bigg) =\\
        &= \frac{1}{2 \pi} \sqrt{\frac{\pi}{A} }\exp\left(\frac{b^2}{4A}  \right) = \frac{1}{2\pi} \sqrt{\frac{4\pi}{t} } \exp\left(-\frac{4x^2}{4t} \right) =\frac{1}{\sqrt{\pi t}} \exp\left(-\frac{x^2}{t} \right)
    \end{align*}
    Which is the same result as previously derived.
\end{exo}

\begin{exo}
    Show that:
    \begin{align*}
        \mathbb{P}(|\Delta x| < \epsilon) = \lim_{\Delta t \to 0^+} \int_{|\Delta x| < \epsilon } \frac{\dd{\Delta x}}{\sqrt{4 \pi D \Delta t }} \exp\left(-\frac{(\Delta x)^2}{4D \Delta t} \right)=1 \qquad \forall \epsilon> 0 
    \end{align*}
    We have used this result to argue that Brownian trajectories are continuous with probability $1$.

    \medskip

    \textbf{Solution}.
    Consider a discretization $\{t_j\}_{j=1,\dots,n}$, with $\Delta t_i = t_i - t_{i-1}$ and $x_i \equiv x(t_i)$. Then, for every $i$:
    \begin{align*}
        \mathbb{P}(|\Delta x_i| < \epsilon) &= \mathbb{P}(x_{i-1} - \epsilon < x_i < x_{i-1}+\epsilon|x(t_{i-1}) = x_{i-1}) =\\
        &= \int_{x_{i-1} - \epsilon}^{x_{i-1} + \epsilon} \frac{\dd{x_i}}{\sqrt{4 \pi D \Delta t_i}}  \exp\left(-\frac{(x_i - x_{i-1})^2}{4D \Delta t_i} \right) =\\
        &\underset{(a)}{=}  \int_{-\epsilon}^{+\epsilon} \frac{\dd{\Delta x_i}}{\sqrt{4 \pi D \Delta t_i}}  \exp\left(-\frac{[\Delta x_i]^2}{4 D \Delta t_i} \right) 
    \end{align*}
    where in (a) we translated the variable of integration $\Delta x_i = x_i - x_{i-1}$.

    To compute the final integral, we perform another change of variable:
    \begin{align*}
        \frac{[\Delta x_i]^2}{\Delta t_i} = z^2 \Rightarrow z = \frac{\Delta x_i }{\sqrt{ \Delta t_i}}    \Rightarrow \dd{\Delta x_i} = \dd{z} \sqrt{\Delta t_i}
    \end{align*} 
    Leading to:
    \begin{align*}
        \mathbb{P}(|\Delta x_i| < \epsilon) = \int_{-\epsilon/\sqrt{\Delta t_i}}^{+\epsilon/\sqrt{\Delta t_i}} \dd{z} \frac{\cancel{\sqrt{\Delta t_i}}}{\sqrt{4 \pi D \cancel{\Delta t_i}}} \exp\left(-\frac{z^2}{4D} \right) 
    \end{align*}
    In the continuum limit $\Delta t_i \to 0^+$ we get:
    \begin{align*}
        \lim_{\Delta t_i \to 0^+} \mathbb{P}(|\Delta x_i| < \epsilon) = \int_{-\infty}^{+\infty} \frac{\dd{z}}{\sqrt{4 \pi D}}  \exp\left(-\frac{z^2}{4 D} \right) = 1
    \end{align*}
\end{exo}

\begin{exo}
    Show that:
    \begin{align*}
        \mathbb{P}\left(\left|\frac{\Delta x}{\Delta t}  \right| > k\right) = \lim_{\Delta t \to 0^+} \int_{|\Delta x| > k \Delta t} \frac{\dd{\Delta x }}{\sqrt{4 \pi D \Delta t}}\exp\left(-\frac{(\Delta x)^2}{4 D \Delta t} \right) = 1 \qquad \forall k > 0
    \end{align*}
    We have used this result to argue that Brownian trajectories are never differentiable with probability $1$. 

    \medskip

    \textbf{Solution}. As in the previous exercise, consider a time discretization $\{t_j\}_{j=1,\dots,n}$ and the usual notation. 
    We then consider the related probability in the discretized path. First, to have a unique integration domain we rewrite the probability in terms of the converse event:
    \begin{align*}
        \mathbb{P}\left(\left|\frac{\Delta x_i}{\Delta t_i }  \right| > k\right) = 1- \mathbb{P}\left(\left|\frac{\Delta x_i}{\Delta t_i }  \right| \leq k\right) = 1-\int_{-k \Delta t_i}^{+k \Delta t_i} \frac{\dd{\Delta x_i}}{\sqrt{4 \pi D \Delta t_i}} \exp\left(-\frac{\Delta x_i^2}{4 D \Delta t_i} \right)
    \end{align*}

    Consider again the change of variable:
    \begin{align*}
        \frac{[\Delta x_i]^2}{\Delta t_i} = z^2 \Rightarrow z = \frac{\Delta x_i }{\sqrt{ \Delta t_i}}    \Rightarrow \dd{\Delta x_i} = \dd{z} \sqrt{\Delta t_i}
    \end{align*}
    leading to:
    \begin{align*}
        \mathbb{P}\left(\left|\frac{\Delta x_i}{\Delta t_i }  \right| > k\right) = 1 - \int_{-k\sqrt{\Delta t_i}}^{+k \sqrt{\Delta t_i}} \frac{\dd{z}}{\sqrt{4 \pi D}}  \exp\left(-\frac{z^2}{4 D} \right) 
    \end{align*}
    Note that in the continuum limit $\Delta t_i \to 0^+$ the integral domain vanishes, while the integrand remains the same, meaning that the whole integral will go to $0$. So:
    \begin{align*}
        \lim_{\Delta t_i \to 0^+} \mathbb{P}\left(\left|\frac{\Delta x_i}{\Delta t_i }  \right| > k\right)  = 1 -0 = 1
    \end{align*}
\end{exo}

\setcounter{chapter}{3}
\chapter{Fokker-Planck Equation and Stochastic Processes}

\begin{exo}
    Show that the Master Equation:
    \begin{align}\label{eqn:me1}
        w_i(t_{n+1}) = \sum_{j} W_{ij}(t_n) w_j(t_n) \qquad 1 = \sum_{i} W_{ij}(t_n)
    \end{align}
    preserves the normalization, i.e. if:
    \begin{align*}
        \sum_{i} w_i(t_n) = 1
    \end{align*}
    when $n=0$, then it holds for all $n>0$. Idem for $\int \dd{x} w(x,t_n)$.

    \medskip

    \textbf{Solution}. By induction, we prove that if the normalization holds for $n$, then it holds for $n+1$. Then, as it holds for $n=0$, it must hold for any $n$.
    \begin{align*}
        \sum_{i} w_i(t_{n+1}) &\underset{(\ref{eqn:me1})}{=}   \sum_i \sum_j W_{ij}(t_n) w_j(t_n) =\\
        &= \sum_j w_j(t_n) \underbrace{\sum_i W_{ij}(t_n)}_{1}  = \sum_j w_j(t_n) = 1
    \end{align*}

    In the case of $\int \dd{x} w(x,t_n)$, the same property holds for all the Riemann sums, and so it holds in the continuum limit.
\end{exo}

\begin{exo}
Show that:
\begin{align}\label{eqn:jump}
    W(z|x,t) = F\left(\frac{z-\epsilon f(x,t)}{\sqrt{\epsilon \hat{D}(x,t)}} \right) \frac{1}{\sqrt{\epsilon \hat{D}(x,t)}} \qquad \int_{\mathbb{R}} \dd{y} F(y) = 1
\end{align}
imply the normalization condition:
\begin{align*}
    \int_{\mathbb{R}} \dd{z} W(+z|x,t_n) = 1
\end{align*}   

\medskip

\textbf{Solution}. By direct computation:
\begin{align*}
    \int_{\mathbb{R}} \dd{z} W(z|x,t) = \frac{1}{\sqrt{\epsilon \hat{D}(x,t)}} \int_{\mathbb{R}} \dd{x} F\left(\frac{z-\epsilon f(x,t)}{\sqrt{\epsilon \hat{D}(x,t)}} \right) \underset{(a)}{=} \int_{\mathbb{R}} \dd{y} F(y) = 1
\end{align*} 
where in (a) we performed the change of variables:
\begin{align} \label{eqn:change}
    y = \frac{z-\epsilon f(x,t)}{\sqrt{\epsilon \hat{D}(x,t)}}  \qquad \dd{z} = \sqrt{\epsilon \hat{D}(x,t)} \dd{y} 
\end{align}
\end{exo}

\begin{exo}
    Show the ansatz (\ref{eqn:jump}) together with:
    \begin{align*}
        \int_{\mathbb{R}} \dd{y} y F(y) = 0
    \end{align*} 
    imply:
    \begin{align*}
        \int_{\mathbb{R}} \dd{z} z W(z|x,t) &= \epsilon f(x,t)\\
        \int_{\mathbb{R}} \dd{z} z^2 W(z|x,t) &= \epsilon \hat{D}(x,t) \int_{\mathbb{R}} \dd{y} y^2 F(y) + O(\epsilon^2)\\
        \int_{\mathbb{R}} \dd{z} z^k W(z|x,t) &= O(\epsilon^{k/2}) \qquad k\geq 3
    \end{align*}

    \medskip

    \textbf{Solution}. For the first moment:
    \begin{align*}
        \langle z \rangle &= \mu_1(x,t) = \int_{\mathbb{R}} \dd{z} z F\left(\frac{z - \epsilon f(x,t)}{\sqrt{\epsilon \hat{D}(x,t)}} \right) \frac{1}{\sqrt{\epsilon \hat{D}(x,t)}}  =\\
        &\underset{(\ref{eqn:change})}{=} \int_{\mathbb{R}} \dd{y} \left(\epsilon f(x,t) + y \sqrt{\epsilon \hat{D}(x,t)}\right) F(y) =\\
        &= \epsilon f(x,t) \underbrace{\int_{\mathbb{R}} F(y) \dd{y} }_{1}+ \sqrt{\epsilon \hat{D}(x,t)} \underbrace{\int_{\mathbb{R}} \dd{y} F(y)}_{0} = \epsilon f(x,t)
    \end{align*} 
    For the $n$-th order moment, with $n>1$, we use Newton's binomial formula:
    \begin{align*}
        \mu_n &= \int_{\mathbb{R}} \dd{z} z^n W(z|x,t) \underset{(\ref{eqn:change})}{=} \int_{\mathbb{R}} \dd{y} \left(\epsilon f(x,t) + y \sqrt{\epsilon \hat{D}(x,t)}\right)^n F(y) =\\
        &= \int_{\mathbb{R}} \dd{y} F(y) \sum_{k=0}^n {n \choose k} (\epsilon f)^{n-k} (y^2 \epsilon \hat{D})^{k/2} =\\
        &= \int_{\mathbb{R}} \dd{y} F(y) \sum_{k=0}^{n} {n\choose k} f^{n-k} (y^2 \hat{D})^{k/2} \epsilon^{n-k+k/2}
    \end{align*}
    For $n=2$ we get:
    \begin{align*}
        \mu_2 &= \epsilon \hat{D}(x,t) \int_{\mathbb{R}} \dd{y} y^2 F(y) + 2 \epsilon \sqrt{\epsilon} \hat{D} \underbrace{\int_{\mathbb{R}} \dd{y} y F(y)}_{0} +\underbrace{ \epsilon^2 f^2}_{ O(\epsilon^2)}\underbrace{ \int_{\mathbb{R}} \dd{y} F(y)}_{1} =\\
        &= \epsilon \hat{D}(x,t) \int_{\mathbb{R}} \dd{y} y^2 F(y) + O(\epsilon^2)
    \end{align*}
    For $k > 2$, note that the exponent of the $\epsilon$ is:
    \begin{align*}
        n - k + \frac{k}{2} = \frac{2n-k}{2} \geq \frac{k}{2} > 1   
    \end{align*}
    And so all terms are of order $O(\epsilon^{k/2})$.
\end{exo}

\begin{exo}
    Consider a spherical particle of radius $r$ subjected to the collisions of an ideal gas of $N$ particles in a volume $V$ at equilibrium at temperature $T$. If the number of collisions during a time interval $\Delta t$ satisfies the central limit theorem, determine the average number of collisions and its variance in a time interval $\Delta t$.

    \medskip

    \textbf{Solution}. Consider an infinitesimal surface $\dd{\Sigma}$ with normal $\hat{\bm{n}}$. Suppose that $\dd{\Sigma}$ is immersed in a fluid, and that \textit{locally} around that surface, the particles are travelling with velocity $\bm{v}$. Then the particles are approaching $\dd{\Sigma}$ with the \textit{perpendicular} component of their velocity, i.e. $\bm{\bm{v}}\cdot\hat{\bm{n}}$. So, in a small time interval $\dd{t}$, all the particles that traverse $\dd{\Sigma}$ are contained in the volume $\bm{v} \cdot \hat{\bm{n}} \dd{t} \dd{\Sigma}$. Multiplying by the local density $\rho$ of particles, we obtain:
    \begin{align*}
        \dd{N} = \bm{v} \cdot \hat{\bm{n}} \dd{t} \dd{\Sigma} \rho
    \end{align*}
    The \textbf{flux}, i.e. the rate of traversing particles per unit time and unit area, is:
    \begin{align*}
        j_{\bm{v}}(\bm{r}) = \frac{\dd{N}}{\dd{t}\dd{\Sigma}} = \bm{v}\cdot \hat{\bm{n}} \rho (\bm{r})
    \end{align*}
    This relation is valid \textit{locally} around a point $\bm{r} \in \mathbb{R}^3$.
    
    Now, consider a sphere $S$ of radius $r$. Let $\hat{\bm{n}}(\bm{r})$ be the (outward) unit normal vector at a point $\bm{r}$ of the sphere. $S$ is immersed in an ideal gas at temperature $T$. In any point $\bm{r}$ we will then have particles moving in \textit{every direction}, with a velocity distribution given at equilibrium by the Maxwell-Boltzmann distribution:
    \begin{align*}
        f(v_x,v_y,v_z) = \left(\frac{m}{2 \pi k_B T} \right)^{3/2} \exp\left(-\frac{m(v_x^2 + v_y^2 + v_z^2)}{2 k_B T} \right)
    \end{align*}
    where $m$ is the mass of the gas particles. 
    
    The rate of particles \textit{colliding} with the spherical surface will be the integral of $j_{\bm{v}}(\bm{r})$ over all the velocities \q{towards $S$} (i.e. with $\bm{v} \cdot \hat{\bm{n}}(\bm{r}) < 0$) and all points of $S$:
    \begin{align*}
        \langle \dv{N}{t}  \rangle = \int_{\bm{v} \cdot \hat{\bm{n}} < 0} \dd[3]{\bm{v}} \int_{S}  \dd{\Sigma}\rho |\bm{v} \cdot \hat{\bm{n}}| f(\bm{v}) 
    \end{align*}
    By symmetry, the rate of hits on a tiny surface $\dd{\Sigma}$ on the sphere will be the same at any point of the sphere. So, we can choose any point, compute the rate there, and then multiply the result by the spherical surface $4 \pi r^2$.

    For example, let's choose the \textit{south pole}. From there, all the velocities \textit{directed towards the sphere} are the ones \q{pointing up}, i.e. on any point of the upper hemisphere. So, moving to spherical coordinates in velocity $\dd[3]{\bm{v}} = v^2 \sin \theta \dd{v} \dd{\theta} \dd{\varphi}$, with:
    \begin{align*}
        \bm{v} = v (\sin\theta \cos \varphi, \sin \theta \sin \varphi, \cos \theta)
    \end{align*}
    and $\hat{n}(\text{South Pole}) = (0,0,1)^T$, we have:
    \begin{align*}
        \langle \dv{N}{t}  \rangle &= 4 \pi r^2 \underbrace{\rho}_{N/V} \left(\frac{m}{2 \pi k_B T} \right)^{3/2} \cdot \\
        &\quad \> \cdot \int_0^{+\infty} \dd{v} \int_0^{\pi/2} \dd{\theta} \int_0^{2 \pi} \dd{\varphi} \underbrace{v \cos \theta}_{\bm{v} \cdot (0,0,1)^T}  \sin \theta v^2 \exp\left(-\frac{mv^2}{2 k_B T} \right) =\\
        &= 4 \pi r^2 \frac{N}{V} \left(\frac{m}{2 \pi k_B T} \right)^{3/2} (2 \pi) \underbrace{\int_0^{\pi/2} \sin \theta \cos \theta \dd{\theta}}_{1/2} \underbrace{\int_0^{+\infty} v^3 \exp\left(-\frac{mv^2}{2k_B T} \right)}_{2k_B^2 T^2 / m^2} =\\
        &= 4 \pi r^2 \frac{N}{V} \left(\frac{m}{2\pi k_B T} \right)^{3/2} \pi \frac{2 k_B^2 T^2}{m^2} = 4 \pi r^2 \frac{N}{V} \sqrt{\frac{k_B T}{2 \pi m}}
    \end{align*}
    For the second moment, we square everything except $f(\bm{v})$, leading to:
    \begin{align*}
        \langle \left(\dv{N}{t}\right)^2  \rangle &= \left(4 \pi r^2 \frac{N}{V} \right)^2 \left(\frac{m}{2 \pi k_B T} \right)^{3/2} \cdot\\
        &\quad\>\cdot \int_0^{2\pi} \dd{\varphi} \underbrace{\int_0^{\pi/2} \sin \theta \cos^2 \theta \dd{\theta}}_{1/3} \underbrace{\int_{0}^{+\infty} \dd{v} v^4 \exp\left(-\frac{mv^2}{2k_BT} \right)}_{3\sqrt{\pi/2} (k_B T/m)^{5/2}} =\\
        &= \left(4 \pi r^2 \frac{N}{V} \right)^2 \left(\frac{m}{2 \pi k_B T} \right)^{3/2} (2\pi) \frac{1}{3} 3 \sqrt{\frac{\pi}{2} } \left(\frac{k_B T}{m} \right)^{5/2} = \\
        &= \left(4 \pi r^2 \frac{N}{V} \right)^2 \frac{k_B T}{2m} 
    \end{align*}
    Finally, the variance is given by:
    \begin{align*}
        \operatorname{Var} \left(\dv{N}{t}\right) &= \langle \left(\dv{N}{t}\right)^2  \rangle - \left(\langle \dv{N}{t}  \rangle\right)^2 = \left(4 \pi r^2 \frac{N}{V} \right)^2 \left(\frac{k_B T}{2m} - \frac{k_B T}{2 \pi m}  \right) =\\
        &= \left(4 \pi r^2 \frac{N}{V} \right)^2\frac{k_B T}{2m}\left(1-\frac{1}{\pi} \right) 
    \end{align*}
\end{exo}

\begin{exo}
    Show that the statistical properties of the Brownian motion, $B(t)$, imply that $\langle \xi(t) \rangle = 0$ and $\langle \xi(t_1) \xi(t_2) \rangle = \delta(t_2 - t_1)$.

    We have defined in a purely formal way $\xi(t) = \dd{B(t)}/\dd{t}$ in the following equation:
    \begin{align*}
        \dv{X}{t}(t) = \sqrt{2D} \xi(t) \qquad \dv{B}{t}(t) \equiv \xi(t)
    \end{align*}
    (Remember that the Brownian trajectories are not differentiable).
    \medskip

    \textit{Hint}: $\langle \Delta B_i \rangle = 0$ and $\langle \Delta B_i \Delta B_j \rangle = \Delta t_i \delta_{ij}$. This result is also consistent with the following formal expression for the $\xi$ trajectories:
    \begin{align*}
        \dd{\mathbb{P}}(\{\xi(\tau)\}) \propto \prod_{\tau} \dd{\xi(\tau)} \exp\left(-\frac{1}{2} \int \dd{\tau} \xi^2(\tau) \right)
    \end{align*}
    which can be deduced from the analogous expression for the $\dd{\mathbb{P}}(\{B(\tau)\})$.

    \medskip

    \textbf{Solution}. Consider a discretization $\{t_j\}_{j=1,\dots,n}$, with $\Delta t_i = t_i - t_{i-1}$, $f_i \equiv f(t_i)$, $t_n \equiv t$ fixed.
    
    Then, by definition, $\Delta B_i \equiv \Delta t_i \xi(t_i)$. Averaging for $i=n$:
    \begin{align*}
        0=\langle \Delta B_n \rangle = \Delta t_n \langle \xi_n \rangle  \Rightarrow \langle \xi(t_n) \rangle = 0  \xrightarrow[n \to \infty]{} \langle \xi(t) \rangle = 0
    \end{align*} 
    For the correlator, choose two instants $t_i$ and $t_j$ in the discretization, keeping them \textit{fixed} when doing the continuum limit. Then:
    \begin{align*}
        \Delta t_i \delta_{ij} = \langle \Delta B_i \Delta B_j \rangle = \Delta t_i \Delta t_j \langle \xi(t_i) \xi(t_j) \rangle
    \end{align*} 
    where we used the linearity of the average. Rearranging:
    \begin{align*}
        \langle \xi(t_i) \xi(t_j)\rangle = \frac{\Delta t_i \delta_{ij}}{\Delta t_i \Delta t_j } = \frac{\delta_{ij}}{\Delta t_j}  \xrightarrow[n \to \infty]{} \delta(t_i -t_j)
    \end{align*}
    This formula for the continuum limit of a Kronecker delta is a \q{discretization rule} (see \url{https://physics.stackexchange.com/questions/127705/where-does-the-delta-of-zero-delta0-come-from}).  Graphically, we can interpolate a discretized function by using the left-most points (as in Ito's prescription), so that:
    \begin{align*}
        \delta_{i0} \Rightarrow f(x) = \begin{cases} 
            1 & 0 \leq x < \Delta t_0\\
            0 & \text{otherwise}
        \end{cases}
    \end{align*}
    So it has an \textit{area} of $\Delta t_0 \neq 1$, so it cannot be a Dirac Delta in the continuum limit, which is always normalized. So, the correct limit involves $\delta_{i0}/\Delta t_0$. %a graph would be nice
\end{exo}

\begin{exo}
    Use the probability distribution for a Brownian trajectory:
    \begin{align*}
        \dd{\mathbb{P}}_{t_1,\dots,t_n}(B_1, \dots, B_n|B_0,t_0) = \exp\left(-\sum_{i=1}^n \frac{(B_i - B_{i-1})^2}{2 \Delta t_i} \right) \prod_{i=1}^n \frac{\dd{B_i}}{\sqrt{2 \pi \Delta t_i}} 
    \end{align*}
    to show that:
    \begin{align*}
        \langle (\Delta B_i)^2 (\Delta B_j)^2 \rangle = 3 \delta_{ij} \Delta t_i^2 + (1-\delta_{ij}) \Delta t_i \Delta t_j
    \end{align*}
    
    \medskip

    \textbf{Solution}. If $i \neq j$, as different increments are independent, we have:
    \begin{align*}
        \langle (\Delta B_i)^2 (\Delta B_j)^2 \rangle = \langle (\Delta B_i)^2 \rangle \langle (\Delta B_j)^2 \rangle = \Delta t_i \Delta t_j
    \end{align*}
    where we used $\langle ( \Delta B_i \Delta B_j  \rangle = \Delta t_i \Delta t_j \delta_{ij}$.

    If $i = j$ we have:
    \begin{align*}
        \langle (\Delta B_i)^4 \rangle
    \end{align*}
    which can be computed through Wick's theorem. Here the number of factors is $4$, and so we have $(4-1)!! = 3$ possible partitions in couples, which are all equal:
    \begin{align*}
        \langle (\Delta B_i)^4 \rangle = 3 \langle (\Delta B_i)^2 \rangle = 3 \Delta t_i^2
    \end{align*}
    We can write both cases at the same time by using a Kronecker's delta:
    \begin{align*}
        \langle (\Delta B_i)^2 (\Delta B_j)^2 \rangle =\underbrace{ 3 \delta_{ij} \Delta t_i^2 }_{i=j}+ \underbrace{(1-\delta_{ij}) \Delta t_i \Delta t_j}_{i \neq j} 
    \end{align*}
\end{exo}

\begin{exo}
    Show that:
    \begin{align*}
        \langle \left[\sum_{i=1}^n \left((\Delta B_i)^2 - \Delta t_i\right)\right]^2 \rangle = 2 \sum_{i=1}^n (\Delta t_i)^2
    \end{align*}

    \medskip

    \textbf{Solution}. We start by expanding the square:
    \begin{align*}
        \sum_{ij}^n \langle [\Delta t_i - (\Delta B_i)^2] [\Delta t_j - (\Delta B_j)^2]\rangle
    \end{align*}
    Then we highlight the case where $i = j$:
    \begin{align} \label{eqn:dig}
        = \sum_{i=1}^n \langle [\Delta t_i - (\Delta B_i)^2]^2 \rangle + \sum_{i \neq j}^n \langle [\Delta t_i - (\Delta B_i)^2] [\Delta t_j - (\Delta B_j)^2]\rangle
    \end{align}
    Note that for $i \neq j$ we can use the \textit{independence of increments} to factorize the average:
    \begin{align*}
        \langle [\Delta t_i - (\Delta B_i)^2] [\Delta t_j - (\Delta B_j)^2]\rangle = \langle [\Delta t_i - (\Delta B_i)^2] \rangle \langle \Delta t_j - (\Delta B_j)^2 \rangle
    \end{align*} 
    Recall that $\langle (\Delta B_i)^2 \rangle = \Delta t_i$, and so, by linearity:
    \begin{align*}
         \langle [\Delta t_i - (\Delta B_i)^2] \rangle=\Delta t_i -\langle (\Delta B_i)^2 \rangle = 0
    \end{align*}
    So we are left with the diagonal terms in (\ref{eqn:dig}). Expanding the square:
    \begin{align} \label{eqn:dig2}
        \sum_{i=1}^n \langle [\Delta t_i - (\Delta B_i)^2]^2 \rangle = \sum_{i=1}^n \left[\Delta t_i^2 - 2 \Delta t_i \langle \underbrace{(\Delta B_i)^2}_{\Delta t_i}  \rangle + \langle \Delta B_i^4 \rangle\right]
    \end{align}
    And by using Wick's theorem:
    \begin{align*}
        \langle (\Delta B_i)^4 \rangle = (4-1)!! \langle (\Delta B_i)^2 \rangle \langle (\Delta B_i)^2\rangle = 3 \Delta t_i^2
    \end{align*}
    Finally, substituting back in (\ref{eqn:dig2}) we arrive at the desired result:
    \begin{align*}
        \langle \left[\sum_{i=1}^n \left((\Delta B_i)^2 - \Delta t_i\right)\right]^2 \rangle &= \sum_{i=1}^n [\Delta t_i^2 - 2 \Delta t_i^2 + 3 \Delta t_i^2] =\\
        &= 2 \sum_{i=1}^n (\Delta t_i)^2
    \end{align*}
\end{exo}

\begin{exo}
    Show that for the generic $\lambda$-prescription:
    \begin{align}
        S\equiv \int_0^t B(\tau) \dd{B(\tau)} = \frac{B^2(t) - B^2(t_0)}{2} + \frac{2\lambda - 1}{2} (t-t_0) \label{eqn:disc-B}
    \end{align}
    where we have considered a generic initial condition, $B(t_0)$ at $\tau = t_0$.

    \medskip

    \textbf{Solution}. Consider a \q{adapted} time discretization $\{t_i\}_{i=1,\dots, 2n}$, with $t_0$ and $t_{2n} \equiv t$ fixed, and:
    \begin{align*}
        t_{2j-1} = \lambda t_{2j} + (1-\lambda ) t_{2i-2}
    \end{align*}
    Graphically, this means that all \textit{odd points} $t_{2j-1}$ are fixed in the \textit{same relative position} in the intervals $[t_{2j-2}, t_{2j}]$, in the sense that, for example:
    \begin{align*}
        t_1 - t_0 = \lambda(t_2 - t_0)
    \end{align*}

    The integral (\ref{eqn:disc-B}) can be then discretized as follows:
    \begin{align*}
        S &= \lim_{n \to \infty} S_n\\
        S_n &= \sum_{i=1}^{2n} [B_{2i-1}] (B_{2i} - B_{2i-2})
    \end{align*}
    We then approximate $B_{2i-1}$ \textit{as if} it were differentiable (this is a non-rigorous \textit{trick} to make computation faster):
    \begin{align*}
        S_n &= \sum_{i=1}^{2n} [\lambda B_{2i} + (1-\lambda) B_{2i-2}](B_{2i} - B_{2i-2}) =\\
        &= ^{2n} [\lambda B_{2i} +B_{2i-2}-\lambda B_{2i-2}](B_{2i} - B_{2i-2}) =\\
        &= \sum_{i=1}^{2n} \Big[\lambda(B_{2i}- B_{2i-2})(B_{2i}-B_{2i-2}) + \underbrace{B_{2i-2} (B_{2i} - B_{2i-2})}_{A} \Big]
    \end{align*}
    Now the \textit{odd} terms (middle points) do not appear anymore, and so these are effectively Ito's integrals. In particular the term in $A$ was already solved:
    \begin{align*}
        A = \int_{t_0}^t B(\tau) \dd{B(\tau)}\Big|_{\mathrm{Ito}} = \frac{B^2(t)-B^2(t_0)}{2} - \frac{(t-t_0)}{2} 
    \end{align*}
    So all that's left is to evaluate:
    \begin{align*}
        \lambda \sum_{i=1}^{2n} (B_{2i}-B_{2i-2})^2  \xrightarrow[n \to \infty]{}  \lambda \int_{t_0}^t  \dd{B(\tau)}^2 = \lambda \int_{t_0}^t \dd{\tau} = \lambda (t-t_0)
    \end{align*}
    Putting it all together:
    \begin{align*}
        S = \frac{B^2(t)-B^2(t_0)}{2} - \frac{(t-t_0)}{2} - \lambda(t-t_0) = \frac{B^2(t)-B^2(t_0)}{2} + \frac{2\lambda -1}{2}(t-t_0)  
    \end{align*}
    
\end{exo}

\begin{exo}
    Show that, in the $\lambda$-prescription:
    \begin{align*}
        \int_{0}^t B(\tau) \dd{B(\tau)} \Big|_{\lambda} = \frac{B^2(t)-B^2(0)}{2} + \frac{2 \lambda -1}{2} (t-t_0)   
    \end{align*}

    \medskip

    \textit{Hint}: Consider the time mesh $\{t_i\}_{i=0,\dots,2n}$ with $t_{2n} = t$, $t_{2i-1} = \lambda t_{2i} + (1-\lambda) t_{2i-2}$ and:
    \begin{align*}
        S_n = \sum_{i=1}^{2n} B_{2i-1} (B_{2i} - B_{2i-2})
    \end{align*} 

    \medskip

    \textbf{Solution}. See the previous exercise. 

\end{exo}


\begin{exo}
    Show that $(\dd{B(\tau)})^{k+2} = 0$ $\forall k > 0$. This is done in a similar way as in the proof of $(\dd{B(\tau)})^2 = \dd{\tau}$. You have to show that:
    \begin{align*}
        \int_{t_0}^t G(\tau) (\dd{B(\tau)})^{2+k} = 0
    \end{align*}
    using ms-convergence. Assume for simplicity that $G$ is bounded. On the same token derive also that:
    \begin{align*}
        \int_{t_0}^t G(\tau) \dd{B(\tau)}\dd{\tau} = 0
    \end{align*}

    \medskip

    \textbf{Solution}. Let $G \colon \mathbb{R} \to \mathbb{R}$ be a \textit{non-anticipating} test function. We want to show that:
    \begin{align*}
        \int_0^t G(\tau) (\dd{B(\tau)})^k = \overset{\mathrm{m.s.} }{\lim_{n \to \infty}} \sum_{i=1}^n G_{i-1} (\Delta B_i)^k = 0
    \end{align*} 
    Expanding the definition of mean square limit, we need to show:
    \begin{align*}
        \langle \left(\sum_{i=1}^n G_{i-1} (\Delta B_i)^k\right)^2 \rangle  \xrightarrow[n \to \infty]{}  0
    \end{align*}
    Then we expand the square:
    \begin{align*}
        \langle \left(\sum_{i=1}^n G_{i-1} (\Delta B_i)^k\right)^2 \rangle = \langle \left(\sum_{i=1}^n G_{i-1} (\Delta B_i)^k\right) \left(\sum_{j=1}^n G_{j-1} (\Delta B_j)^k\right) \rangle
    \end{align*}
    We separate the case for which $i=j$:
    \begin{align*}
        = \sum_{i=1}^n \langle G_{i-1}^2 (\Delta B_i)^{2k} \rangle + 2 \sum_{i<j}^n \langle G_{i-1} G_{j-1} (\Delta B_i)^k (\Delta B_j)^k \rangle
    \end{align*} 
    We can factor all the averages exploiting the fact that $G$ is \textit{non-anticipating}, and that \textit{increments are independent}:
    \begin{align} \label{eqn:last}
        = \sum_{i=1}^n G_{i-1}^2 \hlc{Yellow}{\langle (\Delta B_i)^{2k} \rangle }+ 2 \sum_{i< j}^n \langle G_{i-1} G_{j-1} (\Delta B_i)^k \rangle\hlc{SkyBlue}{ \langle (\Delta B_j)^k \rangle}
    \end{align}  
    Recall that each increment $\Delta B_i$ follows a gaussian distribution, and that the $p$-th moment of $X \sim \mathcal{N}(\mu,\sigma)$ is:
    \begin{align} \label{eqn:wick}
        \mathbb{E}[(X-\mu)^p] = \begin{cases}
            0 & p \text{ is odd}\\
            \sigma^p(p-1)!! & p \text{ is even}
        \end{cases}
    \end{align}
    So we have two cases:
    \begin{itemize}
        \item $k$ is odd: the blue term in (\ref{eqn:last}) vanishes, but the yellow one not. Recall that $\langle \Delta B_i^2 \rangle = \Delta t_i$, and so $\langle (\Delta B_i)^{2k} \rangle = (\Delta t_i)^k (2k-1)!!$ by using (\ref{eqn:wick}).
        We then use the fact that $G$ is bounded, i.e.  $|G(\tau)| < K$ $\forall \tau \in \mathbb{R}$:
        \begin{align*}
            &= \sum_{i=1}^n G_{i-1}^2 (\Delta t_i)^k (2k-1)!! \leq K^2(2k-1)!!\sum_{i=1}^n (\Delta t_i)^k \leq\\
            &\leq K^2(2k-1)!! \left(\max_{i \leq j \leq n} (\Delta t_i)^{k-1}\right) \underbrace{\sum_{i=1}^n \Delta t_i}_{t}  \xrightarrow[n \to \infty]{}  0
        \end{align*}
        In the last step, we use the fact that $\sum_i \Delta t_i$ is fixed, while $\max_i \Delta t_i \to 0$ in the continuum limit.
        \item $k$ is even. We already now that the yellow term goes to $0$ (same argument as the same for $n$ odd). However, now the blue term does not vanish immediately. So, again, we use the fact that $G$ is bounded:
        \begin{align} \label{eqn:last2}
            (\ref{eqn:last}) = 2 \sum_{i<j}^n \langle \underbrace{G_{i-1} G_{j-1}}_{\leq K^2} (\Delta B_i)^k \rangle \langle (\Delta B_j)^k \rangle
        \end{align}
        By using (\ref{eqn:wick}):
        \begin{align*}
            \langle (\Delta B_i)^k \rangle = (\Delta t_i)^{k/2} (k-1)!!
        \end{align*}
        And so:
        \begin{align*}
            (\ref{eqn:last2}) &\leq 2 K^2 [(k-1)!!]^2 \sum_{i< j}^n \Delta t_i^{n/2} \Delta t_j^{n/2} \leq\\
            &\leq  2K^2[(k-1)!!] \left(\max_{i\leq l \leq n} \Delta t_l\right)^{2(n/2-1)} \underbrace{\sum_{i<j}^n \Delta t_i \Delta t_j}_{\leq t^2}  \xrightarrow[n \to \infty]{}  0
        \end{align*}


    \end{itemize}

\end{exo}

\begin{exo}
    Use the following Ito formula:
    \begin{align}\label{eqn:Itof}
        \int_{t_0}^t h'(x(\tau)) g(x(\tau),\tau) \dd{B(\tau)} &= h(x(t))-h(x(t_0))+\span \\ \nonumber
        &- \left[h'(x(\tau)) f(x(\tau), \tau) + \frac{h''(x(\tau))}{2} g^2(x(\tau), \tau) \right]\dd{\tau} 
    \end{align}
    in order to give an expression in terms of ordinary integrals of:
    \begin{align*}
        \int_{t_0}^t G(x(\tau))\dd{B(\tau)}
    \end{align*}
    where $G$ is a non-anticipating function. The trajectory $x(\tau)$ satisfies the Langevin equation:
    \begin{align*}
        \dd{x(t)} = f(x(t),t) \dd{t} + g(x(t),t) \dd{B(t)}
    \end{align*}
    Explain the meaning of the obtained result. Has it to be considered an equality valid for any trajectory $x(\tau)$?

    \medskip

    \textbf{Solution}. We choose a function $h\colon \mathbb{R} \to \mathbb{R}$ so that:
    \begin{align*}
        G(x(\tau)) = h'(x(\tau)) g(x(\tau), \tau) \Rightarrow h'(x(\tau)) = \frac{G(x(\tau))}{g(x(\tau), \tau)} 
    \end{align*}
    Then:
    \begin{align*}
        h(x(t)) = \int_{t^*}^t \frac{G(x(\tau))}{g(x(\tau), \tau)}  \dd{\tau} \qquad h''(x(\tau)) = \frac{G'g - G g'}{g^2} 
    \end{align*}
    where $t^*$ can be any point.

    Inserting in (\ref{eqn:Itof}) we get:
    \begin{align*}
        \int_{t_0}^t G(x(\tau))\dd{B(\tau)} &= \int_{t^*}^t \frac{G(x(\tau))}{g(x(\tau), \tau)}  \dd{\tau} - \int_{t^*}^{t_0} \frac{G(x(\tau))}{g(x(\tau), \tau)}  \dd{\tau} +\\
        &\quad \> - \int_{t_0}^t \left[\frac{G(x(\tau))}{g(x(\tau), \tau)} f(x(\tau), \tau) + \frac{1}{2}[G'g - G g'] \right]\dd{\tau} =\\
        = \int_{t_0}^t \frac{G(x(\tau))}{g(x(\tau), \tau)}  \dd{\tau} - \int_{t_0}^t \left[\frac{G(x(\tau))}{g(x(\tau), \tau)} f(x(\tau), \tau) + \frac{1}{2}[G'g - G g'] \right]\dd{\tau} = \span\\
        &=\int_{t_0}^t \left(\frac{G}{g}(1-f) + \frac{1}{2}[G'g - Gg']  \right)\dd{\tau}
    \end{align*}
    
\end{exo}

\begin{exo}[Change of variables in $\lambda$ prescription]
    Generalize the results for the change of variable formula for Ito integrals to the case of a generic $\lambda$-prescription and re-derive the result of problem 4.9.\\
    
    \textbf{Solution}. The main idea is to start from a Stochastic Differential Equation in $\lambda$-prescription, convert it to an equivalent formulation using the Ito prescription, apply Ito's formula for changing variables, and then go back to the former prescription.
    
    First, two SDEs have the same solution $x(t)$ if, for any realization $B(t)$ of the Brownian noise, their solutions (which can now be found by \textit{normal} integration) coincide.
    
    So, consider the usual SDE in Ito's prescription:
    \begin{align*}
        \dd{x(t)} = a(x(t),t) \dd{t} + b(x(t),t) \dd{B(t)}
    \end{align*}
    which has solution:
    \begin{align*}
        x(t) = x(t_0) + \int_{t_0}^t a(x(\tau),\tau) \dd{\tau} + \int_{t_0}^t b(x(\tau), \tau) \dd{B(\tau)}
    \end{align*}
    where the stochastic integral is formally defined as:
    \begin{align*}
        \int_{t_0}^t b(x(\tau),\tau) \dd{B(\tau)} \equiv \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^n b(x_{i-1}, t_i) \Delta B_i \qquad \Delta B_i = B_i - B_{i-1}
    \end{align*}
    We now consider a  SDE in the $\lambda$ prescription:
    \begin{align*}
        \dd{y(t)} = \alpha(y(t),t) \dd{t} + \beta(y(t),t) \dd{B(t)} \big|_{\lambda}
    \end{align*}
    which has solution:
    \begin{align*}
        y(t) = \int_{t_0}^t \alpha(y(\tau), \tau) \dd{\tau} + \int_{t_0}^t \beta(y(\tau), \tau) \dd{\tau} \Big|_{\lambda}
    \end{align*}
    Now, however, the stochastic integral has a different definition:
    \begin{align*}
        \int_{t_0}^t \beta(y(\tau), \tau) \dd{B(\tau)} \Big|_{\lambda} \equiv \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^n \beta\Big( (1-\lambda) x_{i-1} + \lambda x_i, t_{i-1}\Big) \Delta B_i
    \end{align*}
    We now impose that $y(t) = x(t)$ for every $t$, and search the mapping $a,b \mapsto \alpha, \beta$ that establishes a correspondence between an Ito SDE and a generic $\lambda$ prescription SDE.\\
    
    Let's focus on the argument of the $\lambda$-integral, and expand it about the left extremum of the discretization:
    \begin{align} \nonumber
        \beta(y_{i-1} - \lambda y_{i-1} + \lambda y_i, t_{i-1}) &= \beta(y_{i-1} + \lambda(y_i - y_{i-1}), t_{i-1}) =\\
        &= \beta(y_{i-1}, t_{i-1}) + \partial_x \beta(y_{i-1}, t_{i-1}) \lambda (y_i - y_{i-1}) \label{eqn:beta1}
    \end{align}
    As the paths are the same, $y_i = x_i$, and the increments $\Delta y_i$ follow the rule:
    \begin{align*}
        \Delta y_i = a(x_{i-1}, t_{i-1}) \Delta t_i + b(x_{i-1}, y_{i-1}) \Delta B_i
    \end{align*}
    Leading to:
    \begin{align*}
        (\ref{eqn:beta1}) = \beta_{i-1} + \beta_{i-1}' \lambda [a_{i-1}\Delta t_i + b_{i-1} \Delta B_i]
    \end{align*}
    Substituting inside the integral we get:
    \begin{align*}
        \int_{t_0}^t \beta(y(\tau), \tau) \dd{B(\tau)} \Big|_{\lambda} &= \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^{n} \Big[\beta_{i-1} \Delta B_i + \lambda \beta'_{i-1} \Delta B_i [a_{i-1} \Delta t_i + b_{i-1} \Delta B_{i}] \Big] =\\
        &=\overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^{n}\Big[
        \beta_{i-1} \Delta B_i + \lambda \beta_{i-1}' b_{i-1} \Delta B_i^2 + O(\Delta B_i \Delta t_i)    
        \Big]
    \end{align*}
    We already proved that:
    \begin{align*}
        \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^n G_{i-1} \Delta B_i^2 = \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^n G_{i-1} \Delta t_i
    \end{align*}
    And so we can use this result, setting $G_{i-1} = \lambda \beta'_{i-1} b_{i-1}$, justifying the usual $\dd{B}^2 = \dd{t}$ rule. So, this leads to:
    \begin{align} \nonumber 
        \int_{t_0}^t \beta(y(\tau), \tau) \dd{B(\tau)} \Big|_{\lambda} &= \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^{n} \Big[\beta_{i-1} \Delta B_i + \lambda \beta_{i-1}' b_{i-1} \Delta t_i \Big] = \\
        &=\int_{t_0}^t \beta(y(\tau), \tau) \dd{B(\tau)} + \lambda \int_{t_0}^t b(y(\tau), \tau) \pdv{x} \beta(y(\tau), \tau) \dd{\tau}
        \label{eqn:formula}
    \end{align}
    where the last two integrals are Ito integrals. We have now found a way to evaluate a $\lambda$-integral using an Ito integral (provided the paths are generated by a Ito SDE). We can find the explicit conversion rules by equating the solutions:
    \begin{align*}
        x(t) &= x(t_0) + \int_{t_0}^t a(x(\tau), \tau) \dd{\tau} + \int_{t_0}^t b(x(\tau), \tau) \dd{B(\tau)} =\\
        &\overset{!}{=} x(t_0) + \int_{t_0}^t \alpha(x(\tau), \tau) \dd{\tau} + \int_{t_0}^t \beta(x(\tau), \tau) \dd{B(\tau)} =\\
        &\underset{(\ref{eqn:formula})}{=} x(t_0) + \int_{t_0}^t \alpha(x(\tau), \tau) \dd{\tau} + \int_{t_0}^t \beta(x(\tau), \tau) \dd{B(\tau)} + \lambda \int_{t_0}^t b(x(\tau), \tau) \partial_x \beta(x(\tau), \tau) \dd{\tau}
    \end{align*} 
    leading to:
    \begin{align*}
        \begin{cases}
            \alpha + \lambda b \partial_x \beta = a\\
            b = \beta
        \end{cases} \Rightarrow \begin{cases}
            \alpha = a - \lambda b \partial_x \beta\\
            \beta = b
        \end{cases}
    \end{align*}
    where $(\alpha, \beta)$ are the coefficients in the $\lambda$ SDE, and $(a,b)$ the ones in the equivalent Ito SDE.
    
    Consider now a $\lambda$ SDE:
    \begin{align*}
        \dd{x} = \alpha \dd{t} + \beta \dd{B(t)}
    \end{align*}
    The equivalent Ito SDE is:
    \begin{align}\label{eqn:newdx}
        \dd{x} = (\alpha + \lambda \beta \partial_x \beta) \dd{t} + \beta \dd{B(t)}
    \end{align}
    Squaring:
    \begin{align*}
        \dd{x}^2 = \beta^2 \dd{t}
    \end{align*}
    Where we used $\dd{B}^2 = \dd{t}$ (as this is an Ito SDE), and ignored higher order terms. It is clear that $\dd{x}^n = 0$ with $n > 0$.
    
    Let $y=y(x)$ be a change of variables. The new differential will be:
    \begin{align*}
        \dd{y} = \dv{y}{x} \dd{x} + \frac{1}{2} \dv[2]{y}{x} \dd{x}^2 + \dots
    \end{align*}
    and we can ignore the higher order terms, as they will be $O(\dd{t})$. Substituting in (\ref{eqn:newdx}) we get:
    \begin{align*}
        \dd{y} &= \dv{y}{x} (\alpha + \lambda \beta \partial_x \beta) \dd{t} + \dv{y}{x} \beta \dd{B} + \frac{1}{2} \dv[2]{y}{x} \beta^2 \dd{t}  =\\
        &= \left[\dv{y}{x}\left(\alpha+ \lambda \beta \partial_x \beta \right) + \frac{1}{2} \dv[2]{y}{x} \beta^2 \right]\dd{t} + \dv{y}{x} \beta \dd{B}
    \end{align*}
    To complete the change of variables, we need to express everything in terms of $y$ - in particular the derivatives. One trick is to use the inverse function theorem:
    \begin{align*}
        \dv{y}{x} = \left(\dv{x}{y}\right)^{-1} = \frac{1}{x'(y)} 
    \end{align*}
    For the second derivative, note that, if $f$ and $g$ are the inverse of each other:
    \begin{align*}
        g \circ f = \operatorname{id} &\Rightarrow g(f(x)) = x \underset{\rm d/dx}{\Rightarrow} g'(f(x)) f'(x) = 1\\
         &\underset{\rm d/dx}{\Rightarrow} g''(f(x)) [f'(x)]^2 + g'(f(x)) f''(x) = 0 \\&\underset{y=f(x)}{\Rightarrow}  g''(y) = - \frac{g'(y) f''(x)}{f'(x)^2} = - \frac{f''(x)}{[f'(x)]^3}  
    \end{align*}
    And in our case:
    \begin{align*}
        \dv[2]{y}{x} = -\frac{x''(y)}{[x'(y)]^3} 
    \end{align*}
    One last thing:
    \begin{align*}
        \pdv{x} \beta = \dv{y}{x} \pdv{y} \beta = \frac{1}{x'(y)} \partial_y \beta 
    \end{align*}
    
    This leads to:
    \begin{align*}
        \dd{y} = \underbrace{\left[\frac{\alpha}{x'(y)} + \frac{\lambda \beta \partial_y \beta}{[x'(y)]^2} - \frac{1}{2} \beta^2 \frac{x''(y)}{[x'(y)]^3}    \right]}_{a}  \dd{t} + \underbrace{\frac{\beta}{x'(y)}}_{b}  \dd{B} 
    \end{align*}
    
    We can finally map this back to a $\lambda$ SDE and find the change of variable rule for that case. Applying the substitutions:
    \begin{align*}
        \dd{y} = \tilde{\alpha} \dd{t} + \tilde{\beta} \dd{B} \qquad \begin{cases}
            \tilde{\alpha} &= a-\lambda b \partial_x b\\
        \tilde{\beta} &= b
        \end{cases}
    \end{align*}
    We arrive to:
    \begin{align*}
        \tilde{\alpha} &= \frac{\alpha}{x'(y)} + \frac{\lambda \beta \partial_y \beta}{[x'(y)]^2} - \frac{1}{2} \beta^2 \frac{x''(y)}{[x'(y)]^3} - \lambda \frac{\beta}{x'(y)} \partial_y \frac{\beta}{x'(y)}  =\\
        &= \frac{\alpha}{x'(y)} + \cancel{\frac{\lambda \beta \partial_y \beta}{[x'(y)]^2}} - \frac{1}{2} \beta^2 \frac{x''(y)}{[x'(y)]^3} - \lambda \frac{\beta}{x'(y)}\left[\frac{\cancel{\partial_y \beta x'(y)} - x''(y) \beta}{[x'(y)]^2} \right] =\\
        &= \frac{\alpha}{x'(y)} +\frac{2\lambda - 1}{2} \frac{\beta^2 x''(y)}{[x'(y)]^3}\\
        \tilde{\beta} &= \frac{\beta}{x'(y)}\\
        \dd{y} &= \left[\frac{\alpha}{x'(y)} +\frac{2\lambda - 1}{2} \frac{\beta^2 x''(y)}{[x'(y)]^3}\right] \dd{t} + \frac{\beta}{x'(y)} \dd{B} 
    \end{align*}
    Let's bring this result to the usual notation:
    \begin{align*}
        \alpha = f(x(\tau), \tau) \qquad \beta = g(x(\tau),\tau) \qquad \begin{dcases}
            y = h(x(\tau))\\
            \frac{1}{x'(y)} = \dv{h}{x} = h'(x(\tau))\\
            -\frac{x''(y)}{[x'(y)]^3} = \dv[2]{h}{x} = h''(x(\tau)) 
        \end{dcases}
    \end{align*}
    leading to:
    \begin{align*}
        \dd{h(x(\tau))} &= \left(f(x(\tau),\tau) h'(x(\tau)) + \frac{1-2 \lambda}{2} h''(x(\tau)) g(x(\tau), \tau)^2\right) \dd{\tau} +\\
        &\quad \> + g(x(\tau), \tau) h'(x(\tau)) \dd{B(\tau)} \Big|_\lambda
    \end{align*}
    which is the formula for changing variables in the $\lambda$ prescription. Let's rearrange to isolate the $\dd{B}$ term:
    \begin{align*}
        gh' \dd{B} = \dd{h} - \left(f h' + \frac{1-2\lambda}{2} h'' g^2\right) \dd{\tau}
    \end{align*}
    Then we integrate, leading to the formula:
    \begin{align*}
        \int_{t_0}^t h'(x(\tau)) g(x(\tau), \tau) &= h(x(t))-h(x(t_0)) - \int_{t_0}^t h'(x(\tau))f(x(\tau), \tau) \dd{\tau}\\
        &\quad \> -\frac{1-2 \lambda}{2}\int_{t_{0}}^t  h''(x(\tau)) g(x(\tau), \tau)^2 \dd{\tau} 
    \end{align*}
    Finally, set $g(x(\tau), \tau) \equiv 1$, and $h'(x(\tau)) = B(\tau)$, so that:
    \begin{align*}
        h = \frac{B^2}{2} \qquad h'' = 1 
    \end{align*}
    Substituting in the formula we can compute the desired integral:
    \begin{align*}
        \int_{t_0}^t B(\tau) \dd{B(\tau)} = \frac{B^2(t)-B^2(t_0)}{2} + \frac{2 \lambda - 1}{2} (t-t_0)  
    \end{align*}
    
    \end{exo}

\chapter{Particles in a thermal bath} %sec 5
\begin{exo}[Harmonic oscillator with general initial condition]
    The propagator for a stochastic harmonic oscillator is given by:
    \begin{align*}
        W(x,t|0,0) = \sqrt{\dfrac{k}{2 \pi D (1- e^{-2kt})} } \exp\left(-\frac{k}{2D} \frac{x^2}{1-e^{-2kt}}  \right)
    \end{align*}
    Derive the analogous result for $W(x,t|x_0,t_0)$.
    \medskip

    \textbf{Solution}. Consider a particle of mass $m$, experiencing a drag force $F_d = -\gamma v$, an elastic force $F=-kx = -m \omega^2 x$ and thermal fluctuations with amplitude $\sqrt{2 D} \gamma$. The equation of motion is given by:
    \begin{align*}
        m \ddot{x} = - \gamma \dot{x} - m \omega^2 x + \sqrt{2D} \gamma \xi
    \end{align*}
    where $\xi(t)$ is a white noise \textit{function}, meaning that $\langle \xi(t) \xi(t') \rangle = \delta(t-t')$ (infinite variance). Dividing by $\gamma$ and taking the overdamped limit $m/\gamma \ll 0$ we can ignore the $\ddot{x}$ term, leading to a first order SDE:
    \begin{align}
        \dot{x} = -\underbrace{\frac{m \omega^2}{\gamma}}_{k}x + \sqrt{2D} x+ \sqrt{2D} \xi \Rightarrow \dd{x(t)} = - kx(t)\dd{t} + \sqrt{2D} \underbrace{\xi \dd{t}}_{\dd{B(t)}}  \label{eqn:SDE1}
    \end{align}
    Consider a time discretization $\{t_j\}_{j=1,\dots,n}$, with $t_n \equiv t$ and the usual notation $x(t_i) \equiv x_i$, $\Delta x_i \equiv x_i - x_{i-1}$. In the Ito prescription, equation (\ref{eqn:SDE1}) becomes:
    \begin{align*}
        \Delta x_i = - kx_{i-1} \Delta t_i + \sqrt{2D} \Delta B_i
    \end{align*}
    The probability associated with a sequence $\{\Delta B_j\}_{j=1,\dots,n}$ independent increments is a product of gaussians:
    \begin{align} \label{eqn:prob-increments}
        \mathbb{P}(\{\Delta B_j\}_{j=1,\dots,n}) = \left( \prod_{i=1}^n \frac{\dd{\Delta B_i}}{\sqrt{2 \pi \Delta t_i}} \right) \exp\left(-\sum_{i=1}^n \frac{\Delta B_i^2}{2 \Delta t_i} \right) 
    \end{align}
    From (\ref{eqn:prob-increments}), we can find the probability of the path-increments $\{ \Delta x_i\}_{i=1,\dots,n}$ with a change of random variable:
    \begin{align*}
        \Delta B_i = \frac{\Delta x_i + k x_{i-1} \Delta t_i}{\sqrt{2D}}
    \end{align*}
    With the jacobian:
    \begin{align*}
        J = \operatorname{det} \left|\pdv{\{\Delta B_i\}}{\{\Delta x_j\}} \right| = \left|\pdv{\{\Delta x_i\}}{\{\Delta B_j\}} \right|^{-1} = \left|\begin{array}{cccc}
        \sqrt{2D} & 0 & \cdots & 0 \\ 
        * & \sqrt{2D} & \ddots & \vdots \\ 
        \vdots & \ddots & \ddots & 0 \\ 
        * & \cdots & * & \sqrt{2D}
        \end{array}\right|^{-1} = (2D)^{-n/2}
    \end{align*}
    The starred terms $*$ are generally non-zero (they are due to the presence of $x_{i-1}$ in the $\Delta x_i$ formula, which depends on $\Delta B_j$ with $j < i-1$), but the matrix is still lower triangular, meaning that its determinant is just the product of the diagonal terms.

    Performing the change of variables leads to:
    \begin{align} %\nonumber
        \mathbb{P}(\{\Delta x_i\}_{i=1,\dots,n}) &=\left( \prod_{i=1}^n  \frac{\dd{\Delta x_i}}{\sqrt{4 \pi D \Delta t_i}}  \right) \exp\left(-\sum_{i=1}^n \frac{1}{2 \Delta t_i} \left(\frac{\Delta x_i + k x_{i-1} \Delta t_i}{\sqrt{2D}} \right)^2 \right)
        \label{eqn:dP1}
    \end{align}
    Taking the continuum limit $n \to \infty$:
    \begin{align*}
        \dd{P} \equiv \mathbb{P}(\{x(\tau)\}_{t_0 \leq \tau \leq t}) = \left(\prod_{\tau = t_0^+}^t \frac{\dd{x(\tau)}}{\sqrt{4 \pi D \dd{\tau}}} \right) \exp\left(-\frac{1}{4D} \int_{t_0}^t (\dot{x} + kx)^2 \dd{\tau} \right)
    \end{align*}
We can finally consider the path integral for the propagator:
\begin{align*}
    W(x_t,t|x_0,t_0) &= \langle \delta(x_t - x) \rangle_W = \int_{\mathbb{R}^T} \delta(x_t - x) \dd{P} =\\
    &= \int_{\mathbb{R}^T} \dd{x_W} \delta(x_t - x) \exp\left(-\frac{1}{4D} \int_{t_0}^t (\dot{x} + kx)^2 \dd{\tau} \right)
\end{align*}
The quickest way to compute this integral is to use variational methods. So, consider the functional:
\begin{align*}
    S[x(\tau)] = \int_{t_0}^t [\dot{x}(\tau) + kx(\tau)]^2 \dd{\tau}
\end{align*}
The path $x_c(\tau)$ that stationarizes $S[x(\tau)]$ is the solution of the Euler-Lagrange equations:
\begin{align*}
    \dv{\tau} \pdv{S}{\dot{x}} (x_c) - \pdv{S}{x} (x_c) = 0 \Rightarrow \ddot{x}_c(\tau) = k^2 x(\tau)
\end{align*}
Leading to:
\begin{align*}
    x_c(\tau) = A e^{k \tau} + B e^{-k \tau}
\end{align*}
The boundary conditions are $x_c(t_0) = x_0$ and $x_c(t) = x_t$ (this last one is given by the $\delta$). So:
\begin{align*}
    \begin{dcases}
        x_0 = A e^{kt_0} + B e^{-k t_0 }\\
        x_t = A e^{kt} + B e^{-kt}
    \end{dcases} \Rightarrow \begin{dcases}
        A = \frac{x_t e^{kt} - x_0 e^{k t_0 }}{e^{2kt} - e^{2k t_0}}\\
        B = -A e^{2k t_0} + x_0 e^{k t_0} 
    \end{dcases}
\end{align*}
The integral is then:
\begin{align*}
    W(x_t,t|x_0,t_0) = \Phi(t) \exp\left(-\frac{1}{4D} \int_{t_0}^t (\dot{x}_c + kx)^2 \dd{\tau} \right)
\end{align*}
Note that:
\begin{align*}
    \dot{x_c} + kx = 2k A e^{k \tau}
\end{align*}
And so the integral becomes:
\begin{align*}
    \int_{t_0}^t (2k A e^{k \tau})^2 \dd{\tau} = 2k  A^2 (e^{2kt} - e^{2kt_0}) = 2k \frac{[x_t e^{kt} - x_0 e^{k t_0}]^2}{e^{2kt} - e^{2k t_0}} = 2k \frac{[x_t - x_0 e^{-k(t-t_0)}]^2}{1 - e^{-2k(t-t_0)}} 
\end{align*}
To compute $\Phi(t)$ we impose the normalization:
\begin{align*}
    \int_{\mathbb{R}} \dd{x} W(x, t|x_0,t_0) \overset{!}{=} 1 \Rightarrow \Phi(t) = \left[\int_{\mathbb{R}} \dd{x} \exp\left(-\frac{k}{2D} \frac{[x e^{kt} - x_0 e^{k t_0}]^2}{e^{2kt} - e^{2k t_0}} \right)\right]^{-1}
\end{align*}
With the substitution $s = x e^{kt} - x_0 e^{k t_0}$ this is just a gaussian integral, evaluating to:
\begin{align*}
    \Phi(t) = \sqrt{\frac{k}{2 \pi D (1 - e^{-2k (t -t_0)})} }
\end{align*}
And so the full propagator is:
\begin{align}
    W(x_t, t|x_0, t_0) =  \sqrt{\frac{k}{2 \pi D (1 - e^{-2k (t -t_0)})} } \exp\left(-\frac{k}{2D} \frac{[x_t - x_0 e^{-k(t-t_0)}]^2}{1 - e^{-2k(t-t_0)}}\right)
    \label{eqn:harmonic-propagator-sol}
\end{align}

\end{exo}


\begin{exo}[Stationary harmonic oscillator]
    Derive the stationary solution $W^*(x)$ of the Fokker Planck equation for the harmonic oscillator, which obeys the following equation:
    \begin{align*}
        \partial_x [k x W^*(x) + D\partial_x W^*(x)] = 0
    \end{align*}
    Explain the hypothesis underlying the derivation and the validity of the derived solution.
    \medskip

    \textbf{Solution}. Recall the Fokker-Planck equation for the distribution $W(x,t)$ of a diffusing particle in a medium with diffusion coefficient $D(x,t)$, and in the presence of an external \textbf{conservative} force $F(x,t)$ with potential $V(x,t)$ and a drag force $F_d = - \gamma v$:
    \begin{align*}
        \pdv{t} W(x,t) = -\pdv{x} \left[f(x,t) W(x,t) - \pdv{x} [ D(x,t) W(x,t)]\right]
    \end{align*}
    where:
    \begin{align*}
        f(x,t) = \frac{F_{\mathrm{ext} }}{\gamma} = -\frac{1}{\gamma} \pdv{V}{x} (x)   \qquad \gamma = 6 \pi \eta a
    \end{align*}
    At equilibrium, we expect a time independent solution $W^*(x)$, so that $\partial_t W^*(x) \equiv 0$. We assume, for simplicity, that $\bm{\gamma = 1}$ and $D(x,t) \equiv D$ \textbf{constant}. Letting $F(x,t) = -kx$ be an elastic force, we arrive to:
    \begin{align*}
        0 &= - \partial_x [-kx W^* - D \partial_x W^*] = kx W^*(x) + D \partial_x W^*(x)
    \end{align*}
    This is a first order ODE that can be solved by separating variables:
    \begin{align*}
        \dv{x} W^* = -\frac{kx}{D}W^* \Rightarrow  \frac{\dd{W^*}}{W^*} = -\frac{kx}{D} \dd{x} \Rightarrow W^*(x) = A \exp\left(-\frac{k x^2}{2D} \right) 
    \end{align*}
    To be valid, this solution must be \textbf{consistent} with the Boltzmann distribution:
    \begin{align*}
        W^*(x)_{\mathrm{Boltz}} = \frac{1}{Z} \exp(- \beta V(x)) = \frac{1}{Z} \exp\left(-\beta \frac{kx^2}{2}\right )  
    \end{align*} 
    Meaning that $D = 1/\beta = k_B T$.
\end{exo}

\begin{exo}[Harmonic propagator with Fourier transforms]
    Use Fourier transforms to derive the full time dependent propagator $W(x,t|x_0,t_0)$ from the FP equation of the harmonic oscillator:
    \begin{align} 
        \partial_t W(x,t |x_0,t_0) = \partial_x [kx W(x,t|x_0,t_0)] + D\partial_x W(x,t|x_0,t_0) \label{eqn:FPharm}
    \end{align}
    \medskip
    
    \textbf{Solution}. The idea is to use the Fourier transform to \textit{reduce} the equation to a simpler one, that can be hopefully solved. 

    First, we expand the first derivative:
    \begin{align*}
        \partial_t W(x,t|x_0,t_0) = k W(x,t|x_0,t_0) + kx \partial_x W(x,t|x_0,t_0) + D \partial_x^2 W(x,t|x_0,t_0)
    \end{align*}
    For simplicity, let $W\equiv W(x,t|x_0,t_0)$. Its Fourier transform is given by:
    \begin{align*}
        \mathcal{F}[W](\omega) \equiv \tilde{W} = \int_{\mathbb{R}} \dd{x} e^{-i \omega x} W(x,t|x_0,t_0)
    \end{align*}
    The Fourier transforms of the derivatives become:
    \begin{align*}
        \mathcal{F}[\partial_x W](\omega) &= i \omega \tilde{W}; \qquad \mathcal{F}[\partial_x^2 W](\omega) = (i \omega)^2 \tilde{W} = -\omega^2 \tilde{W}
    \end{align*}
    (These formulas can be proven by repeated integration by parts). All that's left is to transform the remaining term:
    \begin{align*}
        \mathcal{F}[x \partial_x W](\omega) &= \int_{\mathbb{R}} \dd{x} e^{-i \omega x} x \partial_x W = \int_{\mathbb{R}} \dd{x} \textcolor{Red}{i} \partial_\omega [e^{-i \omega x} \partial_x W] = i \dv{\omega} \underbrace{\int_{\mathbb{R}} \dd{x} e^{-i \omega x} \partial_x W}_{i \omega \tilde{W}} =\\
        &= - \tilde{W} - \omega \partial_\omega \tilde{W}
    \end{align*}
    So (\ref{eqn:FPharm}) becomes:
    \begin{align*}
        \partial_t \tilde{W}(\omega, t) = \cancel{k \tilde{W}} -\cancel{ k \tilde{W}} - k \omega \tilde{W}' - D \omega^2 \tilde{W} = -k \omega \tilde{W}'(\omega, t) - D \omega^2 \tilde{W}(\omega, t)
    \end{align*}
    Rearranging:
    \begin{align} \label{eqn:pde-h1}
        k w \partial_w \tilde{W}(\omega, t) + \partial_t \tilde{W}(\omega , t) = - k \omega \tilde{W}(\omega ,t)
    \end{align}
    This is a \textit{linear} first order partial differential equation. One way to solve it is by using the \textit{method of characteristics}.   

    \begin{expl}
        \textbf{Method of charactersitics}. Consider a general \textit{quasilinear} PDE:
        \begin{align}
            a(x,y,z) \pdv{z}{x} + b(x,y,z) \pdv{z}{y} = c(x,y,z) \label{eqn:pde-quasilinear}
        \end{align}
        \textit{Quasilinear} means that $a$ and $b$ can depend also on the dependent variable $z$, and not only on the independent variables $x,y$. A solution $z=z(x,y)$ is, geometrically, a \textit{surface graph} immersed in $\mathbb{R}^3$. Note that the normal at any point is the gradient of $f(x,y,z) = z(x,y) - z$, that is:
        \begin{align*}
            \bm{\nabla} f(x,y,z) = \left(\pdv{z}{x} (x,y), \pdv{z}{y} (x,y), -1\right)
        \end{align*}  
        Rearranging (\ref{eqn:pde-quasilinear}) we can rewrite it as a dot product:
        \begin{align}\label{eqn:pde-h}
            \bm{v} \cdot \bm{\nabla}f = 0 \qquad \bm{v} = \left(a(x,y,z), b(x,y,z), c(x,y,z)\right)^T
        \end{align}
        This means that, at any point $(x,y,z)$, the graph $f(x,y,z)$ is \textit{tangent} to the vector field $\bm{v} \colon \mathbb{R}^3 \to \mathbb{R}^3$, $(x,y,z) \mapsto \bm{v}(x,y,z)$.
        
        So, we can consider a set of parametric curves $t \mapsto (x(t), y(t), z(t))$, and \textit{impose} the tangency condition:
        \begin{align*}
            \begin{dcases}
                \dv{x}{t} = a(x,y,z)\\
                \dv{y}{t} = b(x,y,z)\\
                \dv{z}{t} = c(x,y,z)
            \end{dcases}
        \end{align*} 
        This will result in $3$ parametric equations in $t$. If we are able to solve one of the first two for $t$, we can substitute it and get the desired cartesian form $z=z(x,y)$.
    \end{expl}
    Let $u(\omega,t)$ be a solution. Consider a parameterization $s \mapsto (\omega(s),t(s))$.
    \begin{align} \label{eqn:ode-construct}
        \dv{u}{s} (\omega(s), t(s)) = \pdv{u}{w} \dv{\omega}{s} + \pdv{u}{t}\dv{t}{s}
    \end{align}
    By confronting (\ref{eqn:ode-construct}) with (\ref{eqn:pde-h1}) we get:
    \begin{align}
        \dv{\omega}{s} = k \omega; \qquad \dv{t}{s} = 1 \label{eqn:aux-diff}
    \end{align}
    Note that now $\dd{\omega}/\dd{s}$ is exactly the left side of (\ref{eqn:pde-h1}), so:
    \begin{align} \label{eqn:du}
        \dv{u}{s} (\omega(s), t(s)) = -D \omega(s)^2 u( \omega(s),t(s))
    \end{align}
    For the boundary condition, we suppose $W(x,0) = \delta(x-x_0)$, meaning that:
    \begin{align*}
        \tilde{W}(\omega,0) = \int_{\mathbb{R}} \dd{x} e^{-i \omega x} \delta(x-x_0) = e^{-i \omega x_0} = u(\omega, 0)
    \end{align*}
    Let's fix $\omega = \omega_0$, and choose the parameterization so that $\omega(s=0) = \omega_0$ and $t(s=0)=0$ (meaning that $u(s=0) = u(\omega_0,0)$). We can now solve (\ref{eqn:aux-diff}):
    \begin{align}
        \begin{dcases}
            \dv{\omega}{s} = k \omega\\
            \omega(0) = \omega_0
        \end{dcases} \Rightarrow \omega(s) = \omega_0 e^{ks} \qquad \begin{dcases}
            \dv{t}{s} = 1 \\ t(0) = 0
        \end{dcases}
    \Rightarrow t(s) = s \label{eqn:aux-sol}
    \end{align}
    Finally we can substitute in (\ref{eqn:du}) and solve it:
    \begin{align*}
        \dv{u}{s} = -D \omega_0^2 e^{2ks} u \Rightarrow \ln|u| = - D \omega_0^2 \int e^{2ks} \dd{s} \Rightarrow u(s) = A \exp\left(-\frac{D \omega_0^2}{2k} e^{2ks} \right)
    \end{align*}
    And imposing the boundary condition $u(0) = u(\omega_0, s)$ we get:
    \begin{align*}
        A = u(\omega_0,s) \exp\left(\frac{D \omega_0^2}{2k} \right) \Rightarrow u(\omega(s), t(s)) = u(\omega_0,s) \exp \left[\frac{D \omega_0^2}{2k} (1- e^{2 k s}) \right]
    \end{align*}
    Note that this solution is expressed as a function of the parameter $s$, and a starting point $\omega_0$. By expressing these two as a function of $\omega$ and $t$, we can recover the desired $u(\omega,t)$. To do this, we can simply invert the two solutions (\ref{eqn:aux-sol}), obtaining:
    \begin{align*}
        \begin{cases}
            \omega_0 = \omega e^{-ks} \\
            s = t
        \end{cases} \Rightarrow \begin{cases}
            \omega_0 = \omega e^{-kt}\\
            s = t
        \end{cases} 
    \end{align*}
    So that:
    \begin{align*}
        u(\omega,t) \equiv \tilde{W}(\omega,t) &= \exp(-i x_0 \omega e^{-kt}) \exp \left[\frac{D \omega^2 e^{-2kt}}{2k} (1-e^{2kt}) \right] =\\
        &= \exp(-i x_0 \omega e^{-kt}) \exp \left[-\frac{D \omega^2}{2k}(1-e^{-2kt}) \right]
    \end{align*}
    All that's left is to perform a Fourier anti-transform to obtain $W(\omega,t)$:
    \begin{align*}
        W(\omega,t) &= \mathcal{F}^{-1}[\tilde{W}] = \frac{1}{2\pi} \int_{\mathbb{R}} \dd{\omega} e^{i \omega x} \exp(-i x_0 \omega e^{-kt}) \exp \left[-\frac{D \omega^2}{2k}(1-e^{-2kt}) \right] =\\
        &= \frac{1}{2\pi} \int_{\mathbb{R}} \dd{\omega} \exp\left(-\underbrace{\frac{D}{2k}(1-e^{-2kt})}_{a}  \omega^2 + \underbrace{i(x-x_0 e^{-kt})}_{b} \omega \right)  =\\
        &= \frac{1}{2 \pi} \sqrt{\frac{\pi}{a} } \exp(\frac{b^2}{4a} ) = \sqrt{\frac{k}{2 \pi D (1-e^{-2kt})} } \exp\left(-\frac{k}{2D} \frac{[x-x_0 e^{-kt}]^2}{1-e^{-2kt}} \right)
    \end{align*}
    Which is exactly the same solution found in (\ref{eqn:harmonic-propagator-sol}).
\end{exo}

\begin{exo}[Multidimensional Fokker-Planck]
    Derive the multidimensional Fokker-Planck equation associated to the Langevin equation:
    \begin{align} \label{eqn:Langevin-d}
        \dd{x^\alpha(t)} = f^\alpha(\bm{x}(t),t) \dd{t} + \sqrt{2 D_\alpha (\bm{x}(t),t)} \dd{B^\alpha(t)} \qquad 1 \leq \alpha \leq n
    \end{align}

    \medskip

    \textbf{Solution}. We wish to derive from (\ref{eqn:Langevin-d}) a PDE involving the multi-dimensional pdf $W(\bm{x},t)$. To do this, we consider an \textit{ensemble} of paths generated by (\ref{eqn:Langevin-d}), from which we can compute average values, that we can compare with the analogues obtained using $W(\bm{x},t)$, thus reaching the desired relation.

    First, we consider a generic non-anticipating \textit{test function} $h(\bm{x}(t)) \colon \mathbb{R}^n \to \mathbb{R}$ to be averaged. It's average is, by definition:
    \begin{align*}
        \langle h(\bm{x}(t)) \rangle = \int_{\mathbb{R}} \dd[n]{\bm{x}} W(\bm{x},t) h(\bm{x})
    \end{align*} 
    To construct the ODE, we need the time derivative:
    \begin{align}
        \dv{t} {\langle h(\bm{x}(t)) \rangle} = \int_{\mathbb{R}} \dd[n]{\bm{x}} \dot{W}(\bm{x},t) h(\bm{x}) \label{eqn:dt-1}
    \end{align}
    We can construct this same derivative starting from (\ref{eqn:Langevin-d}). First consider the differential, i.e. the first order \textit{change} of $h(\bm{x}(t))$ after a change of the argument $t \to t+\dd{t}$. We start by considering a change in $\bm{x} \to \bm{x}+\dd{\bm{x}}$, and then use (\ref{eqn:Langevin-d}) to express $\dd{\bm{x}}$ in terms of $\dd{t}$. Note that Ito's rules imply that $\dd{x^\alpha}\dd{x^\beta} = \dd{t} \delta_{\alpha \beta}$ which is linear in $\dd{t}$ and needs to be considered - meaning that we need to expand the $\bm{x}$ differential up to \textit{second} order:  
    \begin{align*}
        \dd{h(\bm{x}(t))} &= h(\bm{x}(t) + \dd{\bm{x}(t)}) - h(\bm{x}(t)) = \\
        &= \cancel{h(\bm{x}(t))} + \sum_{\alpha=1}^n \pdv{h(\bm{x})}{x^\alpha} \dd{x^\alpha} + \frac{1}{2} \sum_{\alpha, \beta=1}^n \pdv{h(\bm{x})}{x^\alpha}{x^\beta} \dd{x^\alpha}\dd{x^\beta}  - \cancel{h(\bm{x}(t))} + O([\dd{x}]^3)
    \end{align*}
    Note that:
    \begin{align*}
        \dd{x^\alpha}\dd{x^\beta} &= (f^\alpha \dd{t} + \sqrt{2D_\alpha} \dd{B^\alpha})(f^\beta \dd{t} + \sqrt{2 D_\beta} \dd{B^\beta}) =\\
        &= 2 D_\alpha D_\beta\dd{t} \delta_{\alpha \beta}+ O(\dd{t}^2) + O(\dd{t} \dd{B}) = 2 D_\alpha^2 \dd{t} \delta_{\alpha \beta} + O(\dd{t}^{3/2})
    \end{align*}
    And so:
    \begin{align*}
        \dd{h(\bm{x}(t))} &= \sum_{\alpha=1}^n \pdv{h(\bm{x})}{x^\alpha} (f^\alpha \dd{t} + \sqrt{2 D_\alpha} \dd{B}^\alpha) + \frac{1}{\cancel{2}} \sum_{\alpha=1}^n \pdv[2]{h(\bm{x})}{(x^\alpha)} \cancel{2} D_\alpha^2 \dd{t} = \\
        &= \dd{t} \left[\sum_{\alpha=1}^n \pdv{h(\bm{x})}{x^\alpha} f^\alpha + D_\alpha^2 \pdv[2]{h(\bm{x})}{(x^\alpha)} \right] + \sum_{\alpha=1}^n \sqrt{2 D_\alpha}\pdv{h(\bm{x})}{x^\alpha} \dd{B^\alpha}
    \end{align*}
    Taking the expected value:
    \begin{align*}
        \dd{\langle h(\bm{x}(t)) \rangle} &= \langle \dd{t} \left[\sum_{\alpha=1}^n \pdv{h(\bm{x})}{x^\alpha} f^\alpha + D_\alpha^2 \pdv[2]{h(\bm{x})}{(x^\alpha)} \right]  \rangle + \langle  \sum_{\alpha=1}^n \sqrt{2 D_\alpha}\pdv{h(\bm{x})}{x^\alpha} \dd{B^\alpha} \rangle =\\
        &\underset{(a)}{=}  \dd{t} \langle \left[\sum_{\alpha=1}^n \pdv{h(\bm{x})}{x^\alpha} f^\alpha + D_\alpha^2 \pdv[2]{h(\bm{x})}{(x^\alpha)} \right]  \rangle  + \sum_{\alpha=1}^n \langle \sqrt{2 D_\alpha}\pdv{h(\bm{x})}{x^\alpha} \rangle\underbrace{ \langle \dd{B^\alpha} \rangle}_{0} =\\
        &=\dd{t} \langle \left[\sum_{\alpha=1}^n \pdv{h(\bm{x})}{x^\alpha} f^\alpha + D_\alpha^2 \pdv[2]{h(\bm{x})}{(x^\alpha)} \right]  \rangle   
    \end{align*}
where in (a) we applied the linearity of the expected value, and then used the fact that $h$ and $D_\alpha$ are non-anticipating, meaning that they are independent of $\dd{B^\alpha}$, leading to a factorization. 

Finally, dividing by $\dd{t}$ and writing explicitly the averages leads to the desired time derivative:
\begin{align*}
    \dv{\langle h(\bm{x}(t)) \rangle}{t} =  \int_{\mathbb{R}^n} \dd[n]{\bm{x}} 
    W(\bm{x},t) \left(\sum_{\alpha=1}^n f^\alpha \pdv{h(\bm{x})}{x^\alpha}\right) + \int_{\mathbb{R}^n} \dd[n]{\bm{x}} W(\bm{x},t) \left(\sum_{\alpha=1}^n D_\alpha^2 \pdv[2]{h(\bm{x})}{(x^\alpha)}
    \right )
\end{align*}
With a repeated integration by parts we can \textit{move} the derivatives on the $W(x,t)$, allowing to factorize $h(\bm{x})$. This is done by exploiting the fact that $h(\bm{x})$ has compact support (as it is a test function), and so:
\begin{align*}
    \int_{\mathbb{R}^n} \dd[n]{\bm{x}} W(\bm{x},t) \left(\sum_{\alpha=1}^n f^\alpha \pdv{h(\bm{x})}{x^\alpha}\right) &= -\int_{\mathbb{R}^n} \dd[n]{\bm{x}} h(\bm{x})\left[ \sum_{\alpha=1}^n \pdv{x^\alpha}\left(W(\bm{x},t) f^\alpha\right) \right] +\\
    &\quad \> \int_{\mathbb{R}^{n-1}} \dd[n-1]{\bm{x}} h(\bm{x}) W(\bm{x},t) \sum_{\alpha=1}^n f^\alpha \Big|_{x^\alpha = -\infty}^{x^\alpha = +\infty}
\end{align*}
and the boundary term vanishes. A similar procedure holds for the second integral:
\begin{align*}
    \int_{\mathbb{R}^n} \dd[n]{\bm{x}} W(\bm{x},t) \left(\sum_{\alpha=1}^n D_\alpha^2 \pdv[2]{h(\bm{x})}{(x^\alpha)} 
    \right ) = \int_{\mathbb{R}^n} \dd[n]{\bm{x}} h(\bm{x}) \sum_{\alpha=1}^n \pdv[2]{(x^\alpha)}[D_\alpha^2 W(\bm{x},t)]
\end{align*}
This leads to:
\begin{align} \nonumber
    \dv{\langle h(\bm{x}(t)) \rangle}{t} &= -\int_{\mathbb{R}^n} \dd[n]{\bm{x}} h(\bm{x})\left[ \sum_{\alpha=1}^n \pdv{x^\alpha}\left(W(\bm{x},t) f^\alpha\right) \right]  +\\
    &\quad \> \int_{\mathbb{R}^n} \dd[n]{\bm{x}} h(\bm{x}) \sum_{\alpha=1}^n \pdv[2]{(x^\alpha)}[D_\alpha^2 W(\bm{x},t)]\label{eqn:dt-2}
\end{align}
Then we equate (\ref{eqn:dt-1}) and (\ref{eqn:dt-2}):
\begin{align*}
    \int_{\mathbb{R}} \dd[n]{\bm{x}} \dot{W}(\bm{x},t) h(\bm{x}) = \int_{\mathbb{R}^n} \dd[n]{\bm{x}} h(\bm{x}) \sum_{\alpha=1}^n \Big[\pdv[2]{(x^\alpha)}[D_\alpha^2 W(\bm{x},t)] -  \pdv{x^\alpha}\left(W(\bm{x},t) f^\alpha\right)\Big]
\end{align*}
This equality holds for \textit{any} $h(\bm{x})$, meaning that the integrands themselves (without the test function) must be everywhere equal:
\begin{align*}
    \dot{W}(\bm{x},t) = \sum_{\alpha=1}^n \Big[\pdv[2]{(x^\alpha)}[D_\alpha^2 W(\bm{x},t)] -  \pdv{x^\alpha}\left(W(\bm{x},t) f^\alpha\right)\Big]
\end{align*}
If we suppose $D_\alpha$ to be independent of $\bm{x}$, we could rewrite this relation in a nicer vector form:
\begin{align*}
    \dot{W}(\bm{x},t) = \norm{\bm{D}}^2 \nabla^2 W(\bm{x},t) - \bm{\nabla} \cdot (W(\bm{x},t) \cdot \bm{f})
\end{align*}
where $\bm{D} = (D_1, \dots, D_n)^T$.
    
\end{exo}

\begin{exo}[Underdamped Wiener measure]
    Derive the discretized Wiener measure for the underdamped Langevin equation:
    \begin{align*}
        m \dd{\bm{v}(t)} = (-\gamma \bm{v} + \bm{F}(\bm{r}))\dd{t} + \gamma \sqrt{2 D} \dd{\bm{B}}
    \end{align*}
    and discuss the formal continuum limit.

    \medskip

    \textbf{Solution}. The equation can be rewritten as a system of two first order SDE:
    \begin{align*}
        \begin{dcases}
            \dd{\bm{x}(t)} = \bm{v}(t) \dd{t}\\
            \dd{\bm{v}(t)} = \left[-\frac{\gamma}{m} \bm{v}(t) + \bm{f}(\bm{r})\right] \dd{t} + \frac{\gamma}{m} \sqrt{2D} \dd{\bm{B}} 
        \end{dcases}
    \end{align*}
    with $\bm{f}(\bm{r}) = \bm{F}(\bm{r})/m$.

    It is convenient to \q{symmetrize} the system, by adding an \textit{independent} stochastic term in the first equation:
    \begin{align*}
        \begin{dcases}
            \dd{\bm{x}(t)} = \bm{v}(t) \dd{t}+ \sqrt{2\hat{D}} \dd{\bm{\hat{B}}}\\
            \dd{\bm{v}(t)} = \left[-\frac{\gamma}{m} \bm{v}(t) + \bm{f}(\bm{r})\right] \dd{t} +\frac{\gamma}{m} \sqrt{2D} \dd{\bm{B}} 
        \end{dcases}
    \end{align*} 
    In this way, we can write a joint pdf for both the position and velocity increments, and then take the limit $\hat{D} \to 0$. As the $\dd{\bm{x}}$ are \textit{deterministic}, we expect them to follow a $\delta$ distribution.
    
    Explicitly, we introduce a discretization $\{t_{i}\}_{i=0,\dots,n}$, with fixed endpoints $t_0 \equiv 0$, $t_n \equiv t$. Following Ito's prescription, the equations become:
    \begin{align}
        \begin{dcases}
            \bm{\Delta x_i} = \bm{v_{i-1}} \Delta t + \sqrt{2 \hat{D}} \bm{\Delta \hat{B_i}}\\
            \bm{\Delta v_i} = \left[-\frac{\gamma}{m} \bm{v_{i-1}} + \bm{f}(\bm{x_{i-1}}) \right] \dd{t} +\frac{\gamma}{m} \sqrt{2 D} \bm{\Delta B_i}
        \end{dcases} \label{eqn:system1}
    \end{align}
    With the usual notation $\bm{x}_j \equiv \bm{x}(t_j)$. The joint pdf for all the increments is:
    \begin{align*}
        \dd{P}(\bm{\Delta B_1}, \bm{\Delta \hat{B}_1}, \dots, \bm{\Delta B_n}, \bm{\Delta \hat{B}_n}) &= \left(\prod_{i=1}^n \frac{\dd[3]{ \bm{\Delta B_i}}}{(2 \pi \Delta t_i)^{3/2}} \frac{\dd[3]{\bm{\Delta \hat{B}_i}}}{(2 \pi \Delta t_i )^{3/2}}  \right) \cdot\\
        &\quad \> \cdot \exp\left(-\frac{1}{2} \sum_{i=1}^n \frac{\norm{\bm{\Delta B_i}}^2 + \norm{\bm{\Delta \hat{B}_i}}^2}{\Delta t_i}  \right)
    \end{align*}
    where $\dd[3]{\bm{\Delta B_i}} \equiv \prod_{\alpha = 1}^3 \dd{\Delta B_i^\alpha}$ (product of the differential of each component of the $d=3$ vector $\bm{\Delta B_i}$).

    To get the distribution of the position and velocity increments we perform a change of random variables, inverting (\ref{eqn:system1}):
    \begin{align*}
        \bm{\Delta \hat{B_i}} &= \frac{\bm{\Delta x_i} - \bm{v_{i-1} } \Delta t}{\sqrt{2 \hat{D}}} \\
        \bm{\Delta {B}_i} &= \frac{m}{\gamma \sqrt{2 D}} \left(\bm{\Delta v_i } + \left[\frac{\gamma}{m} \bm{v_{i-1}} - \bm{f}(\bm{x_{i-1}}) \right] \Delta t_i \right)
    \end{align*}
    with jacobian:
    \begin{align*}
        \operatorname{det}\left |\frac{\partial \{ \Delta \hat{B}_i^\alpha \}}{ \partial \{ \Delta x_j^\beta \}}  \right| &= (2 \hat{D})^{-3n/2} \\
        \operatorname{det}\left |\frac{\partial \{ \Delta {B}_i^\alpha \}}{ \partial \{ \Delta x_j^\beta \}}  \right| &= \operatorname{det}\left |\frac{\partial \{ \Delta x_j^\beta \}}{ \partial \{ \Delta {B}_i^\alpha  \}}  \right|^{-1} = \left(\frac{\gamma^2}{m^2} 2 D \right)^{-3n/2}
    \end{align*}
    And so the final joint distribution is:
    \begin{align*}
        \dd{P(\{\bm{\Delta x_i}, \bm{\Delta v_i}\})} &= \left(\prod_{i=1}^n \hlc{Yellow}{\frac{\dd[3]\bm{\Delta x_i}}{(4 \pi \hat{D}\Delta t_i)^{3/2}}} \frac{\dd[3]{\bm{\Delta v_i}}}{(4 \pi D \Delta t_i \gamma^2/m^2)^{3/2}} \right)  \cdot\\
        &\quad \> \cdot \exp\left(-\frac{m^2}{4 D \gamma^2} \sum_{i=1}^n \norm{\frac{\bm{\Delta v_i}}{\Delta t_i} + \frac{\gamma}{m} \bm{v_{i-1}} - \bm{f}(\bm{x_{i-1}})}^2 \Delta t_i \right) \cdot \\
        &\quad \> \cdot \hlc{Yellow}{\exp\left(-\frac{1}{4 \hat{D}} \sum_{i=1}^n \norm{\frac{\bm{\Delta x_i}}{\Delta t_i} - \bm{v_{i-1}} }^2 \Delta t_i \right)}
    \end{align*}
    The highlighted terms become a $\delta(\bm{\Delta x_i} - \bm{v_{i-1}}\Delta t_i)$ in the limit $\hat{D} \to 0$ (according to the $\delta$ definition as the limit of a normalized gaussian with $\sigma \to 0$). Then, taking the continuum limit leads to:
    \begin{align*}
        \dd{P}(\{\bm{x}(\tau), \bm{v}(\tau)\}) &= \left(\prod_{\tau=0^+}^t  \dd[3]{\bm{x}(\tau)} \frac{\delta^3(\dot{\bm{x}}(\tau) - \bm{v}(\tau))}{(\dd{\tau})^3}\frac{\dd[3]{\bm{v}(\tau)}}{(4 \pi D \dd{\tau} \gamma^2/m^2)^{3/2}} \right) \cdot\\
        &\quad \> \cdot \exp \left(-\frac{m^2}{4D \gamma^2} \int_{0^+}^t \dd{\tau} \norm{\dot{\bm{v}}(\tau) + \frac{\gamma}{m} \bm{v}(\tau) - \bm{f}(\bm{x}(\tau))  }^2 \right)
    \end{align*} 
\end{exo}

\begin{exo}[Maxwell-Boltzmann consistency]
    Verify that the Maxwell-Boltzmann distribution:
    \begin{align} \label{eqn:Wstar}
        W^*(\bm{x},\bm{v}) &= \frac{1}{Z^*} \exp\left(-\beta \left[\frac{m \norm{\bm{v}}^2}{2} + V(\bm{x}) \right]\right)\\ Z^* &= \int_{\mathbb{R}^3} \dd[3]{\bm{v}} \int_{\mathcal{V}} \dd[3]{\bm{x}} \exp\left(-\beta \left[\frac{m \norm{\bm{v}}^2}{2} + V(\bm{x}) \right]\right) \nonumber
    \end{align}
    satisfies the Kramers equation:
    \begin{align} \label{eqn:Kramer}
        0 = \bm{\nabla}_{\bm{v}} \cdot \left[\left(\frac{\gamma \bm{v}}{m} - \frac{\bm{F}(\bm{x})}{m}  \right) W(\bm{x},\bm{v}) + \frac{\gamma^2 D}{m^2} \bm{\nabla}_{\bm{v}} W(\bm{x},\bm{v}) \right] - \bm{\nabla}_{\bm{x}} \cdot (\bm{v} W(\bm{x},\bm{v}))
    \end{align}
    if the noise amplitude $D$ is given by the Einstein relation:
    \begin{align*}
        D = \frac{k_B T}{\gamma} = \frac{1}{\beta \gamma} 
    \end{align*}

    \medskip

    \textbf{Solution}. The idea is to just substitute (\ref{eqn:Wstar}) in (\ref{eqn:Kramer}). First we compute the relevant \textit{blocks}: 
    \begin{align*}
        \grad_{\bm{v}} W^*(\bm{x},\bm{v}) &= - \beta m \bm{v} W^*(\bm{x},\bm{v})\\
        \grad_{\bm{x}} \cdot (\bm{v} W^*) &\underset{(a)}{=}  \bm{v} \cdot \grad_{\bm{x}} W^* = -\bm{v} \cdot W^* \beta \grad_{\bm{x}} V = \beta W^* \bm{v} \cdot \bm{F}
    \end{align*}
    where in (a) we used:
\begin{align*}
    \grad \cdot \bm{a} f(\bm{x}) = \sum_{i=1}^d \pdv{x_i} [a_i f(\bm{x})] = \sum_{i=1}^d a_i \pdv{f}{x_i} (\bm{x}) = \bm{a} \cdot \grad f \qquad \bm{a} \in \mathbb{R}^d \text{ constant}
\end{align*}

    Substituting in (\ref{eqn:Kramer}):
    \begin{align*}
        \grad_{\bm{v}} \cdot  \left( \left[\frac{\gamma \bm{v}}{m} - \frac{\bm{F}(\bm{x})}{m} - \frac{\gamma^2 D}{m^2} \beta m \bm{v} \right] W^*(\bm{x},\bm{y}) \right) - \beta W^* \bm{v} \cdot \bm{F} = \span \\
        &= \grad_{\bm{v}} \cdot \left[ W^*\left(\frac{\gamma}{m} - \frac{\gamma^2 D \beta}{m}  \right) \bm{v}\right ] - \hlc{Yellow}{\grad_{\bm{v}} \cdot \frac{\bm{F}}{m}W^*}  - \beta W^* \bm{v} \cdot \bm{F} = \\
        &=  \grad_{\bm{v}} \cdot \left[ W^*\left(\frac{\gamma}{m} - \frac{\gamma^2 D \beta}{m}  \right) \bm{v}\right ] - \hlc{Yellow}{\frac{\bm{F}}{m} \cdot \grad_{\bm{v}} W^*}  - \beta W^* \bm{v} \cdot \bm{F} = \\
        &= \grad_{\bm{v}} \cdot \left[ W^*\hlc{SkyBlue}{\left(\frac{\gamma}{m} - \frac{\gamma^2 D \beta}{m}  \right) }\bm{v}\right ] + \cancel{\frac{\bm{F} \cdot \bm{v}}{m} \beta m W^*} - \cancel{\beta W^* \bm{v} \cdot \bm{F}}
    \end{align*}
    Note that the first term vanishes when the expression highlighted in blue is $0$, i.e. when:
    \begin{align*}
        \frac{\gamma}{m} - \frac{\gamma^2 D \beta}{m} = 0 \Leftrightarrow D = \frac{1}{\beta \gamma}
    \end{align*}
    which is Einstein's relation for the diffusion coefficient.
\end{exo}

\begin{exo}
    Let $P_i(t)$ be the probability that a system is found in the (discrete) state $i$ at time $t$. If $\dd{t} W_{ij}(t)$ represents the transition probability to go from state $j$ to state $i$ during the time interval $(t, t+ \dd{t})$, prove that the Master Equation governing the time evolution of the system is:
    \begin{align*}
        \dot{P}_i(t) = \sum_j (W_{ij}(t) P_j(t) - W_{ji}(t)P_i(t)) \equiv (H(t)P(t))_i 
    \end{align*}
    where $H_{ij}(t) = W_{ij}(t) - \delta_{ij} \sum_k W_{ki}(t)$.

    \begin{enumerate}
        \item If $a_i$ is an observable quantity (not explicitly dependent on time) of the system when it is in state $i$, show that:
        \begin{align*}
            \dv{\langle a \rangle_t}{t} = \langle H^T a \rangle_t
        \end{align*}
        where $\langle a \rangle_t = \sum_i P_i(t) a_i$
        \item If the initial condition is $P_i(t_0) = \delta_{i,i_0}$, the corresponding solution of the Master Equation is called propagator and it will be denoted $P_{i, i_0}(t|t_0)$. Thus $P(t|t_0)$ is a matrix satisfying:
        \begin{align*}
            \pdv{P(t|t_0)}{t} = H(t) P(t|t_0)
        \end{align*}
        Show that:
        \begin{align*}
            \pdv{P(t|t_0)}{t_0} = -P(t|t_0) H(t_0)
        \end{align*}
        \item Assume now that the transition rates do not depend on time and that an equilibrium stationary state exists. A stationary state $P^*$ satisfies the stationary condition $HP^* = 0$. An equilibrium stationary state, $P^{\mathrm{eq}}$, besides to the stationary condition, satisfies also the so called \textit{detailed balance} (DB) condition $W_{ij} P_j^{\mathrm{eq}} = W_{ji} P_i^{\mathrm{eq}}$ (explain what this means). 
        
        If $S$ is the diagonal matrix $S_{ij} = \delta_{ij} \sqrt{P_i^{\mathrm{eq}}}$ show that, as a consequence of the DB condition, the matrix $\hat{H} = S^{-1} H S$ is symmetric and semi-negative definite. Under the hypothesis that each state $i$ can be reached through a path of non-zero transition rates from any state $j$ show that the equilibrium state is unique.
    \end{enumerate}

    \textbf{Solution}. Consider a uniform time discretization $\{t_n\}_{n\in \mathbb{N}}$, with $t_n - t_{n-1} \equiv \Delta t$.
    
    Suppose we know all the probabilities $\{P_j(t_n)\}$ of the system being in any state $j \in J$ at the present time $t_n$. The probability $P_{j \to i}$ of a particle transiting from $j \to i$ at time $t_n$ is the product of the probability of the particle \textit{being} initially at $j$ ($P_j(t_n)$) and the transition probability $W_{ij}(t_n) \Delta t$:
    \begin{align*}
        P_{j \to i}(t_n) = W_{ij}(t_n)P_j(t_n) \Delta t 
    \end{align*}
    Then the probability of the system being in a certain state $i$ at the next timestep $t_{n+1}$ is just the total probability of the system arriving to $i$ at $t_{n+1}$, that is: 
    \begin{align*}
        P_i(t_{n+1}) = \sum_{j \in J} P_{j \to i}(t_n)  = \sum_{j\in J} W_{ij}(t_n)P_j(t_n) \Delta t 
    \end{align*}
    We can split the sum to highlight the probability $P_{i \to i}$ of \textit{remaining} in $i$, leading to:
    \begin{align*}
        P_{i}(t_{n+1}) = \sum_{j \in J\setminus\{i\}}W_{ij} P_j \Delta t + P_{i \to i}
    \end{align*}
    The probability of remaining is just the probability of being in $i$ and \textit{not} transitioning to any other state from $i$:
    \begin{align*}
        P_{i \to i} = P_i \left( 1- \sum_{j \in J\setminus\{i\}} W_{ji} \Delta t \right)
    \end{align*} 
    Substituting back:
    \begin{align*}
        P_{i}(t_{n+1}) = \Delta t \sum_{j \in J\setminus\{i\}} \left( W_{ij} P_j - W_{ji} P_i \right) + P_i(t_n)
    \end{align*}
    Rearranging and dividing by $\Delta t$ leads to a Newton's different quotient, which becomes a time derivative in the continuum limit $\Delta t \to 0$:
    \begin{align} \nonumber
        \frac{P_{i}(t_{n} + \Delta t)-P_i(t_n)}{\Delta t} = \sum_{j \in J\setminus\{i\}} \left(W_{ij} P_j(t_n) - W_{ji} P_i(t_n)\right) \\ \xrightarrow[\Delta t \to 0]{}  \dot{P}_i =\sum_{j \in J\setminus\{i\}} \left(W_{ij} P_j - W_{ji} P_i\right) \span   \label{eqn:dot-P}
    \end{align}
    Before continuing, we wish to rewrite $\dot{P}_i$ as a \textit{matrix multiplication}, i.e. in the form:
    \begin{align*}
        \dot{P}_i(t) = (H(t)\bm{P}(t))_i
    \end{align*} 
    for a certain $|J| \times |J|$ matrix $H$, with $\bm{P}$ being the vector with the probabilities of each state $(P_j)^T_{j \in J}$. First, notice that we can extend the sum in (\ref{eqn:dot-P}) over the entire $J$, as the term where $j=i$ vanishes:
    \begin{align*}
        \dot{P}_i = \sum_{j \in J} (W_{ij}P_j - W_{ji} P_i)
    \end{align*}
    Then we rewrite the second term as the following:
    \begin{align*}
        \sum_{j \in J} W_{ji} P_i =  \sum_{\textcolor{Red}{k} \in J} W_{\textcolor{Red}{k}i}P_i = \sum_{j \in J} \sum_{k \in J} W_{kj} P_j \delta_{ij}
    \end{align*}
    Now we can collect the $P_j$:
    \begin{align} \label{eqn:evolution-formula}
        \dot{P}_i = \sum_{j \in J} \left(W_{ij} - \delta_{ij} \sum_{k \in J} W_{kj}\right) P_j = \sum_{j \in J} H_{ij} P_j = (H(t)\bm{P}(t))_i
    \end{align}
    with:
    \begin{align*}
        H_{ij}(t) = W_{ij}(t) - \delta_{ij} \sum_{k \in J} W_{kj}(t)
    \end{align*}
    Note that $H_{ij}(t)$ differs from $W_{ij}(t)$ only on the diagonal elements, which are equal to (minus) the probability of \textit{escape} from that state:
    \begin{align*}
        H_{j j} = W_{j j} - W_{jj} - \sum_{k \neq j} W_{k j} = -\sum_{k \neq j} W_{kj}
    \end{align*} 

    \begin{enumerate}
        \item Let $A$ be an observable of the system, assuming values $a_i$ in each state $i$.
        At a fixed time $t$, the system state is described by the discrete probability distribution (or \textit{probability mass function}) $P_i(t)$. So, the average of $A$ at time $t$ is:
        \begin{align*}
            \langle a \rangle_t = \sum_{i \in J} P_i(t) a_i
        \end{align*}
        Suppose that $a_i$ does not depend on time. Differentiating:
        \begin{align*}
            \dv{\langle a \rangle_t}{t} &= \sum_{i \in J} \dot{P}_i(t) a_i \underset{(\ref{eqn:evolution-formula})}{=}  \sum_{i \in J} \sum_{j \in J} H_{ij} P_j(t) a_i =\\&= \sum_{j \in J} P_j(t) \left(\sum_{i \in J} H_{ij} a_i\right) = \sum_{j \in J} P_j (t)(H^T \bm{a})_j = \langle H^T \bm{a} \rangle_t
        \end{align*}
        where $\bm{a}$ is the vector  $(a_j)_{j \in J}^T$.
        \item The propagator $P(i,t|i_0,t_0) \equiv P_{i,i_0}(t|t_0)$ is just the transition probability from an initial defined state $i_0$ at $t_0$ to a generic state $i$ at $t$.
        
        Consider now a uniform time discretization, and construct the desired time derivative:
        \begin{align*}
            \pdv{t_0} P(i,t|i_0, t_0) = \lim_{\Delta t \to 0} \frac{P(i,t|i_0,t_0) - P(i,t|i_0, t_0 - \Delta t)}{\Delta t} 
        \end{align*}
        We choose this definition so that $t_0 - \Delta t < t_0 < t$, and we can apply the Chapman-Kolmogorov equation (that holds as the system is Markovian):
        \begin{align} \label{eqn:esck1}
            P(i,t|i_0, t_0 - \Delta t) = \sum_{i' \in J} P(i,t|i',t_0) P(i',t_0|i_0,t_0 - \Delta t)
        \end{align}
        That is, the transition probability $(i_0,t_0 - \Delta t) \to (i,t)$ can be obtained by \textit{splitting} the path into two steps $(i_0,t_0 - \Delta t) \to (i_0,t_0)$ and $(i_0,t_0) \to (i, t)$, multiplying the two transition probabilities, and summing over all the possible intermediate states $i'$.
        
        Note now that $P(i',t_0|i_0,t_0 - \Delta t)$ is a transition probability over \textit{a single timestep}, and so can be computed using the transition probability matrix:
        \begin{align*}
            P(i',t_0|i_0,t_0 - \Delta t) &= W_{i'i_0} \Delta t
        \end{align*} 
        Substituting back in (\ref{eqn:esck1}):
        \begin{align*}
            P(i,t|i_0, t_0 - \Delta t) &=  \sum_{i' \in J} P(i,t|i',t_0) W_{i' i_0}
        \end{align*}
        As before, we highlight the case of the system remaining in the same state $i_0$:
        \begin{align*}
            &= \sum_{i' \neq i_0} P(i,t|i',t_0) W_{i' i_0} \Delta t + P(i,t|i_0,t_0) \left(1-\sum_{k \neq i_0} W_{k i_0} \Delta t\right)
        \end{align*}
        where the probability of remaining in $i_0$ is equal to the probability of \textit{not} going to any other state $k$.
        
        We can know construct the difference quotient:
        \begin{align*}
            \frac{P(i,t|i_0,t_0) - P(i,t|i_0,t_0-\Delta t)}{\Delta t} = \span\\
            &=- \sum_{i' \neq i_0} P(i,t|i',t_0) W_{i' i_0} + P(i,t|i_0,t_0) \sum_{k \neq i_0} W_{k i_0} =\\
            &=- \sum_{i' } P(i,t|i',t_0) W_{i' t_0} + \textcolor{Red}{\sum_{i'} \delta_{i' i_0}} P(i,t|\textcolor{Red}{i'},t_0) \sum_{k \neq i_0} W_{k i_0} =\\
            &=- \sum_{i' } P(i,t|i',t_0) \underbrace{\left[W_{i' i_0} - \delta_{i'i_0} \sum_{k \neq i_0} W_{k i_0}\right]}_{H_{i' i_0}} =\\
            &= -\sum_{i' } P_{i i'}(t|t_0) H_{i' i_0}(t_0)
        \end{align*} %Can the sum be extended over all i'?
        And so, taking the continuum limit $\Delta t \to 0$:
        \begin{align*}
            \pdv{P_{i i_0}(t|t_0)}{t_0} = - \sum_{i' } P_{i i'}(t|t_0) H_{i' i_0}(t_0) \Rightarrow \pdv{P(t|t_0)}{t_0} = - P(t|t_0) H(t_0)
        \end{align*}

        Where $P(t|t_0)$ is the $|J| \times |J|$ matrix with entries $P_{ij}(t|t_0)$.
        
        \item The detailed balance (DB) condition is:
        \begin{align*}
            W_{ij} P_j^{\mathrm{eq}} = W_{ji} P_i^{\mathrm{eq}}
        \end{align*}
        This means that the probability of a transition $j \to i$ is, at equilibrium, exactly the same as the probability of the inverse transition $i \to j$. In other words, every process that would change the state is \textit{exactly balanced} by its inverse process.  

        We consider now a time-independent $W_{ij}$, and the diagonal matrix $S_{ij} = \delta_{ij} \sqrt{P_i^{\mathrm{eq}}}$. Then:
        \begin{align*}
            \hat{H} = S^{-1} H S \Rightarrow 
            \hat{H}_{ij} = \sum_{ks}  \frac{1}{\sqrt{P_i^{\mathrm{eq.}} }} \delta_{ik}H_{ks} \delta_{sj} \sqrt{P_j^{\mathrm{eq.}}} = \sqrt{\frac{P_j^{\mathrm{eq} }}{P_i^{\mathrm{eq}}} } H_{ij}
        \end{align*} 
        To prove that $\hat{H}$ is symmetric, we need to show that the off-diagonal elements remain the same after inverting $j \leftrightarrow i$. That is:
        \begin{align*}
            (\hat{H}^T)_{ij} = \hat{H}_{ji} = \sqrt{\frac{P_i^{\mathrm{eq} }}{P_j^{\mathrm{eq}}} } H_{ji} \overset{?}{=}  \sqrt{\frac{P_j^{\mathrm{eq} }}{P_i^{\mathrm{eq}}} } H_{ij}
        \end{align*}
        Recall that:
        \begin{align*}
            H_{ij} = W_{ij} - \delta_{ij} \sum_k W_{ki}
        \end{align*}
        meaning that for $i \neq j$, $H_{ij} = W_{ij}$. So:
        \begin{align*}
            \sqrt{\frac{P_i^{\mathrm{eq} }}{P_j^{\mathrm{eq}}} } W_{ji} \overset{?}{=}  \sqrt{\frac{P_j^{\mathrm{eq} }}{P_i^{\mathrm{eq}}} } W_{ij} \Leftrightarrow  W_{ji} P_i^{\mathrm{eq}} \overset{?}{=} W_{ij} P_{j}^{\mathrm{eq}}
        \end{align*}
        And the latter is exactly the DB condition, and so DB implies $\hat{H}$ symmetric.

        To check if $\hat{H}$ is negative definite, we need to show that:
        \begin{align*}
            \sum_{ij} x_i \hat{H}_{ij} x_j \leq 0 \qquad \forall \bm{x} \in \mathbb{R}^{|J|} \setminus \{\bm{0}\} 
        \end{align*}
        Expanding:
        \begin{align*}
            \sum_{ij} x_i \hat{H}_{ij} x_j &= \sum_{ij} x_i \sqrt{\frac{P_j^{\mathrm{eq} }}{P_i^{\mathrm{eq} }} } H_{ij} x_j = \sum_{ij} x_i \sqrt{\frac{P_j^{\mathrm{eq} }}{P_i^{\mathrm{eq} }} } W_{ij} x_j - \sum_{i} x_i^2  \sqrt{\cancel{\frac{P_i^{\mathrm{eq} }}{P_i^{\mathrm{eq} }}} } \sum_k W_{ki} =\\
            &= \sum_{ij} \left( \sqrt{\frac{P_j^{\mathrm{eq} }}{P_i^{\mathrm{eq} }} } W_{ij} x_i x_j - x_i^2 W_{ji}\right)
        \end{align*}
        As $\hat{H}$ is symmetric:
        \begin{align*}
            \sum_{ij} x_i \hat{H}_{ij} x_j &= \sum_{ij} x_j \hat{H}_{ji} x_i = \sum_{ij} \left( \sqrt{\frac{P_i^{\mathrm{eq} }}{P_j^{\mathrm{eq} }} } W_{ji} x_i x_j - x_j^2 W_{ij}\right) =\\
            & \underset{(a)}{=} \sum_{ij} \left( \sqrt{\frac{P_j^{\mathrm{eq} }}{P_i^{\mathrm{eq} }} } W_{ij} x_i x_j -  x_j^2 W_{ij}\right)
        \end{align*}
        where in (a) we used (DB), or more precisely:
        
        \begin{align}
            \label{eqn:Db-conv}
            W_{ji} = W_{ij} \frac{P_j^{\mathrm{eq}}}{P_i^{\mathrm{eq} }}
        \end{align}

        So the sum of the \q{two versions} of the product will be exactly two times the original sum:
        \begin{align*}
            \sum_{ij} x_i \hat{H}_{ij} x_j &= \frac{1}{2} \sum_{ij} \left[x_i \hat{H}_{ij} x_j +  \sum_{ij} x_j \hat{H}_{ji} x_i \right] =\\
            &= \frac{1}{2} \sum_{ij} \left[2 \sqrt{\frac{P_j^{\mathrm{eq} }}{P_i^{\mathrm{eq} }} } W_{ij} x_i x_j - x_i^2 W_{ji} - x_j^2 W_{ij}\right] =\\
            &\underset{(\ref{eqn:Db-conv})}{=} \frac{1}{2} \sum_{ij} \left[ 2 \sqrt{\frac{P_j^{\mathrm{eq} }}{P_i^{\mathrm{eq} }} } W_{ij} x_i x_j - x_i^2 \frac{P_j^{\mathrm{eq}}}{P_i^{\mathrm{eq} }} W_{ij} - x_j^2 W_{ij} \right] =\\
            &= -\frac{1}{2} \sum_{ij} \left [x_i \sqrt{\frac{P_j^{\mathrm{eq}}}{P_i^{\mathrm{eq} }}} + x_j \right ]^2 W_{ij} \leq 0
        \end{align*}
        As $W_{ij} \geq 0$. \\


        All that's left is to show that the equilibrium state is unique, under the hypothesis that each state $i$ can be reached through a path of non-zero transition rates from any state $j$.
        The idea is to get an explicit formula for the equilibrium $\bm{P^*}$ distribution, depending only on the transition matrix $W$, meaning that $\bm{P^*}$ is uniquely determined from the start.

        Recall the detailed balance relation:
        \begin{align*}
            W_{ij} P_{j}^* = W_{ji} P_i^*
        \end{align*}
        Supposing that $W_{ij} \neq 0$, we can rewrite it as:
        \begin{align*}
            P_j^* = \frac{W_{ji}}{W_{ij}} P_i^* \equiv \frac{W(i \to j)}{W(j \to i)} P_i^*
        \end{align*}
        So the probability of the system being at $j$ at equilibrium is proportional to a \textit{rate of transition flows}, i.e. the ratio between the \textit{arriving} flow $W(i \to j)$ and the \textit{leaving} flow $W(j \to i)$.
        
        In general, not all states have $W_{ij} \neq 0$, i.e. there's no direct transition from $i$ to $j$ or viceversa. Suppose, however, that $j$ is connected to $i$ by an intermediate state $a_1$. So, by reiterating the detailed balance condition:
        \begin{align*}
            P_j^* = \frac{W(a_1 \to j )}{W(j \to a_1)} P_{a_1}^* = \frac{W(a_1 \to j )}{W(j \to a_1)} \frac{W(i \to a_1)}{W(a_1 \to i)} P_i^* 
        \end{align*}
        We can generalize this to $n$ intermediate steps, i.e. a path that connects $j$ to $i$ by first going through $a_1, a_2, \dots, a_n$:
        \begin{align*}
            P_j^* = \underbrace{\frac{W(i \to a_1 ) W(a_1 \to a_2 ) \cdots W(a_n \to j)}{W(j \to a_n) W(a_n \to a_{n-1} ) \cdots W(a_1 \to i)}}_{f_j} P_i^*
        \end{align*}
        By hypothesis, every state $i$ is connected to every other $j$ by some path with a finite number of steps and all \textit{non-zero} transition probabilities. So, if we choose the right intermediate states, the denominator in the previous expression is $\neq 0$.
        Summing over all states $j$:
        \begin{align*}
            1 \underset{(a)}{=}  \sum_{j \in J} P_j^* = \underbrace{\sum_{j \in J} f_j}_{F_j}  P_i^* \Rightarrow P_i^* = \frac{1}{F_j} 
        \end{align*}
        where in (a) we used the normalization. This is a explicit relation between the transition matrix $W$ and $\bm{P}^*$, meaning that the equilibrium distribution must be unique.
    \end{enumerate}
\end{exo}

\begin{exo}[Semi-positive definite matrix]
    Show that the matrix $D^{\omega \nu} = \sum_{\alpha =1}^d g_\alpha^\omega g^\nu_\alpha$ is semi-positive definite ($\omega, \nu = 1, \dots, k$, with $k$ and $d$ arbitrary).
    
    \medskip

    \textbf{Solution}. Let $\bm{x} \in \mathbb{R}^d \setminus \{0\}$. Then:
    \begin{align*}
        x_\omega D^{\omega \nu} x_\nu &= \sum_{\omega, \nu, \alpha = 1}^d x_{\omega} g_\alpha^\omega g_\alpha^\nu  x_\nu =\\
        &= \sum_{\alpha=1}^d \sum_{\omega = 1}^d x_\omega g_\alpha^\omega \sum_{\nu =1}^d x_\nu g^\nu_{\alpha} =\\
        &\underset{(a)}{=}  \sum_{\alpha = 1}^d \left(\sum_{\omega= 1}^d x_{\omega} g_\alpha^\omega\right)^2 \geq 0 
    \end{align*} 
    where in (a) we changed the index $\nu \to \omega$. 
\end{exo}

\begin{exo}[Discretized measure from Langevin]
    Consider the following Langevin equation:
    \begin{align*}
        \dd{x^\omega}(t) = f^\omega(\bm{x},t) \dd{t} + \sum_{\alpha= 1}^d g_{\alpha}^\omega(\bm{x},t) \dd{B^\alpha(t)} \qquad \omega=1, \dots, k
    \end{align*}
    For $k = d$ and an invertible $d\times d$ matrix $g(\bm{x},t)$, determine the discretized measure $\dd{P}_{t_1, \dots, t_n}(\bm{x}_1, \dots, \bm{x}_n|\bm{x}_0, t_0)$ and its formal continuum limit.

    \medskip

    \textbf{Solution}.  Consider a discretization $\{t_i\}_{i=1,\dots, n}$. The discretized Langevin equation in the Ito prescription becomes:
    \begin{align*}
        \Delta x^\omega_i = f^\omega(\bm{x_{i-1}}, t_{i-1}) \Delta t_i + \sum_{\alpha = 1}^d g_\alpha^\omega(\bm{x_{i-1}}, t_{i-1}) \Delta B^\alpha_i
    \end{align*}
    This can be rewritten in vector notation as:
    \begin{align} \label{eqn:vector-langevin}
        \bm{\Delta x_i} = \bm{f_{i-1}} \Delta t_i+ g_{i-1} \bm{\Delta B_i}
    \end{align}

    The joint distribution of the increments is given by:
    \begin{align*}
        \dd{P}(\bm{\Delta B_1}, \dots,\bm{\Delta B_n}) &= \left(\prod_{i=1}^n \frac{\dd[d]{ \bm{\Delta B_i}}}{(2 \pi \Delta t_i)^{d/2}}   \right) \cdot\\
        &\quad \> \cdot \exp\left(-\frac{1}{2} \sum_{i=1}^n \frac{\norm{\bm{\Delta B_i}}^2 }{\Delta t_i}  \right)
    \end{align*}
    To get the distribution of the position increments $\bm{\Delta x_i}$ we make a change of random variable by inverting (\ref{eqn:vector-langevin}), which is possible because $g$ is invertible by hypothesis:
    \begin{align*}
        \bm{\Delta B_i} = (g_{i-1})^{-1}(\bm{\Delta x_i} - \bm{f_{i-1}}\Delta t_i)
    \end{align*} 
    with jacobian:
    \begin{align*}
        \operatorname{det}\left|\pdv{\{\Delta B_i^\alpha \}}{\{ \Delta x_j^\beta \}} \right| = \prod_{i=1}^n \left|\operatorname{det}(g_{i-1})\right|^{-1} 
    \end{align*}
    (This is the determinant of a lower triangular block matrix, where each diagonal block is $g_{i-1}$ for a different $i$).

    This leads to:
    \begin{align*}
        \dd{P}(\bm{\Delta x_1}, \dots,\bm{\Delta x_n}) &= \left(\prod_{i=1}^n \frac{\dd[d]{ \bm{\Delta x_i}}}{(2 \pi \Delta t_i)^{d/2} \operatorname{det}|g_{i-1}| }   \right) \cdot\\
        &\quad \> \cdot \exp\left(-\frac{1}{2} \sum_{i=1}^n \frac{\norm{(g_{i-1})^{-1} [ \bm{\Delta x_i} - \bm{f_{i-1}}\Delta t_i]}^2 }{\Delta t_i}  \right)
    \end{align*}
    Multiplying by $\Delta t_i / \Delta t_i$ inside the exponent sum and taking the continuum limit leads to:
    \begin{align*}
        \dd{P}(\bm{x}(\tau)) &= \left(\prod_{\tau=t_0^+}^t \frac{\dd[d]{\bm{x}(\tau)}}{(2 \pi \dd{\tau} )^{d/2} \operatorname{det}|g(\bm{x}(\tau), \tau)| } \right) \cdot \\
        &\quad \> \exp\left(-\frac{1}{2} \int_{t_0}^t \dd{\tau} \norm{g(\bm{x}(\tau), \tau)^{-1} [\dot{\bm{x}}(\tau)] - {\bm{f}(\bm{x}(\tau), \tau)} }^2 \right)
    \end{align*}
\end{exo}

\begin{exo}
    Derive the Fokker-Planck equation from the Langevin equation.
    \medskip

    \textbf{Solution}. See ex. 5.4. %Add correct reference  
\end{exo}


\chapter{The Bloch Equation and the Feynman-Kac formula}
\begin{exo} Prove that:
    \begin{align*}
        \int_{t_0}^t \dd{\tau} \langle V(x(\tau), \tau) \exp\left(-\int_{t_0}^\tau \dd{s} V(x(s),s)\right) \delta(x-x(t)) \rangle_W =\\
        = \int_{t_0}^t \dd{\tau} \int_{\mathbb{R}} \dd{x'} W_B(x',\tau|x_0,t_0) V(x',\tau)W(x,t|x',\tau) \span
    \end{align*}
    where:
    \begin{align*}
        W_B(x,t|x_0,t_0) = \langle \delta(x-x(t)) \exp\left(-\int_{t_0}^t \dd{\tau} V(x(\tau), \tau)\right) \rangle_W
    \end{align*}
    and $V(x(\tau),\tau)$ is a potential.

    \begin{expl}
        Here the $\langle \cdot \rangle_W$ notation denotes the average over paths from $x(t_0) = x_0$ to $x$ at $t$, with \textit{unconstrained} end-point, which corresponds to $\langle \cdot \rangle_w$ in Maritan's notes. For the \textit{fixed end-point} case,  $\langle \cdots \delta(x-x(t))\rangle_W$ in these notes is equivalent to $\langle \cdots \rangle_W$ in Maritan's notes.
    \end{expl} 
    \medskip

    \textbf{Solution}. The equality follows if the integrands are equal, i.e. if:
    \begin{align*}
        \langle V(x(\tau), \tau) \exp\left(-\int_{t_0}^\tau \dd{s} V(x(s),s)\right) \delta(x-x(t))\rangle_W = \span\\
        &= \int_{\mathbb{R}}\dd{x'} W_B(x',\tau|x_0,t_0) V(x',\tau) W(x,t|x',\tau)
    \end{align*} 

    Expanding the average:
    \begin{align*}
        I\equiv \langle V(x(\tau), \tau) \exp\left(-\int_{t_0}^\tau \dd{s} V(x(s),s)\right) \delta(x-x(t))\rangle_W = \span\\
        &= \int_{\mathcal{C}\{x_0,t_0;t\}} \dd{_Wx V(x(\tau), \tau) \exp\left(-\int_{t_0}^\tau \dd{s} V(x(s),s)\right) \delta(x-x(t))}
    \end{align*}
    where the integral is over all paths starting at $x(t_0)=x_0$ and reaching an arbitrary end-point at $t$. The presence of the $\delta$ fixes the \textit{end-point}, leading to:
    \begin{align*}
        &=\int_{\mathcal{C}\{x_0,t_0;x,t\}} \dd{_Wx}  V(x(\tau), \tau) \exp\left(-\int_{t_0}^\tau \dd{s} V(x(s),s)\right) 
    \end{align*} 
    Now the integral is over all paths from $x(t_0) = x_0$ to $x(t) = x$. Note that $\tau$ is fixed, and so is $V(x(\tau), \tau)$ depends on the position $x(\tau)$ reached by a path after $\tau$. We can then rewrite:
    \begin{align*}
        &=\int_{\mathbb{R}} \dd{x'} \int_{\mathcal{C}\{x_0,t_0;x,t\}} \dd{_Wx} V(x',\tau) \delta(x'-x(\tau)) \exp\left(-\int_{t_0}^\tau \dd{s} V(x(s),s)\right)
    \end{align*}
    In this way, $V(x',\tau)$ can be brought out of the path integral:
    \begin{align*}
        &= \int_{\mathbb{R}}\dd{x'} V(x',\tau) \int_{\mathcal{C}\{x_0,t_0;x,t\}} \dd{_Wx}  \delta(x'-x(\tau)) \exp\left(-\int_{t_0}^\tau \dd{s} V(x(s),s)\right)
    \end{align*}
    Note how the integrand depends only on $x(s)$ with $s \leq \tau$. In other words, the paths starting at $x(\tau)$ and arriving at $x(t)$ have an \textit{unit weight}:
    \begin{align*}
        &= \int_{\mathbb{R}} \dd{x'} V(x',\tau)\underbrace{ \int_{\mathcal{C}\{x_0,t_0;x',\tau\}} \dd{_W x} \exp\left(-\int_{t_0}^\tau \dd{s} V(x(s),s)\right) }_{W_B(x',\tau|x_0,t_0)}\underbrace{\int_{\mathcal{C}\{x',\tau;x,t\}}\dd{_Wx}}_{W(x,t|x',\tau)}
    \end{align*}
    
\end{exo}

\begin{exo}
    Prove the \textit{backward Fokker-Planck} equation:
    \begin{align*}
        \partial_{t_0} W_B(x,t|x_0,t_0) = -D(\partial_{x_0}^2 - V(x_0,t_0)) W_B(x,t|x_0,t_0) 
    \end{align*} 
    in two ways:
    \begin{enumerate}
        \item Using the Bloch equation:
        \begin{align}\label{eqn:Bloch}
            \partial_t W_B(x,t|x_0,t_0) = (D\partial_x^2 - V(x,t)) W_B(x,t|x_0,t_0)
        \end{align}
        and defining a $\mathcal{L}_t$ operator so that $\partial_t W_B(t) = \mathcal{L}_t W_B(t)$ and repeated integrations over intermediate times.
        \item Using the path integral formulation
    \end{enumerate}    
    
    \medskip

    \textbf{Solution}.
    
    \begin{enumerate}
        \item We rewrite the Bloch equation in operator form:
        \begin{align} \label{eqn:time-ev}
            \partial_t W_B(t) = \mathcal{L}_t W_B(t)
        \end{align}
        where $W_B(t) \equiv W_B(x,t|x_0,t_0)$ for simplicity. $\mathcal{L}_t$ is a matrix with \textit{infinite elements}, that can act over any function $h(x)$ replicating the rhs of (\ref{eqn:Bloch}):
        \begin{align} \label{eqn:matrix-el}
            (\mathcal{L}_t h)(x) = \int_{\mathbb{R}} \dd{y} \mathcal{L}_t(x,y)h(y) = (D\partial_x^2 - V(x,t)) h(x)
        \end{align}
        where $\mathcal{L}_t(x,y)$ are the \textit{matrix elements} of $\mathcal{L}_t$. From (\ref{eqn:matrix-el}) we can see that $\mathcal{L}_t$ must be diagonal:
        \begin{align} \label{eqn:mtx-el}
            \mathcal{L}_t(x,y) = (D\partial_x^2 - V(x,t)) \delta(x-y)
        \end{align}

        \medskip

        Now we integrate (\ref{eqn:time-ev}) over $[t_0,t]$, with the initial condition $W_B(0) = W_0$:
        \begin{align*}
            \int_{t_0}^t \partial_t W_B(t) = W_B(t) - W_0 =  \int_{t_0}^t \dd{t_1} \mathcal{L}_{t_1} W_B(t_1)
        \end{align*}
        And rearranging:
        \begin{align} \label{eqn:step}
            W_B(t) = W_0 +\int_{t_0}^t \dd{t_1} \mathcal{L}_{t_1} W_B(t_1) 
        \end{align}
        We can use (\ref{eqn:step}) to evaluate $W_B(t_1)$:
        \begin{align} \label{eqn:step2}
            W_B(t_1) = W_0 + \int_{t_0}^{t_1} \dd{t_2} \mathcal{L}_{t_2} W_B(t_2)
        \end{align}
        And substituting (\ref{eqn:step2}) in (\ref{eqn:step}) we get:
        \begin{align*}
            W_B(t) = W_0 +\int_{t_0}^t \dd{t_1} \mathcal{L}_{t_1} \left(W_0 + \int_{t_0}^{t_1} \dd{t_2} \mathcal{L}_{t_2} W_B(t_2)\right)
        \end{align*}
        We can reiterate this procedure an \textit{infinite} number of times, reaching a \textbf{formal solution} of (\ref{eqn:time-ev}):
        \begin{align} \label{eqn:formal-solution1}
            W_B(t)  &= W_0 + \int_{t_0}^t \dd{t_1} \mathcal{L}_{t_1} W_0 + \int_{t_0}^{t} \dd{t_1} \int_{t_0}^{t_1} \dd{t_2} \mathcal{L}_{t_1} \mathcal{L}_{t_2} W_0 + \\ \nonumber
            &\quad \> + \int_{t_0}^t \dd{t_1} \int_{t_0}^{t_1} \dd{t_2} \int_{t_0}^{t_2} \dd{t_3} \mathcal{L}_{t_1} \mathcal{L}_{t_2} \mathcal{L}_{t_3} W_0 + \dots
        \end{align} 
        Note that each integral appearing in $W(t)$ can be interpreted as a sum over \textit{univariate paths}. For example, consider the second one:
        \begin{align}\label{eqn:int-ex}
            \int_{t_0}^{t} \dd{t_1} \int_{t_0}^{t_1} \dd{t_2} \mathcal{L}_{t_1} \mathcal{L}_{t_2} W_0 
        \end{align} 
        Here we are summing over all values of $t_1, t_2$ in the domain $[t_0,t]$ such that $t_2 < t_1$. To see this explicitly, consider the integration extrema:
        \begin{align*}
            t_0 < t_1 < t \qquad t_0 < t_2 < t_1
        \end{align*}
        In other words: evaluate $\mathcal{L}_{t_1} \mathcal{L}_{t_2} W_0$ over all possible choices of two \textit{consecutive} points $t_1, t_2$ in the segment $[t_0,t]$, and then sum all the results.
     
        Written like (\ref{eqn:int-ex}), the procedure is as follows:
        \begin{itemize}
            \item Start by choosing $t_1 \in [t_0,t]$. This will be the \textit{last} point in the segment. 
            \item Choose the second point in the \textit{preceding region}, i.e. $t_2 \in [t_0, t_1]$.
            \item Compute the integrand. 
        \end{itemize}
        Note that here we are starting \textit{from the end}, and proceeding backwards. A more natural way would be to choose a \textit{starting point} and proceed \textit{forwards}. That is:
        \begin{itemize}
            \item Choose $t_2 \in [t_0, t]$. This will be the \textit{first} point in the segment.
            \item Choose $t_1$ in the \textit{consecutive region}, i.e. $t_1 \in [t_2,t]$.   
        \end{itemize}
        This amounts to the rewriting:
        \begin{align*}
            \int_{t_0}^{t} \dd{t_2} \int_{t_2}^{t} \dd{t_1} \mathcal{L}_{t_1} \mathcal{L}_{t_2} W_0
        \end{align*}
        This procedure can be generalized to $n$ points, and so we can rewrite (\ref{eqn:formal-solution1}) as follows:
        \begin{align} \label{eqn:formal-solution2}
            W_B(t)  &= W_0 + \int_{t_0}^t \dd{t_1} \mathcal{L}_{t_1} W_0 + \int_{t_0}^t \dd{t_2} \int_{t_2}^{t} \dd{t_1} \mathcal{L}_{t_1} \mathcal{L}_{t_2} W_0 + \\ \nonumber
            &\quad \> + \int_{t_0}^t \dd{t_3} \int_{t_3}^t \dd{t_2} \int_{t_2}^t \dd{t_1} \mathcal{L}_{t_1} \mathcal{L}_{t_2} \mathcal{L}_{t_3} W_0 + \dots
        \end{align}
        
        Now it would be really nice to make all \textit{integrand extrema} equal, i.e. write, for example:
        \begin{align*}
            \int_{t_0}^t \dd{t_1} \int_{t_0}^t \dd{t_2} \mathcal{L}_{t_1} \mathcal{L}_{t_2} W_0
        \end{align*} 
        However, in order not to break causality, $\mathcal{L}_{t_1} \mathcal{L}_{t_2} W_0$ must be evaluated only with $t_2 < t_1$. This can be solved by \textit{reordering} the operators as needed. That is:
        \begin{itemize}
            \item If $t_2 < t_1$, evaluate $\mathcal{L}_{t_1} \mathcal{L}_{t_2} W_0$ as usual.
            \item If $t_1 < t_2$, evaluate $\mathcal{L}_{t_2} \mathcal{L}_{t_1} W_0$ instead.
        \end{itemize}
        To \textit{automatically} reorder the operators as needed we define the \textbf{time ordering} (meta)\textbf{operator}:
        \begin{align*}
            \mathcal{T}[\mathcal{L}_{t_1}\cdots \mathcal{L}_{t_n}] = \mathcal{L}_{p_1} \cdots \mathcal{L}_{p_n} \text{ such that } t_{p_1} > t_{p_2} > \cdots > t_{p_n}
        \end{align*}   
        So we consider:
        \begin{align*}
            \int_{t_0}^t \dd{t_1} \int_{t_0}^{t} \dd{t_2} \mathcal{T}[\mathcal{L}_{t_1} \mathcal{L}_{t_2}] W_0 
        \end{align*}
        Now the operators are in order, but we are computing \textit{twice} the integral in (\ref{eqn:int-ex})! In fact, for any choice of $t_1, t_2 \in [t_0,t]$, there are \textit{two possible orderings}, and here we are counting both of them (by properly rearranging the operators). We can correct this by dividing by $2$, or - in the general case involving the reordering of $n$ operators, by $n!$.
         
        \medskip

        At the end of this long journey, we can rewrite the formal solution (\ref{eqn:formal-solution2}) as follows:
        \begin{align} \label{eqn:formal-solution3}
            W_B(t)  &= W_0 + \int_{t_0}^t \mathcal{L}_{t_1} W_0 + \frac{1}{2!} \int_{t_0}^{t} \dd{t_1} \int_{t_0}^t \dd{t_2} \mathcal{T}[\mathcal{L}_{t_1} \mathcal{L}_{t_2}] W_0 +\\ \nonumber
            &\quad\> + \frac{1}{3!} \int_{t_0}^t \dd{t_1} \int_{t_0}^t \dd{t_2} \int_{t_0}^t \dd{t_3} \mathcal{T}[\mathcal{L}_{t_1} \mathcal{L}_{t_2} \mathcal{L}_{t_3}]  W_0 + \dots =\\
            &= \sum_{n=0}^{+\infty} \frac{1}{n!} \mathcal{T}\left[\prod_{i=1}^n \int_{t_0}^t \mathcal{L}_{t_i} \dd{t_i}\right] W_0 \equiv  \mathcal{T}\left[\exp\left(\int_{t_0}^t \mathcal{L}_\tau \dd{\tau}\right)\right]
        \end{align}
       
        We are finally arrived at a point when we can \textit{differentiate}! So, without further ado:
        \begin{align*}
            \partial_{t_0} W_B &(t)= \partial_{t_0} \mathcal{T}\left[\exp\left(\int_{t_0}^t \mathcal{L}_\tau \dd{\tau}\right)\right] W_0 =  \mathcal{T}\left[\partial_{t_0}\exp\left(\textcolor{Red}{-}\int_{\textcolor{Red}{t}}^{\textcolor{Red}{t_0}} \mathcal{L}_\tau \dd{\tau}\right)\right] W_0 =\\
            &= \mathcal{T}\left[-\exp\left(-\int_{t}^{t_0} \mathcal{L}_\tau \dd{\tau}\right)\partial_{t_0} \int_{t}^{t_0} \mathcal{L}_\tau \dd{\tau} \right]W_0 =\\
            &= -\mathcal{T}\left[\exp\left(\int_{t_0}^t \mathcal{L}_{\tau} \dd{\tau}\right) \mathcal{L}_{t_0}\right] W_0 
        \end{align*}
        
        Let $W_0 = \delta(x-x_0)$ and let's compute explicitly the matrix product:
        \begin{align*}
            \partial_{t_0} W_B(t|t_0) &= -\int_{\mathbb{R}} \dd{y} \underbrace{\mathcal{T}\left[\exp\left(\int_{t_0}^t \mathcal{L}_{\tau} \dd{\tau}\right) \mathcal{L}_{t_0}\right](x,y) }_{\text{Matrix element}}\delta(y-x_0) =\\
            &= - \mathcal{T}\left[\exp\left(\int_{t_0}^t \mathcal{L}_{\tau} \dd{\tau}\right) \mathcal{L}_{t_0}\right](x,x_0)
        \end{align*}
        Now note that $\mathcal{L}_{t_0}$ is before all the others $\mathcal{L}_\tau$, so it is already ordered - meaning that we can bring it out the $\mathcal{T}$ operator:
        \begin{align*}
            &= -\left( \mathcal{T}\left[\exp\left(\int_{t_0}^t \mathcal{L}_\tau \dd{\tau}\right)\right] \mathcal{L}_{t_0} \right)(x,x_0)
        \end{align*}
        Then we write explicitly the matrix product between the $\mathcal{T}$ block and the $\mathcal{L}_{t_0}$:
        \begin{align*}
            &=-\int_{\mathbb{R}} \dd{y} \underbrace{ \left(\mathcal{T}\left[\exp\left(\int_{t_0}^t \mathcal{L}_\tau \dd{\tau}\right)\right] \right)(x,y)}_{W(x,t|y,t_0)} \mathcal{L}_{t_0}(y,x_0) =\\
            &= -\int_{\mathbb{R}}\dd{y} W_B(x,t|y,t_0) \mathcal{L}_{t_0} (y,x_0)
        \end{align*}
        Finally, use (\ref{eqn:mtx-el}) to evaluate $\mathcal{L}_{t_0}(y,x_0)$:
        \begin{align*}
           \partial_{t_0} W_B(x,t|x_0,t_0) &= -\int_{\mathbb{R}} \dd{y} W_B(x,t|y,t_0) (D \partial_y^2 - V(y,t_0)) \delta(x_0-y) =\\
           &= -(D\partial_{x_0}^2 - V(x_0,t_0))W_B(x,t|x_0,t_0)
        \end{align*}
        which is the \textit{backward} Fokker-Planck equation. 

        \item Recall that:
        \begin{align*}
            W_B(x,t|x_0,t_0) = \langle \delta(x-x(t)) \exp\left(-\int_{t_0}^t \dd{\tau} V(x(\tau), \tau)\right) \rangle_W
        \end{align*}
        Let's introduce a \textbf{uniform}  discretization $\{t_j\}_{j=1,\dots,n+1}$ with fixed $t_0$ and $t_{n+1} \equiv t$. Then we can write $W_B(x,t|x_0,t_0)$ as the continuum limit of the discretized integral:
        \begin{align*}
            \psi_{0} = W_{B}^{(\epsilon)}(x,t_{n+1}|x_0,t_0) &= \int_{\mathbb{R}^{n+1}} \left(\prod_{i=1}^{n+1} \frac{\dd{x_i}}{\sqrt{4 \pi D \epsilon}} \right) \cdot\\
            &\quad \>\cdot  \exp\left(-\sum_{i=1}^{n+1} \frac{(x_i - x_{i-1})^2}{4D \epsilon}  - \epsilon \sum_{i=1}^{n+1} V_i\right) \delta(x_{n+1}-x)
        \end{align*}
        We add a \textit{previous} timestep $t_{-1}$ to the discretization, and consider the paths that started in $x_{-1}$ at $t_{-1}$:
        \begin{align*}
            \psi_{-1} &= W_B^{(\epsilon)}(x,t_{n+1}|x_{-1},t_{-1}) =  \int_{\mathbb{R}^{n+2}} \left(\prod_{i=\textcolor{Red}{0}}^{n+1} \frac{\dd{x_i}}{\sqrt{4 \pi D \epsilon}} \right) \cdot\\
            &\quad \>\cdot  \exp\left(-\sum_{i=\textcolor{Red}{0}}^{n+1} \frac{(x_i - x_{i-1})^2}{4D \epsilon}  - \epsilon \sum_{i=\textcolor{Red}{0}}^{n+1} V_i\right) \delta(x_{n+1}-x)
        \end{align*}
        We interpret $\psi_0$ as the \textit{evolution one timestep later} of $\psi_{-1}$, in the sense that the starting point \q{moves by one step forward}. This is analogy to what we did for the forward Bloch equation, when we considered $\psi_{n+1}$ as the evolution of $\psi_n$, in the sense that in the former the \textit{arrival point} was \textit{one timestep forward} with respect to the latter. So, while considering the \textit{arrival point} leads to the forward equation, considering the \textit{starting point} - as we are doing now - leads to the \textit{backward} one.
        
        \medskip

        The idea is now to highlight a $\psi_0$ inside of $\psi_{-1}$. As $\psi_0$ starts from $x_0$, we highlight the term with $x_0$:
        \begin{align*}
            \psi_{-1} &= \int_{\mathbb{R}^{n+2}} \left(\prod_{i=0}^{n+1}\frac{\dd{x_i}}{\sqrt{4 \pi D \epsilon}} \right) \exp\left(-\frac{(x_0-x_{-1})^2}{4 D \epsilon} - \epsilon V_0\right) \cdot\\
            &\quad \> \cdot  \exp\left(-\sum_{i=1}^{n+1} \frac{(x_i - x_{i-1})^2}{4D \epsilon}  - \epsilon \sum_{i=1}^{n+1} V_i\right) \delta(x_{n+1}-x)
        \end{align*}
        We wish to \textit{free} that term from the path integral. To do this, we \textit{rename} $x_0$ to $x'$ with a $\delta$, so that:
        \begin{align*}
            \psi_{-1} &= \frac{1}{\sqrt{4 \pi D \epsilon}} \textcolor{Red}{\int_{\mathbb{R}}\dd{x'}} \exp\left(-\frac{(\textcolor{Red}{x'}-x_{-1})^2}{4 D \epsilon}  - \epsilon V(x',t_0)\right)   \hlc{Yellow}{\int_{\mathbb{R}^{n+1}}\left(\prod_{i=1}^{n+1} \frac{\dd{x_i}}{\sqrt{4 \pi D \epsilon}} \right)\cdot}\\
            &\quad\> \hlc{Yellow}{\cdot\exp\left(-\sum_{i=1}^{n+1} \frac{(x_i - x_{i-1})^2}{4D \epsilon}  - \epsilon \sum_{i=1}^{n+1} V_i\right) \delta(x_{n+1}-x) \textcolor{Red}{\delta(x'-x_0)}} =\\
            &= \frac{1}{\sqrt{4 \pi D \epsilon}} \int_{\mathbb{R}} \dd{x'} \exp\left(-\underbrace{\frac{(x'-x_{-1})^2}{4 D \epsilon}}_{z^{2}/2} - \epsilon V(x',t_0)\right) \hlc{Yellow}{W_B^{(\epsilon)}(x,t_{n+1}|x',t_0)}
        \end{align*}  

        To simplify the gaussian we change variables:
        \begin{align*}
           \frac{z^2}{2}  = \frac{(x'-x_{-1})^2}{4 D \epsilon}  \Rightarrow z = \frac{x'-x_{-1}}{\sqrt{2D \epsilon}} \Rightarrow \dd{x'} = \sqrt{2 D \epsilon} \dd{z}; \> x'=x_{-1}+z\sqrt{2 D \epsilon}
        \end{align*}
        leading to:
        \begin{align*}
            \psi_{-1} &= \frac{\sqrt{2 D \epsilon}}{\sqrt{4 \pi D \epsilon}} \int_{\mathbb{R}} \dd{x'} \exp\left[-z^2 - \epsilon V(x_{-1}+z\sqrt{2D \epsilon})\right]\cdot\\
            &\quad\> \cdot W_B^{(\epsilon)}(x, t_{n+1}|x_{-1}+z\sqrt{2 D \epsilon},t_0)
        \end{align*}
        Then we perform a Taylor expansion about $z\sqrt{2 D \epsilon} \sim 0$ of both the potential and the solution:
        \begin{align*}
            \exp(-\epsilon V(x_{-1}+z\sqrt{2 D \epsilon})) \approx \exp(-\epsilon[ V x_{-1} + O(\epsilon^{1/2})]) \approx 1 - \epsilon V(x_{-1}) + O(\epsilon^{3/2})
        \end{align*}
        Let $W_B^{(\epsilon)}(x,t_{n+1}|x_{-1},t_0)$ be $\psi$:
        \begin{align*}
            W_B^{(\epsilon)}(x,t_{n+1}|x_{-1}+z\sqrt{2 D \epsilon},t_0) = \psi + \psi' z \sqrt{2 D \epsilon} + \psi'' z^2 2 D \epsilon
        \end{align*}
        Substituting back in the integral, and ignoring higher order terms:
        \begin{align*}
            \psi_{-1} &= \frac{1}{\sqrt{2\pi}} \int_{\mathbb{R}} \dd{x'} \exp\left(-\frac{z^2}{2} \right)[1- \epsilon V(x_{-1})](\psi + \psi' z \sqrt{2 D \epsilon} + \psi'' z^2 2 D \epsilon) =\\
            &=\frac{1}{\sqrt{2 \pi}} \int_{\mathbb{R}}\dd{x'} \exp\left(-\frac{z^2}{2} \right) \left[\psi(1-\epsilon V(x_{-1})) +z\psi'  \sqrt{2 D \epsilon}  +z^2\psi''2D \epsilon )\right]
        \end{align*}
        Note that:
        \begin{align*}
            f(z) = \frac{1}{\sqrt{2 \pi}} \exp\left(-\frac{z^2}{2} \right) 
        \end{align*}
        is a normalized gaussian with $0$ mean and unit variance. So the first moment is null, and the second is $1$, leading to:
        \begin{align*}
            \psi_{-1} = W_B(x,t|x_{-1},t_{-1}) = \span \\
            =(1-\epsilon V(x_{-1})) W_B(x,t|x_{-1}, t_0) +2 D \epsilon \partial_{x_{-1}}^2 W_B(x,t|x_{-1}, t_0) 
        \end{align*} 
        Rearranging:
        \begin{align*}
            W_B(x,t|x_{-1}, t_0) -W_B(x,t|x_{-1},t_{-1}) =\span \\
            = \epsilon V(x_{-1})W_B(x,t|x_{-1}, t_0)  - 2D\epsilon \partial_{x_{-1}}^2 W_B(x,t|x_{-1}, t_0) 
        \end{align*}
        Dividing by $\epsilon$ and taking the continuum limit $\epsilon \to 0^+$:
        \begin{align*}
            \lim_{\epsilon \to 0^+} \frac{W_B(x,t|x_{-1}, t_0) -W_B(x,t|x_{-1},t_{-1})}{\epsilon}  = \partial_{t_0} W(x,t|x_{-1},t_0) = \span \\
            &= V(x_{-1},t_0) W_B(x,t|x_{-1},t_0)) - 2 D \partial_{x_{-1}}^2 W(x,t|x_{-1},t_0)
        \end{align*}
        And renaming $x_{-1} \to x_0$ leads to the desired result.
        
    \end{enumerate}
    
\end{exo}

\begin{exo}
    Prove that:
    \begin{align*}
        W_B(\bm{x},t|\bm{x_0},t_0) = \langle \exp\left(-\int_{t_0}^t V(\bm{x}(s),s) \dd{s}\right) \delta^k(\bm{x}(t)-\bm{x})\rangle
    \end{align*}
    satisfies the backward Bloch equation:
    \begin{align} \label{eqn:backfp} 
        \partial_{t_0} W_B(\bm{x},t|\bm{x_0},t_0) =\span\\ \nonumber
        &\quad \> -\left[\sum_{\omega=1}^k \left(f(\bm{x_0},t_0) \pdv{x_{0,\omega}} + \sum_{\nu=1}^k D^{\omega \nu}(\bm{x_0},t_0)\pdv[2]{x_{0,\nu}}\right) - V(\bm{x_0},t_0)\right] W_B(\bm{x},t|\bm{x_0},t_0)
    \end{align}
    for the simplest case $k=d=1$ and $D(x,t) > 0$ generic.
    
    \medskip

    \textit{Hint}: use the discrete measure for $d=1$:
    \begin{align} \label{eqn:discr}
        \dd{\mathbb{P}}_{t_1,\dots,t_n}(\bm{x_1},\dots,\bm{x_n}|\bm{x_0},t_0) = \span\\ \nonumber
        &\quad \> \prod_{i=1}^n \prod_{\alpha = 1}^d \frac{\dd{x_i^\alpha}}{\sqrt{4 \pi D^\alpha_{i-1} \Delta t_i}} \exp\left(-\sum_{i=1}^n \sum_{\alpha=1}^d \frac{(\Delta x_i^\alpha - f^\alpha_{i-1}\Delta t_i)^2}{4 D^\alpha_{i-1} \Delta t_i} \right) 
    \end{align}
    Notice that it is easier to prove the backward Bloch (\ref{eqn:backfp}) rather than the \textit{forward one}  since a change of variable involved in the derivation does not need complicated derivations).

    \begin{expl}
        Equation (\ref{eqn:backfp}) is different from the one referenced in Maritan's notes, as the derivatives should be wrt the \textit{starting point} and not the \textit{arrival}.  
    \end{expl}
    \medskip



    \textbf{Solution}. The procedure is really similar to that used in ex. 6.2 part 2, but we now consider the dependence of $D$ on $x$ and $t$, and an added force $f(x,t)$. First we rewrite everything in the $d=1$ case. We start from:
    \begin{align} \label{eqn:Wb1}
        W_B(x,t|x_0,t_0) = \langle \exp\left(-\int_{t_0}^t V(x(s),s) \dd{s}\right) \delta(x(t)-x)\rangle
    \end{align}
    and we want to prove that:
    \begin{align*}
        \partial_{t_0} W_B(x,t|x_0,t_0) &= -\left[\left(f(x_0,t_0) \pdv{x_0} + D(x_0,t_0) \pdv[2]{x_0}\right) - V(x_0,t_0) \right]W_B(x,t|x_0,t_0)
    \end{align*}
    Introduce a uniform time discretization $\{t_j\}_{j=0,\dots,n}$, with fixed end-points and $\Delta t_i = t_{i} - t_{i-1} \equiv \epsilon$. Then, following (\ref{eqn:discr}) and adding the term $-\epsilon V_i$ for the exponential of the integral from (\ref{eqn:Wb1}), we get:
    \begin{align*}
        W_B(x,t|x_0 ,t_0) &= \lim_{\epsilon \to 0^+} W_B^{(\epsilon)}(x,t|x_0,t_0) \equiv \psi_0\\
        \psi_0 &= \int_{\mathbb{R}^n} \left(\prod_{i=1}^n \frac{\dd{x_i}}{\sqrt{4\pi D_{i-1} \epsilon}} \right) \cdot\\
        &\quad\> \cdot \exp\left(-\sum_{i=1}^n\frac{(x_i - x_{i-1} - f_{i-1} \epsilon)^2}{4 D_{i-1} \epsilon}  - \sum_{i=1}^n\epsilon V_i\right) \delta(x_n - x)
    \end{align*}
    \begin{expl}
        Note that we have $f_{i-1}$ and $D_{i-1}$, but $V_i$. This is because the first two come from a change of random variables from the Ito SDE, for which Ito's prescription applies. On the other hand, $V$ comes from the functional that we are averaging.
    \end{expl}
    As in the previous exercise, we consider the solution with the starting point \textit{a timestep in the past}, that is:
    \begin{align*} 
        \psi_{-1} &= \int_{\mathbb{R}^{n+1}} \left(\prod_{i=\textcolor{Red}{0}}^n \frac{\dd{x_i}}{\sqrt{4\pi D_{i-1} \epsilon}} \right) \cdot\\
        &\quad\> \cdot \exp\left(-\sum_{i=\textcolor{Red}{0}}^n\frac{(x_i - x_{i-1} - f_{i-1} \epsilon)^2}{4 D_{i-1} \epsilon}  - \sum_{i=\textcolor{Red}{0}}^n\epsilon V_i\right) \delta(x_n - x)
    \end{align*} 
    Then we highlight the first term (the one in $x_0$):
    \begin{align*}
        \psi_{-1} &= \int_{\mathbb{R}^{n+1}} \left(\prod_{i=0}^n \frac{\dd{x_i}}{\sqrt{4 \pi D_{i-1} \epsilon}} \right) \cdot\\
        &\quad \> \cdot \exp\left(-\frac{(x_0 - x_{-1} - f_{-1}\epsilon)^2}{4 D_{-1} \epsilon} - \epsilon V_0 \right) \cdot\\
        &\quad \> \cdot \exp\left(-\sum_{i=1}^n\frac{(x_i - x_{i-1} - f_{i-1} \epsilon)^2}{4 D_{i-1} \epsilon}  - \sum_{i=1}^n\epsilon V_i\right) \delta(x_n - x)
    \end{align*}
    Note that now the last term looks like $\psi_0$, which is what we want. We just need to bring the first term \textit{outside} the path integral - and we do this by renaming $x_0$ to $x'$ with another $\delta$:
    \begin{align*}
        \psi_{-1} &= \int_{\mathbb{R}} \frac{\dd{x'}}{\sqrt{4 \pi D_{-1} \epsilon}} \exp\left(-\frac{(x'- x_{-1} - f_{-1}\epsilon)^2}{4 D_{-1} \epsilon} - \epsilon V(x',t_0) \right) \cdot\\
        &\quad \> \hlc{Yellow}{\cdot \int_{\mathbb{R}^n} \left(\prod_{i=1}^n \frac{\dd{x_i}}{\sqrt{4 \pi D_{i-1}\epsilon}} \right) \exp\left(-\sum_{i=1}^n\frac{(x_i - x_{i-1} - f_{i-1} \epsilon)^2}{4 D_{i-1} \epsilon}  - \sum_{i=1}^n\epsilon V_i\right)\cdot}\\
        &\quad \> \hlc{Yellow}{\cdot \delta(x_n - x) \delta(x_0 - x') }=\\
        &= \int_{\mathbb{R}} \frac{\dd{x'}}{\sqrt{4 \pi D_{-1} \epsilon}} \exp\Bigg(-\underbrace{\frac{(x'- x_{-1} - f_{-1}\epsilon)^2}{4 D_{-1} \epsilon}}_{z^2/2} - \epsilon V(x',t_0) \Bigg) \hlc{Yellow}{W_B^{(\epsilon)}(x,t|x',t_0)}
    \end{align*} 
    We then perform a change of variables:
    \begin{align*}
        z = \frac{x'-x_{-1}-f_{-1}\epsilon}{\sqrt{2 D_{-1}\epsilon}}  \Rightarrow x' = x_{-1} + f_{-1} \epsilon + z \sqrt{4 D_{-1}\epsilon}
    \end{align*}
    leading to:
    \begin{align*}
        W_B^{(\epsilon)}(x,t|x_{-1},t_{-1}) &= \frac{1}{\sqrt{2 \pi}} \int_{\mathbb{R}} \dd{z} \exp\left(-\frac{z^2}{2} \right)\exp\left(-\epsilon V(x_{-1}+f_{-1}\epsilon + z\sqrt{4 D_{-1}\epsilon})\right) \cdot\\
        &\quad \> \cdot W_B(x,t|x_{-1}+f_{-1}\epsilon + z \sqrt{4 D_{-1}\epsilon},t_0)
    \end{align*}
    Finally, we perform some Taylor expansions for $f_{-1}\epsilon + z\sqrt{4 D_{-1}\epsilon} \sim 0$:
    \begin{align*}
        \exp\left(-\epsilon V(x_{-1}+f_{-1}\epsilon + z\sqrt{4 D_{-1}\epsilon})\right) &= \exp\left(-\epsilon V(x_{-1}) + O(\epsilon\sqrt{\epsilon})\right) =\\
        &=1-\epsilon V(x_{-1}) + O(\epsilon^2)
    \end{align*} 
    Let $W_B^{(\epsilon)}(x,t|x_{-1},t_0) = \psi$, and denote with $\psi'$ the first derivative wrt $x_{-1}$ (and so on). Then:
    \begin{align*}
        W_B^{(\epsilon)}(x,t|x_{-1}+f_{-1}\epsilon + z \sqrt{4 D_{-1}\epsilon},t_0) &= \psi + (f_{-1} \epsilon + z\sqrt{4 D_{-1}\epsilon}) \psi' +\\
        &\quad \> + \frac{1}{2}(f_{-1} \epsilon + z \sqrt{4 D_{-1}\epsilon})^2 \psi'' 
    \end{align*}
    Substituting back in the integrand, and neglecting everything of order $>1$ in $\epsilon$:
    \begin{align*}
        W_B^{(\epsilon)}(x,t|x_{-1},t_{-1}) &= \frac{1}{\sqrt{2 \pi}} \int_{\mathbb{R}} \dd{z} \exp\left(-\frac{z^2}{2} \right)\Bigg[\psi(1-\epsilon V(x_{-1})) + f_{-1}\epsilon \psi' +\\
        &\quad \>+  z \psi' \sqrt{4 D_{-1} \epsilon} + \frac{1}{2} z^2 4 D_{-1} \epsilon \psi'' \Bigg]
    \end{align*}
    These are all gaussian integrals involving the moments of a standard gaussian (with $0$ mean and $1$ standard deviation), and so:
    \begin{align*}
        W_B^{(\epsilon)}(x,t|x_{-1},t_{-1}) &= W_B^{(\epsilon)}(x,t|x_{-1},t_0) (1 - \epsilon V(x_{-1},t_0)) + f_{-1} \epsilon \partial_{x_{-1}} W_B^{(\epsilon)}(x,t|x_{-1},t_0)+\\
        &\quad \> + 2D_{-1} \epsilon \partial_{x_{-1}}^2 W_B^{(\epsilon)}(x,t|x_{-1},t_0)
    \end{align*}
    Rearranging, dividing by $\epsilon$ and taking the continuum limit $\epsilon \to 0^+$ finally leads to:
    \begin{align*}
        \partial_{t_0} W_B(x,t|x_0,t_0) = -[f(x_0) \partial_{x_0} + 2D(x_0)\partial_{x_0}^2 - V(x_0)]W(x,t|x_0,t_0)
    \end{align*}
\end{exo}

\setcounter{exo}{4} %Optional!

\begin{exo} Derive the analogous of the Bloch equation and of the backward Bloch equation for the Master Equation of exercise 5.7.

    \textit{Hint}: The trajectory $i(t)$ stays constant and suddenly jumps 
    at random times. Thus $\int_{t_0}^t V_{i(s)} \dd{s}$ is well defined. When evaluating the average:
    \begin{align} \label{eqn:wb2}
        W_B(i,t|i_0,t_0) = \langle \exp\left(-\int_{t_0}^t V_{i(s)} \dd{s} \right) \delta_{i(t),i}\rangle
    \end{align}
    where $W_B(i,t_0|i_0,t_0) = \delta_{i,i_0}$ and $\delta_{i,i_0}$ is the Kronecker delta, at time $t+\dd{t}$ one has to consider two contributions: one from no change of state and the other from the change of state.

    \medskip

    \textbf{Solution}. Recall that in ex. 5.7 we considered a system evolving through states $i \in J$, according to the following rule:
    \begin{align*}
        \dot{P}_i(t) = (H(t)P(t))_i \qquad H_{ij}(t) = W_{ij}(t) - \delta_{ij} \sum_{k \in J} W_{ki}(t)
    \end{align*}
    Let's consider a \textit{uniform} time discretization $\{t_j\}_{j=0,\dots,n}$ with $t_n \equiv t$ and $\Delta t_j = t_{j}-t_{j-1} \equiv \epsilon$. Introduce a potential $V \colon J \to \mathbb{R}$ and denote with $V_i$ the potential at the state $i$. We then consider a \textit{path} through states as a vector $\{i_j\}_{j=0,\dots,n}$, where $i_j \in J$ is the state explored at time $t_j$.


    Then we discretize (\ref{eqn:wb2}):
    \begin{align} \nonumber
        W_B(i,t|i_0,t_0) &= \lim_{\epsilon \to 0^+} W_B^{(\epsilon)}(i,t_n|i_0,t_0)\\ \nonumber
        W_B^{(\epsilon)}(i,t_n|i_0,t_0) &= \langle \exp\left(-\sum_{s=1}^n V_{i_s} \epsilon\right) \delta_{i_n,i}\rangle =\\
        &= \underbrace{\sum_{i_1 \in J} \cdots \sum_{i_n \in J}}_{\parbox{5em}{\centering \footnotesize Sum over all paths}} \> \underbrace{\epsilon W_{i_n,i_{n-1}} \cdots \epsilon W_{i_1,i_0}}_{\parbox{7em}{\footnotesize \centering Probability for a path}} \exp\left(-\sum_{s=1}^n V_{i_s} \epsilon\right) \underbrace{\delta_{i_n,i}}_{\parbox{3.5em}{\footnotesize\centering Fix endpoint}} \label{eqn:wb-discrete}
    \end{align}
    The average is over all \textit{discrete} paths connecting $i_0$ at $t_0$ to $i$ at $t$ (it can't be written as an integral in the Wiener measure, as the states $J$ are discrete too). 

    \medskip

    For the \textbf{forward} Bloch equation we \textit{evolve} the destination by one time-step:
    \begin{align*}
        W_B^{(\epsilon)}(i,t_{n+1}|i_0,t_0) = \psi_{n+1} = \sum_{i_1 \in J} \cdots \sum_{i_{\textcolor{Red}{n+1}} \in J} \epsilon W_{i_{\textcolor{Red}{n+1}},i_n} \cdots   \epsilon W_{i_1,i_0}  \exp\left(-\sum_{s=1}^{n+1} V_{i_s}\epsilon\right) \delta_{i_{\textcolor{Red}{n+1}},i} 
    \end{align*}  
    The sum over $i_{n+1}$ can be computed to remove the $\delta$:
    \begin{align*}
        \psi_{n+1} =\exp\left(-V_{i} \epsilon\right) \sum_{i_1 \in J} \cdots \sum_{i_n \in J} \epsilon W_{i,i_n} \cdots \epsilon W_{i_1,i_0}  \exp\left(-\sum_{s=1}^{n} V_{i_s} \epsilon\right) 
    \end{align*}

    Then we highlight the $i_n$ term:
    \begin{align*}
        \psi_{n+1} =\exp\left(-V_{i} \epsilon\right) \sum_{i_1 \in J} \cdots \sum_{i_{{n}} \in J} \hlc{Yellow}{\epsilon W_{i,i_n} }\cdots \epsilon W_{i_1,i_0}   \exp\left(-\sum_{s=1}^{n} V_{i_s} \epsilon\right) 
    \end{align*}
    To bring it out of the \textit{sum over paths} we insert a $\delta$:
    \begin{align*}
        \psi_{n+1} &= \exp(-V_{i} \epsilon) \sum_{i' \in J}   \hlc{Yellow}{\epsilon W_{i,i'}} \cdot \\
        &\quad\>\cdot \underbrace{\sum_{i_1 \in J} \cdots \sum_{i_n \in J} \epsilon W_{i_n,i_{n-1}} \cdots \epsilon W_{i_1,i_0}   \exp\left(-\sum_{s=1}^{n} V_{i_s} \epsilon\right) \delta_{i_n,i'}}_{(\ref{eqn:wb-discrete})\colon W_B(i',t|i_0,t_0)} =\\
        &=\exp(-V_i \epsilon)  \sum_{i' \in J} \epsilon W_{i,i'} W_B(i',t|i_0,t_0)
    \end{align*} 
    We now split the transition probability $W_{i',i}$ over the case $i' = i$ and $i' \neq i$, using the same trick of ex. 5.7 to express everything with the off-diagonal terms:
    \begin{align*}
        \psi_{n+1} = \exp(-V_i \epsilon) \Big(\sum_{i' \in J\setminus \{i\}} \epsilon W_{i,i'} W_B(i',t|i_0,t_0) + \Big[1-\sum_{i' \in J \setminus \{i\}}\epsilon W_{i',i}\Big] W_B(i,t|i_0,t_0)\Big)
    \end{align*}
    Note that we can extend the sums over the entire $i' \in J$, as the $i=i'$ terms cancel out.

    We then expand the exponential:
    \begin{align*}
        \exp(-V_i \epsilon) = 1 - V_1 \epsilon + O(\epsilon^2)
    \end{align*}
    Substituting back and neglecting higher order terms in $\epsilon$:
    \begin{align*}
        W_B^{(\epsilon)}(i,t_{n+1}|i_0,t_0) &= (1-V_i \epsilon)\Bigg[W_B(i,t|i_0,t_0) +\\
        &\quad \> + \epsilon \sum_{i' \in J}\Big(W_{i,i'} W_B^{(\epsilon)}(i',t|i_0,t_0) - W_{i',i} W_B^{(\epsilon)}(i,t|i_0,t_0)\Big)\Bigg]
    \end{align*}
    Rearranging, dividing by $\epsilon$ and taking the continuum limit leads to:
    \begin{align*}
        \partial_t W_B(i,t|i_0,t_0) &= \lim_{\epsilon \to 0} \frac{W_B^{(\epsilon)}(i,t_{n+1}|i_0,t_0) - W_B^{(\epsilon)}(i,t|i_0,t_0)}{\epsilon} =\\ 
        &= - V_i W_B(i,t|i_0,t_0) + \sum_{i' \in J}\Big(W_{i,i'} W_B(i',t|i_0,t_0) - W_{i',i} W_B(i,t|i_0,t_0)\Big)
    \end{align*}

    \medskip

    For the \textbf{backward} Bloch equation we start from:
    \begin{align*}
        \pdv{t_0} W_B(i,t|i_0,t_0) = \lim_{\epsilon \to 0^+} \frac{W_B(i,t|i_0,t_0) - W_B(i,t|i_0,t_{-1})}{\epsilon} 
    \end{align*}
    Applying the ESCK relation:
    \begin{align*}
        W_B(i,t|i_0,t_{-1}) = \sum_{i' \in J} W_B(i,t|i',t_0) W_B(i',t_0|i_0,t_{-1})
    \end{align*}
    The last term is the average over only one step:
    \begin{align*}
        W_B(i',t_0|i_0,t_{-1}) = \exp(-V_{i'}\epsilon) W_{i',i_0} \epsilon
    \end{align*}
    And so:
    \begin{align*}
        W_B(i,t|i_0,t_{-1}) &= \sum_{i' \in J} W_B(i,t|i',t_0)\exp(-V_{i'}\epsilon) W_{i',i_0} \epsilon =\\
        &=\sum_{i' \neq i_0} W_B(i,t|i',t_0) W_{i',i_0} \exp(-V_{i'} \epsilon) +\\
        &\quad \> +W_B(i,t|i_0,t_0) \exp(-V_{i_0} \epsilon) \left[1 - \sum_{k \neq i_0} W_{k,i_0} \epsilon\right]
    \end{align*}
    Then we compute the difference:
    \begin{align*}
        W_B(i,t|i_0,t_0) - W_B(i,t|i_0,t_{-1}) &= -\sum_{i' \neq i_0} W_B(i,t|i',t_0) W_{i',i_0} \exp(-V_{i'} \epsilon) +\\
        &\quad \> +W_B(i,t|i_0,t_0) \exp(-V_{i_0} \epsilon)  \sum_{k \neq i_0} W_{k,i_0} \epsilon
    \end{align*}
    We then add a $\delta$ to merge the two sums, and extend them to include the diagonal terms (without adding anything, because the two terms cancel out in that case):
    \begin{align*}
        &= -\sum_{i'} W_B(i,t|i',t_0) W_{i',i_0} \epsilon \exp(-V_{i'} \epsilon) + \sum_{i} \delta_{i',i_0} W_B(i,t|i',t_0) \exp(-V_{i'}\epsilon) \sum_{k \neq i_0} W_{k i_0} \epsilon =\\
        &= -\sum_{i'} W_B(i,t|i',t_0) \exp(-V_{i'}\epsilon) \epsilon[W_{i' i_0} - \delta_{i',i_0} \sum_{k \neq i_0} W_{k,i_0}] 
    \end{align*}
    Dividing by $\epsilon$ and taking the continuum limit leads to:
    \begin{align*}
        \partial_{t_0} W_B(i,t|i_0,t_0) = 
    \end{align*}

    \begin{align*}
        W_B^{(\epsilon)}(i,t_n|i_0,t_{-1}) &= \psi_{-1} = \sum_{i_0' \in J} \sum_{i_1 \in J} \cdots \sum_{i_n \in J} \epsilon W_{i_n, i_{n-1}} \cdots \epsilon W_{i_1,i_0'}\epsilon W_{i_0', i_0} \cdot \\
        &\quad \>\cdot \exp(-V_{i_0'}\epsilon)\exp\left(-\sum_{s=1}^n V_{i_s} \epsilon\right) \delta_{i_n,i} = \\
        &= \sum_{i' \in J} \exp(-V_{i'}\epsilon) \epsilon W_{i',i_0} W_B^{(\epsilon)}(i,t|i_0',t_0) =\\
        &= \sum_{i' \in J\setminus \{i_0\}} \exp(-V_{i'}\epsilon) \epsilon W_{i',i_0} W_B^{(\epsilon)}(i,t|i_0',t_0) + \\
        &\quad \> + \exp(-V_{i_0}\epsilon)\Big(1-\sum_{i' \in J\setminus \{i\}} \epsilon W_{i',i_0}\Big) W_B^{(\epsilon)}(i,t|i_0,t_0)
    \end{align*}   
    \begin{align*}
        W_B^{(\epsilon)}(i,t_n|i_0,t_0) - W_B^{(\epsilon)}(i,t_n|i_0,t_{-1}) = 
    \end{align*}
    Highlight the term with $i_0'$:
    \begin{align*}
        \psi_{-1} = \sum_{i_0 \in J} \cdots \sum_{i_n \in J} \exp(-V_{i_0} \epsilon) \epsilon W_{i_0,i_{-1}} \left[\epsilon W_{i_n, i_{n-1}} \cdots \epsilon W_{i_1,i_0}\right] \exp\left(-\sum_{s=1}^n V_{i_s} \epsilon\right) \delta_{i_n,i}
    \end{align*}
    And we extract it out the paths sum by inserting a $\delta$:
    \begin{align*}
        \psi_{-1} &= \sum_{i' \in J} \epsilon W_{i',i_{-1}} \cdot \\
        &\quad \> \cdot \sum_{i_0 \in J} \cdots \sum_{i_n \in J} \epsilon W_{i_n, i_{n-1}} \cdots \epsilon W_{i_1,i_0} \exp\left(-\sum_{s=1}^n V_{i_s} \epsilon\right) \exp(-V_{i'} \epsilon) \delta_{i_n,i} \delta_{i_0,i'}
    \end{align*}
    Now the sum over $i_0$ can be computed, eliminating a $\delta$:
    \begin{align*}
        \psi_{-1} &= \sum_{i' \in J} \epsilon W_{i',i_{-1}} \cdot \\
        &\quad \> \cdot \exp(-V_{i_0}\epsilon)\sum_{i_1 \in J}\cdots \sum_{i_n \in J} \epsilon W_{i_n, i_{n-1}}\cdots \epsilon W_{i_1, i'} \exp\left(-\sum_{s=1}^n V_{i_s} \epsilon\right)
    \end{align*}

\end{exo}


\listoftheorems

\end{document}
