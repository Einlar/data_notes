%&latex
%
\documentclass[../template.tex]{subfiles}
\begin{document}

\begin{exo} Prove that:
    \begin{align*}
        \int_{t_0}^t \dd{\tau} \langle V(x(\tau), \tau) \exp\left(-\int_{t_0}^\tau \dd{s} V(x(s),s)\right) \delta(x-x(t)) \rangle_W =\\
        = \int_{t_0}^t \dd{\tau} \int_{\mathbb{R}} \dd{x'} W_B(x',\tau|x_0,t_0) V(x',\tau)W(x,t|x',\tau) \span
    \end{align*}
    where:
    \begin{align*}
        W_B(x,t|x_0,t_0) = \langle \delta(x-x(t)) \exp\left(-\int_{t_0}^t \dd{\tau} V(x(\tau), \tau)\right) \rangle_W
    \end{align*}
    and $V(x(\tau),\tau)$ is a potential.

    \begin{expl}
        Here the $\langle \cdot \rangle_W$ notation denotes the average over paths from $x(t_0) = x_0$ to $x$ at $t$, with \textit{unconstrained} end-point, which corresponds to $\langle \cdot \rangle_w$ in Maritan's notes. For the \textit{fixed end-point} case,  $\langle \cdots \delta(x-x(t))\rangle_W$ in these notes is equivalent to $\langle \cdots \rangle_W$ in Maritan's notes.
    \end{expl} 
    \medskip

    \textbf{Solution}. The equality follows if the integrands are equal, i.e. if:
    \begin{align*}
        \langle V(x(\tau), \tau) \exp\left(-\int_{t_0}^\tau \dd{s} V(x(s),s)\right) \delta(x-x(t))\rangle_W = \span\\
        &= \int_{\mathbb{R}}\dd{x'} W_B(x',\tau|x_0,t_0) V(x',\tau) W(x,t|x',\tau)
    \end{align*} 

    Expanding the average:
    \begin{align*}
        I\equiv \langle V(x(\tau), \tau) \exp\left(-\int_{t_0}^\tau \dd{s} V(x(s),s)\right) \delta(x-x(t))\rangle_W = \span\\
        &= \int_{\mathcal{C}\{x_0,t_0;t\}} \dd{_Wx V(x(\tau), \tau) \exp\left(-\int_{t_0}^\tau \dd{s} V(x(s),s)\right) \delta(x-x(t))}
    \end{align*}
    where the integral is over all paths starting at $x(t_0)=x_0$ and reaching an arbitrary end-point at $t$. The presence of the $\delta$ fixes the \textit{end-point}, leading to:
    \begin{align*}
        &=\int_{\mathcal{C}\{x_0,t_0;x,t\}} \dd{_Wx}  V(x(\tau), \tau) \exp\left(-\int_{t_0}^\tau \dd{s} V(x(s),s)\right) 
    \end{align*} 
    Now the integral is over all paths from $x(t_0) = x_0$ to $x(t) = x$. Note that $\tau$ is fixed, and so is $V(x(\tau), \tau)$ depends on the position $x(\tau)$ reached by a path after $\tau$. We can then rewrite:
    \begin{align*}
        &=\int_{\mathbb{R}} \dd{x'} \int_{\mathcal{C}\{x_0,t_0;x,t\}} \dd{_Wx} V(x',\tau) \delta(x'-x(\tau)) \exp\left(-\int_{t_0}^\tau \dd{s} V(x(s),s)\right)
    \end{align*}
    In this way, $V(x',\tau)$ can be brought out of the path integral:
    \begin{align*}
        &= \int_{\mathbb{R}}\dd{x'} V(x',\tau) \int_{\mathcal{C}\{x_0,t_0;x,t\}} \dd{_Wx}  \delta(x'-x(\tau)) \exp\left(-\int_{t_0}^\tau \dd{s} V(x(s),s)\right)
    \end{align*}
    Note how the integrand depends only on $x(s)$ with $s \leq \tau$. In other words, the paths starting at $x(\tau)$ and arriving at $x(t)$ have an \textit{unit weight}:
    \begin{align*}
        &= \int_{\mathbb{R}} \dd{x'} V(x',\tau)\underbrace{ \int_{\mathcal{C}\{x_0,t_0;x',\tau\}} \dd{_W x} \exp\left(-\int_{t_0}^\tau \dd{s} V(x(s),s)\right) }_{W_B(x',\tau|x_0,t_0)}\underbrace{\int_{\mathcal{C}\{x',\tau;x,t\}}\dd{_Wx}}_{W(x,t|x',\tau)}
    \end{align*}
    
\end{exo}

\begin{exo}
    Prove the \textit{backward Fokker-Planck} equation:
    \begin{align*}
        \partial_{t_0} W_B(x,t|x_0,t_0) = -D(\partial_{x_0}^2 - V(x_0,t_0)) W_B(x,t|x_0,t_0) 
    \end{align*} 
    in two ways:
    \begin{enumerate}
        \item Using the Bloch equation:
        \begin{align}\label{eqn:Bloch}
            \partial_t W_B(x,t|x_0,t_0) = (D\partial_x^2 - V(x,t)) W_B(x,t|x_0,t_0)
        \end{align}
        and defining a $\mathcal{L}_t$ operator so that $\partial_t W_B(t) = \mathcal{L}_t W_B(t)$ and repeated integrations over intermediate times.
        \item Using the path integral formulation
    \end{enumerate}    
    
    \medskip

    \textbf{Solution}.
    
    \begin{enumerate}
        \item We rewrite the Bloch equation in operator form:
        \begin{align} \label{eqn:time-ev}
            \partial_t W_B(t) = \mathcal{L}_t W_B(t)
        \end{align}
        where $W_B(t) \equiv W_B(x,t|x_0,t_0)$ for simplicity. $\mathcal{L}_t$ is a matrix with \textit{infinite elements}, that can act over any function $h(x)$ replicating the rhs of (\ref{eqn:Bloch}):
        \begin{align} \label{eqn:matrix-el}
            (\mathcal{L}_t h)(x) = \int_{\mathbb{R}} \dd{y} \mathcal{L}_t(x,y)h(y) = (D\partial_x^2 - V(x,t)) h(x)
        \end{align}
        where $\mathcal{L}_t(x,y)$ are the \textit{matrix elements} of $\mathcal{L}_t$. From (\ref{eqn:matrix-el}) we can see that $\mathcal{L}_t$ must be diagonal:
        \begin{align} \label{eqn:mtx-el}
            \mathcal{L}_t(x,y) = (D\partial_x^2 - V(x,t)) \delta(x-y)
        \end{align}

        \medskip

        Now we integrate (\ref{eqn:time-ev}) over $[t_0,t]$, with the initial condition $W_B(0) = W_0$:
        \begin{align*}
            \int_{t_0}^t \partial_t W_B(t) = W_B(t) - W_0 =  \int_{t_0}^t \dd{t_1} \mathcal{L}_{t_1} W_B(t_1)
        \end{align*}
        And rearranging:
        \begin{align} \label{eqn:step}
            W_B(t) = W_0 +\int_{t_0}^t \dd{t_1} \mathcal{L}_{t_1} W_B(t_1) 
        \end{align}
        We can use (\ref{eqn:step}) to evaluate $W_B(t_1)$:
        \begin{align} \label{eqn:step2}
            W_B(t_1) = W_0 + \int_{t_0}^{t_1} \dd{t_2} \mathcal{L}_{t_2} W_B(t_2)
        \end{align}
        And substituting (\ref{eqn:step2}) in (\ref{eqn:step}) we get:
        \begin{align*}
            W_B(t) = W_0 +\int_{t_0}^t \dd{t_1} \mathcal{L}_{t_1} \left(W_0 + \int_{t_0}^{t_1} \dd{t_2} \mathcal{L}_{t_2} W_B(t_2)\right)
        \end{align*}
        We can reiterate this procedure an \textit{infinite} number of times, reaching a \textbf{formal solution} of (\ref{eqn:time-ev}):
        \begin{align} \label{eqn:formal-solution1}
            W_B(t)  &= W_0 + \int_{t_0}^t \dd{t_1} \mathcal{L}_{t_1} W_0 + \int_{t_0}^{t} \dd{t_1} \int_{t_0}^{t_1} \dd{t_2} \mathcal{L}_{t_1} \mathcal{L}_{t_2} W_0 + \\ \nonumber
            &\quad \> + \int_{t_0}^t \dd{t_1} \int_{t_0}^{t_1} \dd{t_2} \int_{t_0}^{t_2} \dd{t_3} \mathcal{L}_{t_1} \mathcal{L}_{t_2} \mathcal{L}_{t_3} W_0 + \dots
        \end{align} 
        Note that each integral appearing in $W(t)$ can be interpreted as a sum over \textit{univariate paths}. For example, consider the second one:
        \begin{align}\label{eqn:int-ex}
            \int_{t_0}^{t} \dd{t_1} \int_{t_0}^{t_1} \dd{t_2} \mathcal{L}_{t_1} \mathcal{L}_{t_2} W_0 
        \end{align} 
        Here we are summing over all values of $t_1, t_2$ in the domain $[t_0,t]$ such that $t_2 < t_1$. To see this explicitly, consider the integration extrema:
        \begin{align*}
            t_0 < t_1 < t \qquad t_0 < t_2 < t_1
        \end{align*}
        In other words: evaluate $\mathcal{L}_{t_1} \mathcal{L}_{t_2} W_0$ over all possible choices of two \textit{consecutive} points $t_1, t_2$ in the segment $[t_0,t]$, and then sum all the results.
     
        Written like (\ref{eqn:int-ex}), the procedure is as follows:
        \begin{itemize}
            \item Start by choosing $t_1 \in [t_0,t]$. This will be the \textit{last} point in the segment. 
            \item Choose the second point in the \textit{preceding region}, i.e. $t_2 \in [t_0, t_1]$.
            \item Compute the integrand. 
        \end{itemize}
        Note that here we are starting \textit{from the end}, and proceeding backwards. A more natural way would be to choose a \textit{starting point} and proceed \textit{forwards}. That is:
        \begin{itemize}
            \item Choose $t_2 \in [t_0, t]$. This will be the \textit{first} point in the segment.
            \item Choose $t_1$ in the \textit{consecutive region}, i.e. $t_1 \in [t_2,t]$.   
        \end{itemize}
        This amounts to the rewriting:
        \begin{align*}
            \int_{t_0}^{t} \dd{t_2} \int_{t_2}^{t} \dd{t_1} \mathcal{L}_{t_1} \mathcal{L}_{t_2} W_0
        \end{align*}
        This procedure can be generalized to $n$ points, and so we can rewrite (\ref{eqn:formal-solution1}) as follows:
        \begin{align} \label{eqn:formal-solution2}
            W_B(t)  &= W_0 + \int_{t_0}^t \dd{t_1} \mathcal{L}_{t_1} W_0 + \int_{t_0}^t \dd{t_2} \int_{t_2}^{t} \dd{t_1} \mathcal{L}_{t_1} \mathcal{L}_{t_2} W_0 + \\ \nonumber
            &\quad \> + \int_{t_0}^t \dd{t_3} \int_{t_3}^t \dd{t_2} \int_{t_2}^t \dd{t_1} \mathcal{L}_{t_1} \mathcal{L}_{t_2} \mathcal{L}_{t_3} W_0 + \dots
        \end{align}
        
        Now it would be really nice to make all \textit{integrand extrema} equal, i.e. write, for example:
        \begin{align*}
            \int_{t_0}^t \dd{t_1} \int_{t_0}^t \dd{t_2} \mathcal{L}_{t_1} \mathcal{L}_{t_2} W_0
        \end{align*} 
        However, in order not to break causality, $\mathcal{L}_{t_1} \mathcal{L}_{t_2} W_0$ must be evaluated only with $t_2 < t_1$. This can be solved by \textit{reordering} the operators as needed. That is:
        \begin{itemize}
            \item If $t_2 < t_1$, evaluate $\mathcal{L}_{t_1} \mathcal{L}_{t_2} W_0$ as usual.
            \item If $t_1 < t_2$, evaluate $\mathcal{L}_{t_2} \mathcal{L}_{t_1} W_0$ instead.
        \end{itemize}
        To \textit{automatically} reorder the operators as needed we define the \textbf{time ordering} (meta)\textbf{operator}:
        \begin{align*}
            \mathcal{T}[\mathcal{L}_{t_1}\cdots \mathcal{L}_{t_n}] = \mathcal{L}_{p_1} \cdots \mathcal{L}_{p_n} \text{ such that } t_{p_1} > t_{p_2} > \cdots > t_{p_n}
        \end{align*}   
        So we consider:
        \begin{align*}
            \int_{t_0}^t \dd{t_1} \int_{t_0}^{t} \dd{t_2} \mathcal{T}[\mathcal{L}_{t_1} \mathcal{L}_{t_2}] W_0 
        \end{align*}
        Now the operators are in order, but we are computing \textit{twice} the integral in (\ref{eqn:int-ex})! In fact, for any choice of $t_1, t_2 \in [t_0,t]$, there are \textit{two possible orderings}, and here we are counting both of them (by properly rearranging the operators). We can correct this by dividing by $2$, or - in the general case involving the reordering of $n$ operators, by $n!$.
         
        \medskip

        At the end of this long journey, we can rewrite the formal solution (\ref{eqn:formal-solution2}) as follows:
        \begin{align} \label{eqn:formal-solution3}
            W_B(t)  &= W_0 + \int_{t_0}^t \mathcal{L}_{t_1} W_0 + \frac{1}{2!} \int_{t_0}^{t} \dd{t_1} \int_{t_0}^t \dd{t_2} \mathcal{T}[\mathcal{L}_{t_1} \mathcal{L}_{t_2}] W_0 +\\ \nonumber
            &\quad\> + \frac{1}{3!} \int_{t_0}^t \dd{t_1} \int_{t_0}^t \dd{t_2} \int_{t_0}^t \dd{t_3} \mathcal{T}[\mathcal{L}_{t_1} \mathcal{L}_{t_2} \mathcal{L}_{t_3}]  W_0 + \dots =\\
            &= \sum_{n=0}^{+\infty} \frac{1}{n!} \mathcal{T}\left[\prod_{i=1}^n \int_{t_0}^t \mathcal{L}_{t_i} \dd{t_i}\right] W_0 \equiv  \mathcal{T}\left[\exp\left(\int_{t_0}^t \mathcal{L}_\tau \dd{\tau}\right)\right]
        \end{align}
       
        We are finally arrived at a point when we can \textit{differentiate}! So, without further ado:
        \begin{align*}
            \partial_{t_0} W_B &(t)= \partial_{t_0} \mathcal{T}\left[\exp\left(\int_{t_0}^t \mathcal{L}_\tau \dd{\tau}\right)\right] W_0 =  \mathcal{T}\left[\partial_{t_0}\exp\left(\textcolor{Red}{-}\int_{\textcolor{Red}{t}}^{\textcolor{Red}{t_0}} \mathcal{L}_\tau \dd{\tau}\right)\right] W_0 =\\
            &= \mathcal{T}\left[-\exp\left(-\int_{t}^{t_0} \mathcal{L}_\tau \dd{\tau}\right)\partial_{t_0} \int_{t}^{t_0} \mathcal{L}_\tau \dd{\tau} \right]W_0 =\\
            &= -\mathcal{T}\left[\exp\left(\int_{t_0}^t \mathcal{L}_{\tau} \dd{\tau}\right) \mathcal{L}_{t_0}\right] W_0 
        \end{align*}
        
        Let $W_0 = \delta(x-x_0)$ and let's compute explicitly the matrix product:
        \begin{align*}
            \partial_{t_0} W_B(t|t_0) &= -\int_{\mathbb{R}} \dd{y} \underbrace{\mathcal{T}\left[\exp\left(\int_{t_0}^t \mathcal{L}_{\tau} \dd{\tau}\right) \mathcal{L}_{t_0}\right](x,y) }_{\text{Matrix element}}\delta(y-x_0) =\\
            &= - \mathcal{T}\left[\exp\left(\int_{t_0}^t \mathcal{L}_{\tau} \dd{\tau}\right) \mathcal{L}_{t_0}\right](x,x_0)
        \end{align*}
        Now note that $\mathcal{L}_{t_0}$ is before all the others $\mathcal{L}_\tau$, so it is already ordered - meaning that we can bring it out the $\mathcal{T}$ operator:
        \begin{align*}
            &= -\left( \mathcal{T}\left[\exp\left(\int_{t_0}^t \mathcal{L}_\tau \dd{\tau}\right)\right] \mathcal{L}_{t_0} \right)(x,x_0)
        \end{align*}
        Then we write explicitly the matrix product between the $\mathcal{T}$ block and the $\mathcal{L}_{t_0}$:
        \begin{align*}
            &=-\int_{\mathbb{R}} \dd{y} \underbrace{ \left(\mathcal{T}\left[\exp\left(\int_{t_0}^t \mathcal{L}_\tau \dd{\tau}\right)\right] \right)(x,y)}_{W(x,t|y,t_0)} \mathcal{L}_{t_0}(y,x_0) =\\
            &= -\int_{\mathbb{R}}\dd{y} W_B(x,t|y,t_0) \mathcal{L}_{t_0} (y,x_0)
        \end{align*}
        Finally, use (\ref{eqn:mtx-el}) to evaluate $\mathcal{L}_{t_0}(y,x_0)$:
        \begin{align*}
           \partial_{t_0} W_B(x,t|x_0,t_0) &= -\int_{\mathbb{R}} \dd{y} W_B(x,t|y,t_0) (D \partial_y^2 - V(y,t_0)) \delta(x_0-y) =\\
           &= -(D\partial_{x_0}^2 - V(x_0,t_0))W_B(x,t|x_0,t_0)
        \end{align*}
        which is the \textit{backward} Fokker-Planck equation. 

        \item Recall that:
        \begin{align*}
            W_B(x,t|x_0,t_0) = \langle \delta(x-x(t)) \exp\left(-\int_{t_0}^t \dd{\tau} V(x(\tau), \tau)\right) \rangle_W
        \end{align*}
        Let's introduce a \textbf{uniform}  discretization $\{t_j\}_{j=1,\dots,n+1}$ with fixed $t_0$ and $t_{n+1} \equiv t$. Then we can write $W_B(x,t|x_0,t_0)$ as the continuum limit of the discretized integral:
        \begin{align*}
            \psi_{0} = W_{B}^{(\epsilon)}(x,t_{n+1}|x_0,t_0) &= \int_{\mathbb{R}^{n+1}} \left(\prod_{i=1}^{n+1} \frac{\dd{x_i}}{\sqrt{4 \pi D \epsilon}} \right) \cdot\\
            &\quad \>\cdot  \exp\left(-\sum_{i=1}^{n+1} \frac{(x_i - x_{i-1})^2}{4D \epsilon}  - \epsilon \sum_{i=1}^{n+1} V_i\right) \delta(x_{n+1}-x)
        \end{align*}
        We add a \textit{previous} timestep $t_{-1}$ to the discretization, and consider the paths that started in $x_{-1}$ at $t_{-1}$:
        \begin{align*}
            \psi_{-1} &= W_B^{(\epsilon)}(x,t_{n+1}|x_{-1},t_{-1}) =  \int_{\mathbb{R}^{n+2}} \left(\prod_{i=\textcolor{Red}{0}}^{n+1} \frac{\dd{x_i}}{\sqrt{4 \pi D \epsilon}} \right) \cdot\\
            &\quad \>\cdot  \exp\left(-\sum_{i=\textcolor{Red}{0}}^{n+1} \frac{(x_i - x_{i-1})^2}{4D \epsilon}  - \epsilon \sum_{i=\textcolor{Red}{0}}^{n+1} V_i\right) \delta(x_{n+1}-x)
        \end{align*}
        We interpret $\psi_0$ as the \textit{evolution one timestep later} of $\psi_{-1}$, in the sense that the starting point \q{moves by one step forward}. This is analogy to what we did for the forward Bloch equation, when we considered $\psi_{n+1}$ as the evolution of $\psi_n$, in the sense that in the former the \textit{arrival point} was \textit{one timestep forward} with respect to the latter. So, while considering the \textit{arrival point} leads to the forward equation, considering the \textit{starting point} - as we are doing now - leads to the \textit{backward} one.
        
        \medskip

        The idea is now to highlight a $\psi_0$ inside of $\psi_{-1}$. As $\psi_0$ starts from $x_0$, we highlight the term with $x_0$:
        \begin{align*}
            \psi_{-1} &= \int_{\mathbb{R}^{n+2}} \left(\prod_{i=0}^{n+1}\frac{\dd{x_i}}{\sqrt{4 \pi D \epsilon}} \right) \exp\left(-\frac{(x_0-x_{-1})^2}{4 D \epsilon} - \epsilon V_0\right) \cdot\\
            &\quad \> \cdot  \exp\left(-\sum_{i=1}^{n+1} \frac{(x_i - x_{i-1})^2}{4D \epsilon}  - \epsilon \sum_{i=1}^{n+1} V_i\right) \delta(x_{n+1}-x)
        \end{align*}
        We wish to \textit{free} that term from the path integral. To do this, we \textit{rename} $x_0$ to $x'$ with a $\delta$, so that:
        \begin{align*}
            \psi_{-1} &= \frac{1}{\sqrt{4 \pi D \epsilon}} \textcolor{Red}{\int_{\mathbb{R}}\dd{x'}} \exp\left(-\frac{(\textcolor{Red}{x'}-x_{-1})^2}{4 D \epsilon}  - \epsilon V(x',t_0)\right)   \hlc{Yellow}{\int_{\mathbb{R}^{n+1}}\left(\prod_{i=1}^{n+1} \frac{\dd{x_i}}{\sqrt{4 \pi D \epsilon}} \right)\cdot}\\
            &\quad\> \hlc{Yellow}{\cdot\exp\left(-\sum_{i=1}^{n+1} \frac{(x_i - x_{i-1})^2}{4D \epsilon}  - \epsilon \sum_{i=1}^{n+1} V_i\right) \delta(x_{n+1}-x) \textcolor{Red}{\delta(x'-x_0)}} =\\
            &= \frac{1}{\sqrt{4 \pi D \epsilon}} \int_{\mathbb{R}} \dd{x'} \exp\left(-\underbrace{\frac{(x'-x_{-1})^2}{4 D \epsilon}}_{z^{2}/2} - \epsilon V(x',t_0)\right) \hlc{Yellow}{W_B^{(\epsilon)}(x,t_{n+1}|x',t_0)}
        \end{align*}  

        To simplify the gaussian we change variables:
        \begin{align*}
           \frac{z^2}{2}  = \frac{(x'-x_{-1})^2}{4 D \epsilon}  \Rightarrow z = \frac{x'-x_{-1}}{\sqrt{2D \epsilon}} \Rightarrow \dd{x'} = \sqrt{2 D \epsilon} \dd{z}; \> x'=x_{-1}+z\sqrt{2 D \epsilon}
        \end{align*}
        leading to:
        \begin{align*}
            \psi_{-1} &= \frac{\sqrt{2 D \epsilon}}{\sqrt{4 \pi D \epsilon}} \int_{\mathbb{R}} \dd{x'} \exp\left[-z^2 - \epsilon V(x_{-1}+z\sqrt{2D \epsilon})\right]\cdot\\
            &\quad\> \cdot W_B^{(\epsilon)}(x, t_{n+1}|x_{-1}+z\sqrt{2 D \epsilon},t_0)
        \end{align*}
        Then we perform a Taylor expansion about $z\sqrt{2 D \epsilon} \sim 0$ of both the potential and the solution:
        \begin{align*}
            \exp(-\epsilon V(x_{-1}+z\sqrt{2 D \epsilon})) \approx \exp(-\epsilon[ V x_{-1} + O(\epsilon^{1/2})]) \approx 1 - \epsilon V(x_{-1}) + O(\epsilon^{3/2})
        \end{align*}
        Let $W_B^{(\epsilon)}(x,t_{n+1}|x_{-1},t_0)$ be $\psi$:
        \begin{align*}
            W_B^{(\epsilon)}(x,t_{n+1}|x_{-1}+z\sqrt{2 D \epsilon},t_0) = \psi + \psi' z \sqrt{2 D \epsilon} + \psi'' z^2 2 D \epsilon
        \end{align*}
        Substituting back in the integral, and ignoring higher order terms:
        \begin{align*}
            \psi_{-1} &= \frac{1}{\sqrt{2\pi}} \int_{\mathbb{R}} \dd{x'} \exp\left(-\frac{z^2}{2} \right)[1- \epsilon V(x_{-1})](\psi + \psi' z \sqrt{2 D \epsilon} + \psi'' z^2 2 D \epsilon) =\\
            &=\frac{1}{\sqrt{2 \pi}} \int_{\mathbb{R}}\dd{x'} \exp\left(-\frac{z^2}{2} \right) \left[\psi(1-\epsilon V(x_{-1})) +z\psi'  \sqrt{2 D \epsilon}  +z^2\psi''2D \epsilon )\right]
        \end{align*}
        Note that:
        \begin{align*}
            f(z) = \frac{1}{\sqrt{2 \pi}} \exp\left(-\frac{z^2}{2} \right) 
        \end{align*}
        is a normalized gaussian with $0$ mean and unit variance. So the first moment is null, and the second is $1$, leading to:
        \begin{align*}
            \psi_{-1} = W_B(x,t|x_{-1},t_{-1}) = \span \\
            =(1-\epsilon V(x_{-1})) W_B(x,t|x_{-1}, t_0) +2 D \epsilon \partial_{x_{-1}}^2 W_B(x,t|x_{-1}, t_0) 
        \end{align*} 
        Rearranging:
        \begin{align*}
            W_B(x,t|x_{-1}, t_0) -W_B(x,t|x_{-1},t_{-1}) =\span \\
            = \epsilon V(x_{-1})W_B(x,t|x_{-1}, t_0)  - 2D\epsilon \partial_{x_{-1}}^2 W_B(x,t|x_{-1}, t_0) 
        \end{align*}
        Dividing by $\epsilon$ and taking the continuum limit $\epsilon \to 0^+$:
        \begin{align*}
            \lim_{\epsilon \to 0^+} \frac{W_B(x,t|x_{-1}, t_0) -W_B(x,t|x_{-1},t_{-1})}{\epsilon}  = \partial_{t_0} W(x,t|x_{-1},t_0) = \span \\
            &= V(x_{-1},t_0) W_B(x,t|x_{-1},t_0)) - 2 D \partial_{x_{-1}}^2 W(x,t|x_{-1},t_0)
        \end{align*}
        And renaming $x_{-1} \to x_0$ leads to the desired result.
        
    \end{enumerate}
    
\end{exo}

\begin{exo}
    Prove that:
    \begin{align*}
        W_B(\bm{x},t|\bm{x_0},t_0) = \langle \exp\left(-\int_{t_0}^t V(\bm{x}(s),s) \dd{s}\right) \delta^k(\bm{x}(t)-\bm{x})\rangle
    \end{align*}
    satisfies the backward Bloch equation:
    \begin{align} \label{eqn:backfp} 
        \partial_{t_0} W_B(\bm{x},t|\bm{x_0},t_0) =\span\\ \nonumber
        &\quad \> -\left[\sum_{\omega=1}^k \left(f(\bm{x_0},t_0) \pdv{x_{0,\omega}} + \sum_{\nu=1}^k D^{\omega \nu}(\bm{x_0},t_0)\pdv[2]{x_{0,\nu}}\right) - V(\bm{x_0},t_0)\right] W_B(\bm{x},t|\bm{x_0},t_0)
    \end{align}
    for the simplest case $k=d=1$ and $D(x,t) > 0$ generic.
    
    \medskip

    \textit{Hint}: use the discrete measure for $d=1$:
    \begin{align} \label{eqn:discr}
        \dd{\mathbb{P}}_{t_1,\dots,t_n}(\bm{x_1},\dots,\bm{x_n}|\bm{x_0},t_0) = \span\\ \nonumber
        &\quad \> \prod_{i=1}^n \prod_{\alpha = 1}^d \frac{\dd{x_i^\alpha}}{\sqrt{4 \pi D^\alpha_{i-1} \Delta t_i}} \exp\left(-\sum_{i=1}^n \sum_{\alpha=1}^d \frac{(\Delta x_i^\alpha - f^\alpha_{i-1}\Delta t_i)^2}{4 D^\alpha_{i-1} \Delta t_i} \right) 
    \end{align}
    Notice that it is easier to prove the backward Bloch (\ref{eqn:backfp}) rather than the \textit{forward one}  since a change of variable involved in the derivation does not need complicated derivations).

    \begin{expl}
        Equation (\ref{eqn:backfp}) is different from the one referenced in Maritan's notes, as the derivatives should be wrt the \textit{starting point} and not the \textit{arrival}.  
    \end{expl}
    \medskip



    \textbf{Solution}. The procedure is really similar to that used in ex. 6.2 part 2, but we now consider the dependence of $D$ on $x$ and $t$, and an added force $f(x,t)$. First we rewrite everything in the $d=1$ case. We start from:
    \begin{align} \label{eqn:Wb1}
        W_B(x,t|x_0,t_0) = \langle \exp\left(-\int_{t_0}^t V(x(s),s) \dd{s}\right) \delta(x(t)-x)\rangle
    \end{align}
    and we want to prove that:
    \begin{align*}
        \partial_{t_0} W_B(x,t|x_0,t_0) &= -\left[\left(f(x_0,t_0) \pdv{x_0} + D(x_0,t_0) \pdv[2]{x_0}\right) - V(x_0,t_0) \right]W_B(x,t|x_0,t_0)
    \end{align*}
    Introduce a uniform time discretization $\{t_j\}_{j=0,\dots,n}$, with fixed end-points and $\Delta t_i = t_{i} - t_{i-1} \equiv \epsilon$. Then, following (\ref{eqn:discr}) and adding the term $-\epsilon V_i$ for the exponential of the integral from (\ref{eqn:Wb1}), we get:
    \begin{align*}
        W_B(x,t|x_0 ,t_0) &= \lim_{\epsilon \to 0^+} W_B^{(\epsilon)}(x,t|x_0,t_0) \equiv \psi_0\\
        \psi_0 &= \int_{\mathbb{R}^n} \left(\prod_{i=1}^n \frac{\dd{x_i}}{\sqrt{4\pi D_{i-1} \epsilon}} \right) \cdot\\
        &\quad\> \cdot \exp\left(-\sum_{i=1}^n\frac{(x_i - x_{i-1} - f_{i-1} \epsilon)^2}{4 D_{i-1} \epsilon}  - \sum_{i=1}^n\epsilon V_i\right) \delta(x_n - x)
    \end{align*}
    \begin{expl}
        Note that we have $f_{i-1}$ and $D_{i-1}$, but $V_i$. This is because the first two come from a change of random variables from the Ito SDE, for which Ito's prescription applies. On the other hand, $V$ comes from the functional that we are averaging.
    \end{expl}
    As in the previous exercise, we consider the solution with the starting point \textit{a timestep in the past}, that is:
    \begin{align*} 
        \psi_{-1} &= \int_{\mathbb{R}^{n+1}} \left(\prod_{i=\textcolor{Red}{0}}^n \frac{\dd{x_i}}{\sqrt{4\pi D_{i-1} \epsilon}} \right) \cdot\\
        &\quad\> \cdot \exp\left(-\sum_{i=\textcolor{Red}{0}}^n\frac{(x_i - x_{i-1} - f_{i-1} \epsilon)^2}{4 D_{i-1} \epsilon}  - \sum_{i=\textcolor{Red}{0}}^n\epsilon V_i\right) \delta(x_n - x)
    \end{align*} 
    Then we highlight the first term (the one in $x_0$):
    \begin{align*}
        \psi_{-1} &= \int_{\mathbb{R}^{n+1}} \left(\prod_{i=0}^n \frac{\dd{x_i}}{\sqrt{4 \pi D_{i-1} \epsilon}} \right) \cdot\\
        &\quad \> \cdot \exp\left(-\frac{(x_0 - x_{-1} - f_{-1}\epsilon)^2}{4 D_{-1} \epsilon} - \epsilon V_0 \right) \cdot\\
        &\quad \> \cdot \exp\left(-\sum_{i=1}^n\frac{(x_i - x_{i-1} - f_{i-1} \epsilon)^2}{4 D_{i-1} \epsilon}  - \sum_{i=1}^n\epsilon V_i\right) \delta(x_n - x)
    \end{align*}
    Note that now the last term looks like $\psi_0$, which is what we want. We just need to bring the first term \textit{outside} the path integral - and we do this by renaming $x_0$ to $x'$ with another $\delta$:
    \begin{align*}
        \psi_{-1} &= \int_{\mathbb{R}} \frac{\dd{x'}}{\sqrt{4 \pi D_{-1} \epsilon}} \exp\left(-\frac{(x'- x_{-1} - f_{-1}\epsilon)^2}{4 D_{-1} \epsilon} - \epsilon V(x',t_0) \right) \cdot\\
        &\quad \> \hlc{Yellow}{\cdot \int_{\mathbb{R}^n} \left(\prod_{i=1}^n \frac{\dd{x_i}}{\sqrt{4 \pi D_{i-1}\epsilon}} \right) \exp\left(-\sum_{i=1}^n\frac{(x_i - x_{i-1} - f_{i-1} \epsilon)^2}{4 D_{i-1} \epsilon}  - \sum_{i=1}^n\epsilon V_i\right)\cdot}\\
        &\quad \> \hlc{Yellow}{\cdot \delta(x_n - x) \delta(x_0 - x') }=\\
        &= \int_{\mathbb{R}} \frac{\dd{x'}}{\sqrt{4 \pi D_{-1} \epsilon}} \exp\Bigg(-\underbrace{\frac{(x'- x_{-1} - f_{-1}\epsilon)^2}{4 D_{-1} \epsilon}}_{z^2/2} - \epsilon V(x',t_0) \Bigg) \hlc{Yellow}{W_B^{(\epsilon)}(x,t|x',t_0)}
    \end{align*} 
    We then perform a change of variables:
    \begin{align*}
        z = \frac{x'-x_{-1}-f_{-1}\epsilon}{\sqrt{2 D_{-1}\epsilon}}  \Rightarrow x' = x_{-1} + f_{-1} \epsilon + z \sqrt{4 D_{-1}\epsilon}
    \end{align*}
    leading to:
    \begin{align*}
        W_B^{(\epsilon)}(x,t|x_{-1},t_{-1}) &= \frac{1}{\sqrt{2 \pi}} \int_{\mathbb{R}} \dd{z} \exp\left(-\frac{z^2}{2} \right)\exp\left(-\epsilon V(x_{-1}+f_{-1}\epsilon + z\sqrt{4 D_{-1}\epsilon})\right) \cdot\\
        &\quad \> \cdot W_B(x,t|x_{-1}+f_{-1}\epsilon + z \sqrt{4 D_{-1}\epsilon},t_0)
    \end{align*}
    Finally, we perform some Taylor expansions for $f_{-1}\epsilon + z\sqrt{4 D_{-1}\epsilon} \sim 0$:
    \begin{align*}
        \exp\left(-\epsilon V(x_{-1}+f_{-1}\epsilon + z\sqrt{4 D_{-1}\epsilon})\right) &= \exp\left(-\epsilon V(x_{-1}) + O(\epsilon\sqrt{\epsilon})\right) =\\
        &=1-\epsilon V(x_{-1}) + O(\epsilon^2)
    \end{align*} 
    Let $W_B^{(\epsilon)}(x,t|x_{-1},t_0) = \psi$, and denote with $\psi'$ the first derivative wrt $x_{-1}$ (and so on). Then:
    \begin{align*}
        W_B^{(\epsilon)}(x,t|x_{-1}+f_{-1}\epsilon + z \sqrt{4 D_{-1}\epsilon},t_0) &= \psi + (f_{-1} \epsilon + z\sqrt{4 D_{-1}\epsilon}) \psi' +\\
        &\quad \> + \frac{1}{2}(f_{-1} \epsilon + z \sqrt{4 D_{-1}\epsilon})^2 \psi'' 
    \end{align*}
    Substituting back in the integrand, and neglecting everything of order $>1$ in $\epsilon$:
    \begin{align*}
        W_B^{(\epsilon)}(x,t|x_{-1},t_{-1}) &= \frac{1}{\sqrt{2 \pi}} \int_{\mathbb{R}} \dd{z} \exp\left(-\frac{z^2}{2} \right)\Bigg[\psi(1-\epsilon V(x_{-1})) + f_{-1}\epsilon \psi' +\\
        &\quad \>+  z \psi' \sqrt{4 D_{-1} \epsilon} + \frac{1}{2} z^2 4 D_{-1} \epsilon \psi'' \Bigg]
    \end{align*}
    These are all gaussian integrals involving the moments of a standard gaussian (with $0$ mean and $1$ standard deviation), and so:
    \begin{align*}
        W_B^{(\epsilon)}(x,t|x_{-1},t_{-1}) &= W_B^{(\epsilon)}(x,t|x_{-1},t_0) (1 - \epsilon V(x_{-1},t_0)) + f_{-1} \epsilon \partial_{x_{-1}} W_B^{(\epsilon)}(x,t|x_{-1},t_0)+\\
        &\quad \> + 2D_{-1} \epsilon \partial_{x_{-1}}^2 W_B^{(\epsilon)}(x,t|x_{-1},t_0)
    \end{align*}
    Rearranging, dividing by $\epsilon$ and taking the continuum limit $\epsilon \to 0^+$ finally leads to:
    \begin{align*}
        \partial_{t_0} W_B(x,t|x_0,t_0) = -[f(x_0) \partial_{x_0} + 2D(x_0)\partial_{x_0}^2 - V(x_0)]W(x,t|x_0,t_0)
    \end{align*}
\end{exo}

\setcounter{exo}{4} %Optional!

\begin{exo} Derive the analogous of the Bloch equation and of the backward Bloch equation for the Master Equation of exercise 5.7.

    \textit{Hint}: The trajectory $i(t)$ stays constant and suddenly jumps 
    at random times. Thus $\int_{t_0}^t V_{i(s)} \dd{s}$ is well defined. When evaluating the average:
    \begin{align} \label{eqn:wb2}
        W_B(i,t|i_0,t_0) = \langle \exp\left(-\int_{t_0}^t V_{i(s)} \dd{s} \right) \delta_{i(t),i}\rangle
    \end{align}
    where $W_B(i,t_0|i_0,t_0) = \delta_{i,i_0}$ and $\delta_{i,i_0}$ is the Kronecker delta, at time $t+\dd{t}$ one has to consider two contributions: one from no change of state and the other from the change of state.

    \medskip

    \textbf{Solution}. Recall that in ex. 5.7 we considered a system evolving through states $i \in J$, according to the following rule:
    \begin{align*}
        \dot{P}_i(t) = (H(t)P(t))_i \qquad H_{ij}(t) = W_{ij}(t) - \delta_{ij} \sum_{k \in J} W_{ki}(t)
    \end{align*}
    Let's consider a \textit{uniform} time discretization $\{t_j\}_{j=0,\dots,n}$ with $t_n \equiv t$ and $\Delta t_j = t_{j}-t_{j-1} \equiv \epsilon$. Introduce a potential $V \colon J \to \mathbb{R}$ and denote with $V_i$ the potential at the state $i$. We then consider a \textit{path} through states as a vector $\{i_j\}_{j=0,\dots,n}$, where $i_j \in J$ is the state explored at time $t_j$.


    Then we discretize (\ref{eqn:wb2}):
    \begin{align} \nonumber
        W_B(i,t|i_0,t_0) &= \lim_{\epsilon \to 0^+} W_B^{(\epsilon)}(i,t_n|i_0,t_0)\\ \nonumber
        W_B^{(\epsilon)}(i,t_n|i_0,t_0) &= \langle \exp\left(-\sum_{s=1}^n V_{i_s} \epsilon\right) \delta_{i_n,i}\rangle =\\
        &= \underbrace{\sum_{i_1 \in J} \cdots \sum_{i_n \in J}}_{\parbox{5em}{\centering \footnotesize Sum over all paths}} \> \underbrace{\epsilon W_{i_n,i_{n-1}} \cdots \epsilon W_{i_1,i_0}}_{\parbox{7em}{\footnotesize \centering Probability for a path}} \exp\left(-\sum_{s=1}^n V_{i_s} \epsilon\right) \underbrace{\delta_{i_n,i}}_{\parbox{3.5em}{\footnotesize\centering Fix endpoint}} \label{eqn:wb-discrete}
    \end{align}
    The average is over all \textit{discrete} paths connecting $i_0$ at $t_0$ to $i$ at $t$ (it can't be written as an integral in the Wiener measure, as the states $J$ are discrete too). 

    \medskip

    For the \textbf{forward} Bloch equation we \textit{evolve} the destination by one time-step:
    \begin{align*}
        W_B^{(\epsilon)}(i,t_{n+1}|i_0,t_0) = \psi_{n+1} = \sum_{i_1 \in J} \cdots \sum_{i_{\textcolor{Red}{n+1}} \in J} \epsilon W_{i_{\textcolor{Red}{n+1}},i_n} \cdots   \epsilon W_{i_1,i_0}  \exp\left(-\sum_{s=1}^{n+1} V_{i_s}\epsilon\right) \delta_{i_{\textcolor{Red}{n+1}},i} 
    \end{align*}  
    The sum over $i_{n+1}$ can be computed to remove the $\delta$:
    \begin{align*}
        \psi_{n+1} =\exp\left(-V_{i} \epsilon\right) \sum_{i_1 \in J} \cdots \sum_{i_n \in J} \epsilon W_{i,i_n} \cdots \epsilon W_{i_1,i_0}  \exp\left(-\sum_{s=1}^{n} V_{i_s} \epsilon\right) 
    \end{align*}

    Then we highlight the $i_n$ term:
    \begin{align*}
        \psi_{n+1} =\exp\left(-V_{i} \epsilon\right) \sum_{i_1 \in J} \cdots \sum_{i_{{n}} \in J} \hlc{Yellow}{\epsilon W_{i,i_n} }\cdots \epsilon W_{i_1,i_0}   \exp\left(-\sum_{s=1}^{n} V_{i_s} \epsilon\right) 
    \end{align*}
    To bring it out of the \textit{sum over paths} we insert a $\delta$:
    \begin{align*}
        \psi_{n+1} &= \exp(-V_{i} \epsilon) \sum_{i' \in J}   \hlc{Yellow}{\epsilon W_{i,i'}} \cdot \\
        &\quad\>\cdot \underbrace{\sum_{i_1 \in J} \cdots \sum_{i_n \in J} \epsilon W_{i_n,i_{n-1}} \cdots \epsilon W_{i_1,i_0}   \exp\left(-\sum_{s=1}^{n} V_{i_s} \epsilon\right) \delta_{i_n,i'}}_{(\ref{eqn:wb-discrete})\colon W_B(i',t|i_0,t_0)} =\\
        &=\exp(-V_i \epsilon)  \sum_{i' \in J} \epsilon W_{i,i'} W_B(i',t|i_0,t_0)
    \end{align*} 
    We now split the transition probability $W_{i',i}$ over the case $i' = i$ and $i' \neq i$, using the same trick of ex. 5.7 to express everything with the off-diagonal terms:
    \begin{align*}
        \psi_{n+1} = \exp(-V_i \epsilon) \Big(\sum_{i' \in J\setminus \{i\}} \epsilon W_{i,i'} W_B(i',t|i_0,t_0) + \Big[1-\sum_{i' \in J \setminus \{i\}}\epsilon W_{i',i}\Big] W_B(i,t|i_0,t_0)\Big)
    \end{align*}
    Note that we can extend the sums over the entire $i' \in J$, as the $i=i'$ terms cancel out.

    We then expand the exponential:
    \begin{align*}
        \exp(-V_i \epsilon) = 1 - V_1 \epsilon + O(\epsilon^2)
    \end{align*}
    Substituting back and neglecting higher order terms in $\epsilon$:
    \begin{align*}
        W_B^{(\epsilon)}(i,t_{n+1}|i_0,t_0) &= (1-V_i \epsilon)\Bigg[W_B(i,t|i_0,t_0) +\\
        &\quad \> + \epsilon \sum_{i' \in J}\Big(W_{i,i'} W_B^{(\epsilon)}(i',t|i_0,t_0) - W_{i',i} W_B^{(\epsilon)}(i,t|i_0,t_0)\Big)\Bigg]
    \end{align*}
    Rearranging, dividing by $\epsilon$ and taking the continuum limit leads to:
    \begin{align*}
        \partial_t W_B(i,t|i_0,t_0) &= \lim_{\epsilon \to 0} \frac{W_B^{(\epsilon)}(i,t_{n+1}|i_0,t_0) - W_B^{(\epsilon)}(i,t|i_0,t_0)}{\epsilon} =\\ 
        &= - V_i W_B(i,t|i_0,t_0) + \sum_{i' \in J}\Big(W_{i,i'} W_B(i',t|i_0,t_0) - W_{i',i} W_B(i,t|i_0,t_0)\Big)
    \end{align*}

    \medskip

    For the \textbf{backward} Bloch equation we start from:
    \begin{align*}
        \pdv{t_0} W_B(i,t|i_0,t_0) = \lim_{\epsilon \to 0^+} \frac{W_B(i,t|i_0,t_0) - W_B(i,t|i_0,t_{-1})}{\epsilon} 
    \end{align*}
    Applying the ESCK relation:
    \begin{align*}
        W_B(i,t|i_0,t_{-1}) = \sum_{i' \in J} W_B(i,t|i',t_0) W_B(i',t_0|i_0,t_{-1})
    \end{align*}
    The last term is the average over only one step:
    \begin{align*}
        W_B(i',t_0|i_0,t_{-1}) = \exp(-V_{i'}\epsilon) W_{i',i_0} \epsilon
    \end{align*}
    And so:
    \begin{align*}
        W_B(i,t|i_0,t_{-1}) &= \sum_{i' \in J} W_B(i,t|i',t_0)\exp(-V_{i'}\epsilon) W_{i',i_0} \epsilon =\\
        &=\sum_{i' \neq i_0} W_B(i,t|i',t_0) W_{i',i_0} \exp(-V_{i'} \epsilon) +\\
        &\quad \> +W_B(i,t|i_0,t_0) \exp(-V_{i_0} \epsilon) \left[1 - \sum_{k \neq i_0} W_{k,i_0} \epsilon\right]
    \end{align*}
    Then we compute the difference:
    \begin{align*}
        W_B(i,t|i_0,t_0) - W_B(i,t|i_0,t_{-1}) &= -\sum_{i' \neq i_0} W_B(i,t|i',t_0) W_{i',i_0} \exp(-V_{i'} \epsilon) +\\
        &\quad \> +W_B(i,t|i_0,t_0) \exp(-V_{i_0} \epsilon)  \sum_{k \neq i_0} W_{k,i_0} \epsilon
    \end{align*}
    We then add a $\delta$ to merge the two sums, and extend them to include the diagonal terms (without adding anything, because the two terms cancel out in that case):
    \begin{align*}
        &= -\sum_{i'} W_B(i,t|i',t_0) W_{i',i_0} \epsilon \exp(-V_{i'} \epsilon) + \sum_{i} \delta_{i',i_0} W_B(i,t|i',t_0) \exp(-V_{i'}\epsilon) \sum_{k \neq i_0} W_{k i_0} \epsilon =\\
        &= -\sum_{i'} W_B(i,t|i',t_0) \exp(-V_{i'}\epsilon) \epsilon[W_{i' i_0} - \delta_{i',i_0} \sum_{k \neq i_0} W_{k,i_0}] 
    \end{align*}
    Dividing by $\epsilon$ and taking the continuum limit leads to:
    \begin{align*}
        \partial_{t_0} W_B(i,t|i_0,t_0) = 
    \end{align*}

    \begin{align*}
        W_B^{(\epsilon)}(i,t_n|i_0,t_{-1}) &= \psi_{-1} = \sum_{i_0' \in J} \sum_{i_1 \in J} \cdots \sum_{i_n \in J} \epsilon W_{i_n, i_{n-1}} \cdots \epsilon W_{i_1,i_0'}\epsilon W_{i_0', i_0} \cdot \\
        &\quad \>\cdot \exp(-V_{i_0'}\epsilon)\exp\left(-\sum_{s=1}^n V_{i_s} \epsilon\right) \delta_{i_n,i} = \\
        &= \sum_{i' \in J} \exp(-V_{i'}\epsilon) \epsilon W_{i',i_0} W_B^{(\epsilon)}(i,t|i_0',t_0) =\\
        &= \sum_{i' \in J\setminus \{i_0\}} \exp(-V_{i'}\epsilon) \epsilon W_{i',i_0} W_B^{(\epsilon)}(i,t|i_0',t_0) + \\
        &\quad \> + \exp(-V_{i_0}\epsilon)\Big(1-\sum_{i' \in J\setminus \{i\}} \epsilon W_{i',i_0}\Big) W_B^{(\epsilon)}(i,t|i_0,t_0)
    \end{align*}   
    \begin{align*}
        W_B^{(\epsilon)}(i,t_n|i_0,t_0) - W_B^{(\epsilon)}(i,t_n|i_0,t_{-1}) = 
    \end{align*}
    Highlight the term with $i_0'$:
    \begin{align*}
        \psi_{-1} = \sum_{i_0 \in J} \cdots \sum_{i_n \in J} \exp(-V_{i_0} \epsilon) \epsilon W_{i_0,i_{-1}} \left[\epsilon W_{i_n, i_{n-1}} \cdots \epsilon W_{i_1,i_0}\right] \exp\left(-\sum_{s=1}^n V_{i_s} \epsilon\right) \delta_{i_n,i}
    \end{align*}
    And we extract it out the paths sum by inserting a $\delta$:
    \begin{align*}
        \psi_{-1} &= \sum_{i' \in J} \epsilon W_{i',i_{-1}} \cdot \\
        &\quad \> \cdot \sum_{i_0 \in J} \cdots \sum_{i_n \in J} \epsilon W_{i_n, i_{n-1}} \cdots \epsilon W_{i_1,i_0} \exp\left(-\sum_{s=1}^n V_{i_s} \epsilon\right) \exp(-V_{i'} \epsilon) \delta_{i_n,i} \delta_{i_0,i'}
    \end{align*}
    Now the sum over $i_0$ can be computed, eliminating a $\delta$:
    \begin{align*}
        \psi_{-1} &= \sum_{i' \in J} \epsilon W_{i',i_{-1}} \cdot \\
        &\quad \> \cdot \exp(-V_{i_0}\epsilon)\sum_{i_1 \in J}\cdots \sum_{i_n \in J} \epsilon W_{i_n, i_{n-1}}\cdots \epsilon W_{i_1, i'} \exp\left(-\sum_{s=1}^n V_{i_s} \epsilon\right)
    \end{align*}

\end{exo}

\begin{exo}[Change of variables in $\lambda$ prescription]
Generalize the results for the change of variable formula for Ito integrals to the case of a generic $\lambda$-prescription and re-derive the result of problem 4.9.\\

\textbf{Solution}. The main idea is to start from a Stochastic Differential Equation in $\lambda$-prescription, convert it to an equivalent formulation using the Ito prescription, apply Ito's formula for changing variables, and then go back to the former prescription.

First, two SDEs have the same solution $x(t)$ if, for any realization $B(t)$ of the Brownian noise, their solutions (which can now be found by \textit{normal} integration) coincide.

So, consider the usual SDE in Ito's prescription:
\begin{align*}
    \dd{x(t)} = a(x(t),t) \dd{t} + b(x(t),t) \dd{B(t)}
\end{align*}
which has solution:
\begin{align*}
    x(t) = x(t_0) + \int_{t_0}^t a(x(\tau),\tau) \dd{\tau} + \int_{t_0}^t b(x(\tau), \tau) \dd{B(\tau)}
\end{align*}
where the stochastic integral is formally defined as:
\begin{align*}
    \int_{t_0}^t b(x(\tau),\tau) \dd{B(\tau)} \equiv \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^n b(x_{i-1}, t_i) \Delta B_i \qquad \Delta B_i = B_i - B_{i-1}
\end{align*}
We now consider a  SDE in the $\lambda$ prescription:
\begin{align*}
    \dd{y(t)} = \alpha(y(t),t) \dd{t} + \beta(y(t),t) \dd{B(t)} \big|_{\lambda}
\end{align*}
which has solution:
\begin{align*}
    y(t) = \int_{t_0}^t \alpha(y(\tau), \tau) \dd{\tau} + \int_{t_0}^t \beta(y(\tau), \tau) \dd{\tau} \Big|_{\lambda}
\end{align*}
Now, however, the stochastic integral has a different definition:
\begin{align*}
    \int_{t_0}^t \beta(y(\tau), \tau) \dd{B(\tau)} \Big|_{\lambda} \equiv \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^n \beta\Big( (1-\lambda) x_{i-1} + \lambda x_i, t_{i-1}\Big) \Delta B_i
\end{align*}
We now impose that $y(t) = x(t)$ for every $t$, and search the mapping $a,b \mapsto \alpha, \beta$ that establishes a correspondence between an Ito SDE and a generic $\lambda$ prescription SDE.\\

Let's focus on the argument of the $\lambda$-integral, and expand it about the left extremum of the discretization:
\begin{align} \nonumber
    \beta(y_{i-1} - \lambda y_{i-1} + \lambda y_i, t_{i-1}) &= \beta(y_{i-1} + \lambda(y_i - y_{i-1}), t_{i-1}) =\\
    &= \beta(y_{i-1}, t_{i-1}) + \partial_x \beta(y_{i-1}, t_{i-1}) \lambda (y_i - y_{i-1}) \label{eqn:beta1}
\end{align}
As the paths are the same, $y_i = x_i$, and the increments $\Delta y_i$ follow the rule:
\begin{align*}
    \Delta y_i = a(x_{i-1}, t_{i-1}) \Delta t_i + b(x_{i-1}, y_{i-1}) \Delta B_i
\end{align*}
Leading to:
\begin{align*}
    (\ref{eqn:beta1}) = \beta_{i-1} + \beta_{i-1}' \lambda [a_{i-1}\Delta t_i + b_{i-1} \Delta B_i]
\end{align*}
Substituting inside the integral we get:
\begin{align*}
    \int_{t_0}^t \beta(y(\tau), \tau) \dd{B(\tau)} \Big|_{\lambda} &= \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^{n} \Big[\beta_{i-1} \Delta B_i + \lambda \beta'_{i-1} \Delta B_i [a_{i-1} \Delta t_i + b_{i-1} \Delta B_{i}] \Big] =\\
    &=\overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^{n}\Big[
    \beta_{i-1} \Delta B_i + \lambda \beta_{i-1}' b_{i-1} \Delta B_i^2 + O(\Delta B_i \Delta t_i)    
    \Big]
\end{align*}
We already proved that:
\begin{align*}
    \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^n G_{i-1} \Delta B_i^2 = \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^n G_{i-1} \Delta t_i
\end{align*}
And so we can use this result, setting $G_{i-1} = \lambda \beta'_{i-1} b_{i-1}$, justifying the usual $\dd{B}^2 = \dd{t}$ rule. So, this leads to:
\begin{align} \nonumber 
    \int_{t_0}^t \beta(y(\tau), \tau) \dd{B(\tau)} \Big|_{\lambda} &= \overset{\rm m.s.}{\lim_{n \to +\infty}} \sum_{i=1}^{n} \Big[\beta_{i-1} \Delta B_i + \lambda \beta_{i-1}' b_{i-1} \Delta t_i \Big] = \\
    &=\int_{t_0}^t \beta(y(\tau), \tau) \dd{B(\tau)} + \lambda \int_{t_0}^t b(y(\tau), \tau) \pdv{x} \beta(y(\tau), \tau) \dd{\tau}
    \label{eqn:formula}
\end{align}
where the last two integrals are Ito integrals. We have now found a way to evaluate a $\lambda$-integral using an Ito integral (provided the paths are generated by a Ito SDE). We can find the explicit conversion rules by equating the solutions:
\begin{align*}
    x(t) &= x(t_0) + \int_{t_0}^t a(x(\tau), \tau) \dd{\tau} + \int_{t_0}^t b(x(\tau), \tau) \dd{B(\tau)} =\\
    &\overset{!}{=} x(t_0) + \int_{t_0}^t \alpha(x(\tau), \tau) \dd{\tau} + \int_{t_0}^t \beta(x(\tau), \tau) \dd{B(\tau)} =\\
    &\underset{(\ref{eqn:formula})}{=} x(t_0) + \int_{t_0}^t \alpha(x(\tau), \tau) \dd{\tau} + \int_{t_0}^t \beta(x(\tau), \tau) \dd{B(\tau)} + \lambda \int_{t_0}^t b(x(\tau), \tau) \partial_x \beta(x(\tau), \tau) \dd{\tau}
\end{align*} 
leading to:
\begin{align*}
    \begin{cases}
        \alpha + \lambda b \partial_x \beta = a\\
        b = \beta
    \end{cases} \Rightarrow \begin{cases}
        \alpha = a - \lambda b \partial_x \beta\\
        \beta = b
    \end{cases}
\end{align*}
where $(\alpha, \beta)$ are the coefficients in the $\lambda$ SDE, and $(a,b)$ the ones in the equivalent Ito SDE.

Consider now a $\lambda$ SDE:
\begin{align*}
    \dd{x} = \alpha \dd{t} + \beta \dd{B(t)}
\end{align*}
The equivalent Ito SDE is:
\begin{align}\label{eqn:newdx}
    \dd{x} = (\alpha + \lambda \beta \partial_x \beta) \dd{t} + \beta \dd{B(t)}
\end{align}
Squaring:
\begin{align*}
    \dd{x}^2 = \beta^2 \dd{t}
\end{align*}
Where we used $\dd{B}^2 = \dd{t}$ (as this is an Ito SDE), and ignored higher order terms. It is clear that $\dd{x}^n = 0$ with $n > 0$.

Let $y=y(x)$ be a change of variables. The new differential will be:
\begin{align*}
    \dd{y} = \dv{y}{x} \dd{x} + \frac{1}{2} \dv[2]{y}{x} \dd{x}^2 + \dots
\end{align*}
and we can ignore the higher order terms, as they will be $O(\dd{t})$. Substituting in (\ref{eqn:newdx}) we get:
\begin{align*}
    \dd{y} &= \dv{y}{x} (\alpha + \lambda \beta \partial_x \beta) \dd{t} + \dv{y}{x} \beta \dd{B} + \frac{1}{2} \dv[2]{y}{x} \beta^2 \dd{t}  =\\
    &= \left[\dv{y}{x}\left(\alpha+ \lambda \beta \partial_x \beta \right) + \frac{1}{2} \dv[2]{y}{x} \beta^2 \right]\dd{t} + \dv{y}{x} \beta \dd{B}
\end{align*}
To complete the change of variables, we need to express everything in terms of $y$ - in particular the derivatives. One trick is to use the inverse function theorem:
\begin{align*}
    \dv{y}{x} = \left(\dv{x}{y}\right)^{-1} = \frac{1}{x'(y)} 
\end{align*}
For the second derivative, note that, if $f$ and $g$ are the inverse of each other:
\begin{align*}
    g \circ f = \operatorname{id} &\Rightarrow g(f(x)) = x \underset{\rm d/dx}{\Rightarrow} g'(f(x)) f'(x) = 1\\
     &\underset{\rm d/dx}{\Rightarrow} g''(f(x)) [f'(x)]^2 + g'(f(x)) f''(x) = 0 \\&\underset{y=f(x)}{\Rightarrow}  g''(y) = - \frac{g'(y) f''(x)}{f'(x)^2} = - \frac{f''(x)}{[f'(x)]^3}  
\end{align*}
And in our case:
\begin{align*}
    \dv[2]{y}{x} = -\frac{x''(y)}{[x'(y)]^3} 
\end{align*}
One last thing:
\begin{align*}
    \pdv{x} \beta = \dv{y}{x} \pdv{y} \beta = \frac{1}{x'(y)} \partial_y \beta 
\end{align*}

This leads to:
\begin{align*}
    \dd{y} = \underbrace{\left[\frac{\alpha}{x'(y)} + \frac{\lambda \beta \partial_y \beta}{[x'(y)]^2} - \frac{1}{2} \beta^2 \frac{x''(y)}{[x'(y)]^3}    \right]}_{a}  \dd{t} + \underbrace{\frac{\beta}{x'(y)}}_{b}  \dd{B} 
\end{align*}

We can finally map this back to a $\lambda$ SDE and find the change of variable rule for that case. Applying the substitutions:
\begin{align*}
    \dd{y} = \tilde{\alpha} \dd{t} + \tilde{\beta} \dd{B} \qquad \begin{cases}
        \tilde{\alpha} &= a-\lambda b \partial_x b\\
    \tilde{\beta} &= b
    \end{cases}
\end{align*}
We arrive to:
\begin{align*}
    \tilde{\alpha} &= \frac{\alpha}{x'(y)} + \frac{\lambda \beta \partial_y \beta}{[x'(y)]^2} - \frac{1}{2} \beta^2 \frac{x''(y)}{[x'(y)]^3} - \lambda \frac{\beta}{x'(y)} \partial_y \frac{\beta}{x'(y)}  =\\
    &= \frac{\alpha}{x'(y)} + \cancel{\frac{\lambda \beta \partial_y \beta}{[x'(y)]^2}} - \frac{1}{2} \beta^2 \frac{x''(y)}{[x'(y)]^3} - \lambda \frac{\beta}{x'(y)}\left[\frac{\cancel{\partial_y \beta x'(y)} - x''(y) \beta}{[x'(y)]^2} \right] =\\
    &= \frac{\alpha}{x'(y)} +\frac{2\lambda - 1}{2} \frac{\beta^2 x''(y)}{[x'(y)]^3}\\
    \tilde{\beta} &= \frac{\beta}{x'(y)}\\
    \dd{y} &= \left[\frac{\alpha}{x'(y)} +\frac{2\lambda - 1}{2} \frac{\beta^2 x''(y)}{[x'(y)]^3}\right] \dd{t} + \frac{\beta}{x'(y)} \dd{B} 
\end{align*}
Let's bring this result to the usual notation:
\begin{align*}
    \alpha = f(x(\tau), \tau) \qquad \beta = g(x(\tau),\tau) \qquad \begin{dcases}
        y = h(x(\tau))\\
        \frac{1}{x'(y)} = \dv{h}{x} = h'(x(\tau))\\
        -\frac{x''(y)}{[x'(y)]^3} = \dv[2]{h}{x} = h''(x(\tau)) 
    \end{dcases}
\end{align*}
leading to:
\begin{align*}
    \dd{h(x(\tau))} &= \left(f(x(\tau),\tau) h'(x(\tau)) + \frac{1-2 \lambda}{2} h''(x(\tau)) g(x(\tau), \tau)^2\right) \dd{\tau} +\\
    &\quad \> + g(x(\tau), \tau) h'(x(\tau)) \dd{B(\tau)} \Big|_\lambda
\end{align*}
which is the formula for changing variables in the $\lambda$ prescription. Let's rearrange to isolate the $\dd{B}$ term:
\begin{align*}
    gh' \dd{B} = \dd{h} - \left(f h' + \frac{1-2\lambda}{2} h'' g^2\right) \dd{\tau}
\end{align*}
Then we integrate, leading to the formula:
\begin{align*}
    \int_{t_0}^t h'(x(\tau)) g(x(\tau), \tau) &= h(x(t))-h(x(t_0)) - \int_{t_0}^t h'(x(\tau))f(x(\tau), \tau) \dd{\tau}\\
    &\quad \> -\frac{1-2 \lambda}{2}\int_{t_{0}}^t  h''(x(\tau)) g(x(\tau), \tau)^2 \dd{\tau} 
\end{align*}
Finally, set $g(x(\tau), \tau) \equiv 1$, and $h'(x(\tau)) = B(\tau)$, so that:
\begin{align*}
    h = \frac{B^2}{2} \qquad h'' = 1 
\end{align*}
Substituting in the formula we can compute the desired integral:
\begin{align*}
    \int_{t_0}^t B(\tau) \dd{B(\tau)} = \frac{B^2(t)-B^2(t_0)}{2} + \frac{2 \lambda - 1}{2} (t-t_0)  
\end{align*}





\end{exo}

\end{document}
