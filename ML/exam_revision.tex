%&latex
%
\documentclass[../template.tex]{subfiles}

\begin{document}

\chapter{Exam revision}

\section{Simple Learning Framework}
The simplest learning task is to \textit{classify} features in two classes, in the case where there exists a unique and deterministic map between them. 
\begin{itemize}
    \item \textbf{Domain set} (\textit{Instance space}) $\mathcal{X}$, where elements $\bm{x} \in \mathcal{X}$ are \textit{vectors of features}.
    \item \textbf{Label set} $\mathcal{Y}$
    \item \textbf{Training data}: sequence (order matters, there can be repetitions) of elements in $\mathcal{X}\times \mathcal{Y}$: $S=((x_1 , y_1 ), \dots, (x_m, y_m))$. $m$ denotes the number of training samples.
    \item The ML algorithm $A$, given $S$, outputs a \textbf{hypothesis function} (prediction rule, or \textit{predictor}) $A(S) = h_S\colon \mathcal{X} \to \mathcal{Y}$ that represents the mapping between $\mathcal{X}$ and $\mathcal{Y}$ \textit{learned} by the algorithm.
    \item We assume a \textbf{simple data generation model} for constructing $S$. $\mathcal{X}$ is generated by sampling a distribution (pdf) $\mathcal{D}$ which is \textit{not known} by the learning model, and are then labelled by a function $f\colon \mathcal{X} \to \mathcal{Y}$ (labelling function, representing the \textit{ground truth}).  $\mathcal{D}$ allows to assign probabilities to \textit{events}, i.e. subsets $A \subset \mathcal{X}$. We introduce the following notation:
    \begin{align*}
        \mathcal{D}(A) = \mathbb{P}[x \in A] \qquad A \subset \mathcal{X}
    \end{align*} 
    Equivalently, we can denote subsets $A$ using their \textit{characteristic function} $\pi_A(x) \colon \mathcal{X} \to \{0,1\}$:
    \begin{align*}
        \pi_A(x) = \begin{cases}
            1 & x \in A\\
            0 & x \not\in A
        \end{cases} \qquad A = \{x \in A \colon \pi(x) = 1\}
    \end{align*} 
    And so $\mathcal{D}(A) = \mathbb{P}_{x \sim \mathcal{D}}[\pi_A(x)]$.
    \item We define the \textbf{generalization error} (or \textbf{risk}, \textbf{true error})  of $h_S$ as the probability of randomly choosing a sample from $\mathcal{D}$ so that $h_S(x) \neq f(x)$:
    \begin{align*}
        L_{\mathcal{D},f}(h_S) \equiv \underset{x \sim \mathcal{D}}{\mathbb{P}}[h_S(x) \neq f(x)] \equiv \mathcal{D}(\{x \colon h_S(x) \neq f(x)\})
    \end{align*} 
    Note that $L_{\mathcal{D},f}$ depends on the \textit{data generation} ($\mathcal{D}$, $f$), and so cannot be known by the learner. For simplicity, we denote $L_{\mathcal{D}} \equiv L_{\mathcal{D},f}$.
    \item The \textbf{empirical risk} (or \textbf{training error}) is defined as:
    \begin{align*}
        L_S(h) \equiv \frac{|\{1 \leq i \leq m\colon h(x_i) \neq y_i\}|}{m} 
    \end{align*} 
    (Recall that $|\{\dots \}|$ denotes the cardinality, i.e. the number of elements, of the set $\{\dots\}$).
    \item \textbf{Empirical Risk Minimization} (ERM): the paradigm for choosing $A$ so that it minimizes $L_S(h)$, hoping that this will translate to a minimum $L_{\mathcal{D}}(h)$ as well. Formally, this is written as:
    \begin{align*}
        \mathrm{ERM}(S) \in \arg\min L_S(h)
    \end{align*}  
    Note that there could be \textit{many} possible choiches for $h$ so that $L_S(h)$ is minimum, and the algorithm simply returns one of them. 
    \item \textbf{Overfitting}: when $L_S(h) \sim 0$, but $L_{\mathcal{D}}(h) \gg 0$.  This happens if the ERM search is not limited, meaning that the algorithm can simply \textit{memorize} the training set $S$, leading to a performance on a different dataset which is no better than chance.
    \item \textbf{Hypothesis class}: set of predictors $h \in \mathcal{H}$ that are available for the ERM search (meaning that the final $h_S$ must be $\in \mathcal{H}$). We denote a \textit{constrained } ERM algorithm with $\mathrm{ERM}_{\mathcal{H}}$, so that:
    \begin{align*}
        \mathrm{ERM}_{\mathcal{H}}(S) \in \arg\min_{h\in \mathcal{H}} L_S(h) 
    \end{align*}    
    The choice of $\mathcal{H}$ introduce a \textit{inductive bias} in the learner, and should be made according to some prior knowledge about the problem. 
    \item \textbf{PAC learnability}. A hypothesis class $\mathcal{H}$ is PAC learnable if, given a sufficient number of examples, an ERM algorithm will output an hypothesis which is \textit{probably approximately correct}, meaning that with probability $1-\delta$ it is $\epsilon$-accurate. We require the existence of a sufficient $m$ for any choice of $\epsilon$ and $\delta$, meaning that the model can be made more robust and accurate with more training samples.

    \medskip

    More formally, a hypothesis class $\mathcal{H}$ is PAC learnable if there exist a function (\textbf{sample complexity}) $m_{\mathcal{H}} \colon (0,1) \times (0,1) \to \mathbb{N}$, $(\epsilon,\delta) \mapsto m_{\mathcal{H}}(\epsilon, \delta)$, and a learning algorithm that $\forall \epsilon, \delta \in (0,1)$ and every distribution $\mathcal{D}$ over $\mathcal{X}$, and for every labelling function $f\colon \mathcal{X} \to \{0,1\}$, assuming realizability, when running over $m \geq m_{\mathcal{H}}(\epsilon, \delta)$ i.i.d. samples generated by $\mathcal{D}$ and labelled by $f$, the algorithm returns a hypothesis $h$ such that, with probability $\geq 1-\delta$ (over the choice of the samples) satisfies $L_{\mathcal{D},f} (h) \leq \epsilon$. We also require $m_{\mathcal{H}}$ to be the minimum integer that guarantees the last property.


    

\end{itemize}


\subsection{Theorems}

\subsubsection{PAC for finite hypothesis classes}
The idea is to find a minimum number of samples $m$ that are needed to construct an $S$ so that the model $h_S$ learned by the ERM algorithm will be $\epsilon-$\textit{accurate}, i.e. satisfying:
\begin{align*}
    L_{\mathcal{D},f}(h_S) \leq \epsilon
\end{align*}
with probability $\geq 1-\delta$ (\textit{confidence}) over the choice of an i.i.d. sample $S$ of size $m$ from the $\mathcal{D}$ distribution.
\medskip

\textbf{Hypotheses}
\begin{enumerate}
    \item \textbf{Finite hypothesis class}: $h_S \in \mathcal{H}$, with $|\mathcal{H}| < \infty$.
    \item \textbf{Realizability}. There exists a \q{perfect} $h^* \in \mathcal{H}$, such that $L_{\mathcal{D}, f}(h^*) = 0$. By the definition of \textit{generalization error}, this means that $h^*(x) = f(x)$ with probability $1$ for a random sample $x \in \mathcal{X}$, implying that $L_S(h^*) = 0$.
    \item \textbf{i.i.d. assumption}. The examples in the training dataset $S$ are \textbf{independently} and \textit{identically distributed} according to $\mathcal{D}$, meaning that $S \sim \prod_{i=1}^m \mathcal{D}= \mathcal{D}^m$.    
\end{enumerate}
 

\textbf{Proof}. $\delta$ is the maximum probability of the learner failing, that is of not being $\epsilon$-accurate:
\begin{align*}
     \mathcal{D}^m(\{S \colon L_{\mathcal{D},f} (h_S) > \epsilon \}) \leq \delta
\end{align*}
The learner fails only for certain \q{bad hypotheses}, that are inside a set $\mathcal{H}_B$:
\begin{align*}
    \mathcal{H}_B = \{h \in \mathcal{H}\colon L_{\mathcal{D},f}(h) > \epsilon\}
\end{align*}
The $\mathrm{ERM}_{\mathcal{H}}$ algorithm will choose an $h_S$ that verifies $L_S(h_S) = 0$, due to realizability (generally $h_S \neq h^*$, because there could be many $h_S$ with a $0$ training error). So, it may choose a \textit{bad hypothesis} only if it \q{appears good} on $S$, that is if $S$ is an element of the set of \textit{misleading training sets}: 
\begin{align*}
    M = \{S \colon \exists h \in \mathcal{H}_B, L_S(h) = 0 \}
\end{align*}   
Note that any set $S$ that results in the learner failing must be in $M$ (because it contains all sets that \textit{may be chosen} and result in failing), and so:
\begin{align*}
    \{S\colon L_{\mathcal{D},f}(h_S) > \epsilon\} \subseteq M
\end{align*}
The converse is not necessarily true: maybe there are certain sets in $M$ that, even if misleading, result nonetheless in a good performing $h_S$, because they are compatible with more than one hypothesis.

We can now rewrite $M$ as the union of misleading sets:
\begin{align}
    \{S\colon L_{\mathcal{D},f}(h_S) > \epsilon\} \subseteq M = \bigcup_{h \in \mathcal{H}_B} \{S \colon L_S(h) = 0 \}
    \label{eqn:set_inequality1}
\end{align}
Applying the probability measure to (\ref{eqn:eqn:set_inequality1}) leads to:
\begin{align}
    \mathcal{D}^m(\{S\colon L_{\mathcal{D},f}(h_S) > \epsilon\}) \leq \mathcal{D}^m(M) = \mathcal{D}^m \left( \bigcup_{h \in \mathcal{H}_B} \{S \colon L_S(h) = 0 \}\right)
    \label{eqn:inequality1}
\end{align}
Recall the \textbf{union bound}, that is the measure of the union of two sets is less or equal to the sum of the measure of each set (because there could be a non-empty intersection):
\begin{align*}
    \mathcal{D}(A \cup B) \leq \mathcal{D}(A) + \mathcal{D}(B)
\end{align*} 
Applying it to the right hand side of (\ref{eqn:inequality1}) we have:
\begin{align*}
     \mathcal{D}^m(\{S\colon L_{\mathcal{D},f}(h_S) > \epsilon\}) \leq \sum_{h \in \mathcal{H}_B} \mathcal{D}^m (\{S \colon L_S(h) = 0\})
\end{align*}
Fix a certain \textit{bad} hypothesis $h \in \mathcal{H}_B$. The event $L_S(h) = 0$ is equivalent to $\forall i $ $h(x_i) = f(x_i)$, and so:
\begin{align*}
    \mathcal{D}^m(\{S \colon L_S(h) = 0\}) &= \mathcal{D}^m (\{S \colon \forall i, h(x_i) = f(x_i)\})
\intertext{which is just the product of probabilities that each $h(x_i) = f(x_i)$, as $x_i$ are i.i.d:}
    &= \prod_{i=1}^m \mathcal{D}(\{x_i \colon h(x_i) = f(x_i)\})
\end{align*}
The probability of a good prediction is $1-$ the probability of error, which is $L_{\mathcal{D},f}$. As $h \in \mathcal{H}_B$, $L_{\mathcal{D},f} > \epsilon$, and so:
\begin{align*}
    \mathcal{D}(\{x_i \colon h(x_i) = y_i\}) = 1- L_{\mathcal{D},f} (h) \leq 1 - \epsilon
\end{align*} 
Substituting back leads to:
\begin{align*}
    \mathcal{D}^m (\{S \colon L_S(h) = 0\})\leq (1-\epsilon)^m \leq e^{- \epsilon m}
\end{align*}
And finally:
\begin{align*}
     \textcolor{Red}{\mathcal{D}^m(\{S\colon L_{\mathcal{D},f}(h_S) > \epsilon\})} \leq \sum_{h \in \mathcal{H}_B} \mathcal{D}^m (\{S \colon L_S(h) = 0\}) \leq |\mathcal{H}_B| e^{-\epsilon m} \leq |\mathcal{H}| e^{-\epsilon m}
\end{align*}
We now want to choose $m$ so that the red term is $\leq \delta$. So we impose:
\begin{align*}
    |\mathcal{H}|e^{-\epsilon m} \leq \delta \Rightarrow 
    m \geq \frac{\log(|\mathcal{H}|/\delta)}{\epsilon} 
\end{align*}

This means that \textit{every finite hypothesis class is PAC learnable with sample complexity}:
\begin{align*}
    m_{\mathcal{H}}(\epsilon, \delta) \leq \frac{\log(|\mathcal{H}|/\delta)}{\epsilon} 
\end{align*}  

\section{Agnostic Simple Learning}
We consider now a binary classification task where features are not sufficient to uniquely predict labels. In other words, a couple of exactly equal features can lead to opposite labels.

\begin{itemize}
    \item \textbf{Data generation}. Consider a distribution $\mathcal{D}$ over $\mathcal{X}\times \mathcal{Y}$, with $\mathcal{Y}= \{0,1\}$. The probability of sampling a certain datapoint $(x, y)$ is given by:
    \begin{align*}
    \mathbb{P}(x,y) = \mathbb{P}(x) \mathbb{P}(y|x)    
    \end{align*}
    For example $x=4$ can be associated to $y=0$ in $75\%$ of the cases, and to $y=1$ in the other $25\%$. In such a case realizability cannot hold, as the learner is deterministic, and must choose one output or the other, meaning that it is bound to make a certain error.
    \item \textbf{True error}. The risk of a predictor $h$ is given by the probability of sampling $(x,y) \sim \mathcal{D}$ so that $h(x) \neq y$:
    \begin{align*}
        L_{\mathcal{D}}(h) \equiv \underset{(x,y) \sim \mathcal{D}}{\mathbb{P}} [h(x) \neq y] \equiv \mathcal{D}(\{(x,y) \colon h(x) \neq y\})
    \end{align*} 
    \item \textbf{Empirical risk} maintains the same formula as before:
    \begin{align*}
    L_S(h) \equiv \frac{|\{1 \leq 1 \leq m\colon h(x_i) \neq y_i\}|}{m}     
    \end{align*} 
    \item \textbf{Bayes Optimal Predictor} Given any probability distribution $\mathcal{D}$ over $\mathcal{X}\times \{0,1\}$, the best label predicting function $\mathcal{X} \to \{0,1\}$ is:
    \begin{align*}
        f_{\mathcal{D}}(x) = \begin{cases}
            1 & \mathbb{P}[y=1|x] \geq 1/2\\
            0 & \text{otherwise}
        \end{cases}
    \end{align*} 
    That is, predict $1$ if the probability of $x$ being $y=1$ is greater than chance ($1/2$), and $0$ otherwise.
    \item \textbf{Agnostic PAC learnability}. A hypothesis class $\mathcal{H}$ is agnostic PAC learnable if there exists a function $m_{\mathcal{H}}\colon (0,1)^2 \to \mathbb{N}$ and a learning algorithm with the following property: for every $\epsilon, \delta \in (0,1)$ and every distribution $\mathcal{D}$ over $\mathcal{X}\times \mathcal{Y}$, when running the algorithm on $m \geq m_{\mathcal{H}}(\epsilon, \delta)$ i.i.d. samples generated by $\mathcal{D}$, the algorithm returns an hypothesis $h$ such that, with probability $\geq 1 -\delta$ it satisfies:
    \begin{align*}
        L_{\mathcal{D}}(h) \leq \min_{h' \in \mathcal{H}} L_{\mathcal{D}} (h') + \epsilon
    \end{align*} 

\end{itemize}

\section{Agnostic PAC learnable}
Finally, we generalize the previous case to a \textit{general} learning task, i.e. classification with a larger number of classes, or regression.

\begin{itemize}
    \item \textbf{Generalized Loss Functions}. A loss function $\ell \colon \mathcal{H}\times Z \to \mathbb{R}_+$, with $Z$ being a certain domain, is a function that \textit{evaluates} the performance of a model $h \in \mathcal{H}$ on the domain $Z$.
    \item \textbf{Risk function} is defined just as the expected value of the loss:
    \begin{align*}
        L_{\mathcal{D}}(h) \equiv \mathbb{E}_{z \sim \mathcal{D}}[\ell(h,z)]
    \end{align*}
    \item The \textbf{empirical risk} is the average loss over the training set:
    \begin{align*}
        L_S(h) \equiv \frac{1}{m} \sum_{i=1}^m \ell(h, z_i) 
    \end{align*} 
    \item \textbf{Agnostic PAC learnability with generalized loss function}. A hypothesis class $\mathcal{H}$ is agnostic PAC learnable with respect to a domain $Z$ and a loss function $\ell\colon \mathcal{H}\times Z \to \mathbb{R}_+$ if there exist a function $m_{\mathcal{H}}\colon (0,1)^2 \to \mathbb{N}$ and a learning algorithm with the following property: for every $\epsilon,\delta \in (0,1)$, and every distribution $\mathcal{D}$ over $Z$, running the algorithm over $m \geq m_{\mathcal{H}}$ i.i.d. samples $S$ generated by $\mathcal{D}$ results in a hypothesis $h_S \in \mathcal{H}$ that, with probability $1-\delta$, satisfies:
    \begin{align*}
        L_{\mathcal{D}}(h_S) \leq \min_{h' \in \mathcal{H}} L_{\mathcal{D}}(h') + \epsilon
    \end{align*}
    where $L_{\mathcal{D}}(h_S) = \mathbb{E}_{z \sim \mathcal{D}}[\ell(h_S,z)]$.
\end{itemize}

\section{Uniform Convergence}
ERM algorithm minimizes the empirical risk, hoping that this will result in a low true risk as well. So, we want to prove which conditions are needed so that $L_S$ is close to $L_{\mathcal{D}}$.

\begin{itemize}
    \item \textbf{$\epsilon$-representative}: A training set $S$ is called $\epsilon$-representative (with respect to a distribution $\mathcal{D}$ over a domain $Z$, a hypothesis class $\mathcal{H}$, and a loss $\ell$) if:
    \begin{align*}
        \forall h \in \mathcal{H}, \quad |L_S(h) - L_{\mathcal{D}}(h)| \leq \epsilon
    \end{align*} 
    \item \textbf{Uniform convergence}: A hypothesis class $\mathcal{H}$ has the \textit{uniform convergence property} (with respect to a domain $Z$ and a loss function $\ell$) if there exists a function $m_{\mathcal{H}}^{\mathrm{UC}}\colon (0,1)^2 \to \mathbb{N}$ such that for every $\epsilon$, $\delta$ $\in (0,1)$ and for every probability distribution $\mathcal{D}$ over $Z$, if $S$ is a sample of at least $m \geq m_{\mathcal{H}}^{\mathrm{UC}}(\epsilon, \delta)$ examples drawn i.i.d. from $\mathcal{D}$, then, with probability of at least $1-\delta$, $S$ is $\epsilon$-representative.    
\end{itemize}

\subsection{Theorems}
\begin{itemize}
    \item \textbf{Learned hypotheses on representative sample}. If a training set $S$ is $\epsilon/2$ representative, then \textbf{any} output of the learner $\mathrm{ERM}_{\mathcal{H}}(S)$, i.e. any:
    \begin{align*}
        h_S \in \arg\min_{h \in \mathcal{H}}L_S(h)
    \end{align*}
    satisfies:
    \begin{align*}
        L_{\mathcal{D}}(h_S) \leq \min_{h \in \mathcal{H}} L_{\mathcal{D}}(h) + \epsilon
    \end{align*}
    \textbf{Proof}. By the definition of $\epsilon/2$-representative we have:
    \begin{align} \label{eqn:e-inequality}
        -\frac{\epsilon}{2} \leq L_{S}(h) - L_{\mathcal{D}}(h) \leq \frac{\epsilon}{2} 
    \end{align}
    Rearranging to isolate $L_{\mathcal{D}}(h)$:
    \begin{align*}
        -\frac{\epsilon}{2} - L_S(h) \leq -L_{\mathcal{D}}(h) \leq \frac{\epsilon}{2} - L_{S}(h) \Rightarrow \frac{\epsilon}{2} + L_S(h) \geq L_{\mathcal{D}}(h) \geq -\frac{\epsilon}{2} + L_S(h)     
    \end{align*}
    This inequality holds for any $h\in \mathcal{H}$, and in particular for the $h_S$ outputted by the learner:
    \begin{align*}
        L_{\mathcal{D}}(h_S) \leq L_S(h_S) + \frac{\epsilon}{2} 
    \end{align*}
    $h_S$ is a minimum point for $L_S$ (by the definition of ERM algorithm) and so $L_S(h_S) \leq L_S(h)$, leading to:
    \begin{align} \label{eqn:ineq}
        L_{\mathcal{D}}(h_S) \leq L_S(h_S) + \frac{\epsilon}{2} \leq L_S(h) + \frac{\epsilon}{2} 
    \end{align}
    We can now reuse (\ref{eqn:e-inequality}) to get an inequality for $L_S(h)$:
    \begin{align*}
        L_S(h) \leq L_{\mathcal{D}}(h) + \frac{\epsilon}{2} 
    \end{align*}
    That can be applied in (\ref{eqn:ineq}) completing the chain:
    \begin{align*}
        L_{\mathcal{D}}(h_S) \leq L_S(h_S) + \frac{\epsilon}{2} \leq L_S(h) + \frac{\epsilon}{2} \leq L_{\mathcal{D}}(h) + \frac{\epsilon}{2} + \frac{\epsilon}{2} = L_{\mathcal{D}}(h) + \epsilon  
    \end{align*}
    This holds for any $h \in \mathcal{H}$, and so we can pick the \textit{minimum} $L_{\mathcal{D}}(h)$ to get the stronger condition:
    \begin{align*}
        L_{\mathcal{D}}(h_S) \leq \min_{h \in \mathcal{H}} L_{\mathcal{D}}(h) + \epsilon
    \end{align*}  
    In other words, if $S$ is $\epsilon$-representative, then the true risk and the empirical risk are close to each other.

    \item \textbf{Uniform convergence implies PAC learnability}. If a class $\mathcal{H}$ has the uniform convergence property with a sample complexity $m_{\mathcal{H}}^{\mathrm{UC}}$ then the class is agnostically PAC learnable with the sample complexity $m_{\mathcal{H}}(\epsilon, \delta) \leq m_{\mathcal{H}}^{\mathrm{UC}}(\epsilon/2, \delta)$.
    
    \medskip

    \textbf{Proof}. Thanks to uniform convergence, we are certain that we can use the ERM algorithm to get \textit{nice} results, meaning empirical risks that are close to true risks, and so, using the previous theorem, we reach the PAC learning condition.

    \item \textbf{Finite Classes are agnostic PAC learnable}. 
    We want to prove that uniform convergence holds for finite classes, implying that they are agnostic PAC learnable.

    Fix some accuracy $\epsilon$ and confidence $\delta$. We need to find a sufficient $m = |S|$ so that:
    \begin{align*}
        \mathcal{D}^m(\{S \colon \forall h \in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h)| \leq \epsilon \}) \geq 1-\delta
    \end{align*}
    That is, if we pick $m$ samples from $\mathcal{D}$ to generate $S$, then with probability $1-\delta$, uniformly for \textit{any} hypothesis $h \in \mathcal{H}$, the empirical risk and true risk are $\epsilon$-close: $L_S(h) - L_{\mathcal{D}}(h)| \leq \epsilon$.
    
    If the probability of an event $p$ is $\geq 1-\delta$, then the probability of the opposite event $\bar{p}$ is $< 1-(1-\delta) = \delta$:
    \begin{align*}
        \mathcal{D}^m(\{S \colon \exists h \in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon \}) < \delta
    \end{align*} 
    (Note how $\forall \to \exists$).

    Consider now the argument of $\mathcal{D}^m$. This is the set of all samples sets $S$ for which there is an hypothesis that satisfies some property. Equivalently, we could select all samples with that property on a \textit{fixed} $h$, do the same for every other $h$, and join all the results: 
    \begin{align*}
        \{S \colon \exists h \in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon\} = \bigcup_{h \in \mathcal{H}} \{S \colon |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon \}
    \end{align*} 
    Applying $\mathcal{D}^m$ to both sides along with the union bound leads to:
    \begin{align*}
        \mathcal{D}^m(\{S \colon \exists h \in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon \}) \leq \sum_{h \in \mathcal{H}} \mathcal{D}^m (\{S\colon |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon\})
    \end{align*}
    We now consider a generic element of the sum, and search for a upper bound. The idea is that $L_S(h)$ is an \textit{estimator} of $L_{\mathcal{D}}(h)$ (it is a sample mean vs an expected value), and so it should be \textit{closer to it} if we increase the sample size $m$. In fact:
    \begin{align*}
        L_{\mathcal{D}}(h) = \mathbb{E}_{z \sim \mathcal{D}}[\ell(h, z)] \qquad L_S(h) = \frac{1}{m} \sum_{i=1}^m \ell(h, z_i) 
    \end{align*}   
    meaning that $L_{\mathcal{D}}(h) = \mathbb{E}_{z \sim \mathcal{D}}[L_S(h)]$. The gap between averages (over i.i.d. samples) and their expected value can be quantified through \textbf{Hoeffding inequality}.
    
    \medskip

    \textbf{Hoeffding inequality}. Let $\theta_1, \dots, \theta_m$ be a sequence of i.i.d. random variables and assume that for all $i$, $\mathbb{E}[\theta_i] = \mu$ and $\mathbb{P}[a \leq \theta_i \leq b] =1$ (i.e. the support of the probability distribution lies in $[a,b]$). Then, for any $\epsilon > 0$:
    \begin{align*}
        \mathbb{P} \left[\left| \frac{1}{m} \sum_{i=1}^m \theta_i - \mu  \right | > \epsilon\right] \leq 2 \exp\left(-\frac{2 m \epsilon^2}{(b-a)^2} \right)
    \end{align*}
    
    \medskip

    In our case, this means that:
    \begin{align*}
        \mathcal{D}^m(\{S\colon |L_S(h) - L_{\mathcal{D}}(h)|> \epsilon\}) \leq 2 \exp\left(-\frac{2m \epsilon^2}{(b-a)^2} \right)
    \end{align*}
    We assume $\ell \in [0,1]$, meaning that $b-a=1$. Then, computing the sum:
    \begin{align*}
        \textcolor{Red}{\mathcal{D}^m(\{S \colon \exists h \in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon \})} \leq \sum_{h \in \mathcal{H}} 2 \exp(-2 m \epsilon^2) = 2 |\mathcal{H}| \exp(-2 m \epsilon^2)
    \end{align*}
    For our thesis, we want the red term to be $< \delta$, and so we impose:
    \begin{align*}
        2 |\mathcal{H}| \exp(-2 m \epsilon^2) < \delta \Rightarrow m \geq \frac{1}{2 \epsilon^2} \log\left(\frac{2|\mathcal{H}|}{\delta} \right) 
    \end{align*}
    
    So, let $\mathcal{H}$ be a finite hypothesis class, let $Z$ be a domain, and let $\ell \colon \mathcal{H}\times Z \to [0,1]$ be a loss function. Then $\mathcal{H}$ enjoys the uniform convergence property with sample complexity:
    \begin{align*}
        m_{\mathcal{H}}^{\mathrm{UC}}(\epsilon, \delta) \leq \left \lceil \frac{1}{2 \epsilon^2} \log(\frac{2|\mathcal{H}|}{\delta} )\right \rceil
    \end{align*}
    (Note that previously we found a \textit{general} sufficient condition for the minimum size of $m$ needed for uniform convergence. In a specific case, it is probable that a much lower $m$ would allow the same property.)

\end{itemize}

\section{Bias-complexity tradeoff}
\begin{itemize}
    \item \textbf{Error decomposition}. The true risk $L_{\mathcal{D}}(h_S)$ of an output $h_S$ of the $\mathrm{ERM}_{\mathcal{H}}$ algorithm is a combination of the \textit{bias} contained in the choice of $\mathcal{H}$ (which could be not optimal) - the \textbf{approximation error} - and the inefficiency of the ERM algorithm in minimizing the true risk (as it minimizes the empirical risk) - the \textbf{estimation error}:
    \begin{align*}
        L_{\mathcal{D}}(h_S) = \epsilon_{\mathrm{approximation}} + \epsilon_{\mathrm{estimation}} \span \\
        \epsilon_{\mathrm{app}} = \min_{h \in \mathcal{H}} L_{\mathcal{D}}(h); \qquad \epsilon_{\mathrm{est}} = L_{\mathcal{D}} (h_S) - \epsilon_{\mathrm{app}}
    \end{align*}
    \begin{itemize}
        \item $\epsilon_{\mathrm{app}}$ usually decreases when enlarging $|\mathcal{H}|$, and does not depend on $m$. If realizability holds, it is $0$, otherwise it includes always the error of the Bayes optimal predictor.
        \item $\epsilon_{\mathrm{est}}$ increases logarithmically with $|\mathcal{H}|$ (because it is easier to overfit) and decreases with $m$.
    \end{itemize}    
    $|\mathcal{H}|$ too large makes $\epsilon_{\mathrm{est}}$ dominate (overfitting), while $|\mathcal{H}|$ too small makes $\epsilon_{\mathrm{app}}$ dominate (underfitting). 
\end{itemize}

\subsection{Theorems}
\begin{itemize}
    \item \textbf{No free lunch}. Let $A$ be any learning algorithm for the task of binary classification with respect to the $0-1$ loss over a domain $\mathcal{X}$. Let $m$ be any number smaller than $|\mathcal{X}|/2$, representing a training set size (so the algorithm knows \textit{less} than half the elements). Then, there exists a distribution $\mathcal{D}$ over $\mathcal{X}\times \{0,1\}$ such that: 
    \begin{enumerate}
        \item There exists a function $f \colon \mathcal{X}-> \{0,1\}$ with $L_{\mathcal{D}} (f) = 0$
        \item With probability of at least $1/7$ over the choice of $S \sim \mathcal{D}^m$ we have that $L_{\mathcal{D}}(A(S)) \geq 1/8$.  
    \end{enumerate}
    \item \textbf{No prior knowledge implies not PAC learnable}. Let $\mathcal{X}$ be an \textit{infinite} domain set and let $\mathcal{H}$ be the set of \textbf{all} functions $\mathcal{X} \to \{0,1\}$ (no bias, no prior knowledge). Then $\mathcal{H}$ is not PAC learnable.  
    \medskip

    \textbf{Proof}. Just pick $\epsilon < 1/8$ and $\delta < 1/7$, suppose (by absurdity) that $\mathcal{H}$ is PAC learnable, and apply the no-free-lunch theorem to get a contradiction (no amount of $m$, as $|\mathcal{H}| = \infty$ is $> |\mathcal{H}|/2$, and so with probability $> \delta > 1/7$, $L_{\mathcal{D}}(A(S)) > 1/8 > \epsilon$). 
\end{itemize}

\section{VC-Dimension}

\begin{itemize}
    \item \textbf{Restriction of a hypothesis class to a set of samples}. Let $\mathcal{H}$ be a class of functions $\mathcal{X} \to \{0,1\}$ and let $C = \{c_1, \dots, c_m\} \subset \mathcal{X}$. The restriction of $\mathcal{H}$ to $C$ is the set of functions $C \to \{0,1\}$ that can be derived from $\mathcal{H}$. 
    
    We can identify these functions by their outcomes on the elements of $C$:
    \begin{align*}
        \mathcal{H}_C = \{(h(c_1), \dots, h(c_m)) \colon h \in \mathcal{H}\}
    \end{align*}
    Note that if two $h$ result in the same outcomes on $C$, then they are effectively \textit{the same function} on the restricted class. So $\mathcal{H}_C$ is the \textit{intersection} between the set of all functions $C \to \{0,1\}$ and $\mathcal{H}^{|C|}$. 

    \item \textbf{Shattering}. A hypothesis class $\mathcal{H}$ shatters a finite set $C \subset \mathcal{X}$ if the restriction of $\mathcal{H}$ to $C$ is the set of all functions $C \to \{0,1\}$. That is, $|\mathcal{H}_C| = 2^{|C|}$.
    
    \medskip

    \textbf{Example}. Let $\mathcal{H}$ be the hypothesis class of threshold functions $h_a \colon \mathbb{R} \to \{0,1\}$:
    \begin{align*}
        \mathcal{H}\ni h_a (x) = \begin{cases}
            1 & x > a\\
            0 & x \leq a
        \end{cases}
    \end{align*} 
    Consider $C = \{c_1\}$. If we take $a = c_1 - 1$, then $h_a(c_1) = 1$, but for $b = c_1 +1$ we have $h_b(c_1) = 0$. As the set of all functions $C \to \{0,1\}$ is just $\{0,1\}$, this means that $\mathcal{H}$ shatters $C$.

    However, consider $D = \{c_1,c_2\}$ with $c_1 < c_2$. Here the set of all functions $D \to \{0,1\}$ is $\{(0,0), (0,1), (1,0), (1,1)\}$. By correctly choosing $a$, we can construct only $\{0,0\}$, $\{1,1\}$ or $\{0,1\}$, meaning that $\mathcal{H}$ does not shatter $D$.

\end{itemize}

\subsection{Theorems}
\begin{itemize}
    \item \textbf{Shattering and No-Free-Lunch}. Let $\mathcal{H}$ be a hypothesis class of functions $\mathcal{X} \to\{0,1\}$. Let $m$ be a training set size. Assume that there exists a set $C \subset \mathcal{X}$ of double the size ($|C| = 2m$) that is shattered by $\mathcal{H}$. Then, for \textit{any} learning algorithm $A$, there exist a distribution $\mathcal{D}$ over $\mathcal{X}\times \{0,1\}$ and a predictor $h \in \mathcal{H}$ such that $L_{\mathcal{D}}(h) = 0$ but with probability of at least $1/7$ over the choice of $S \sim \mathcal{D}^m$ we have that $L_{\mathcal{D}}(A(S)) \geq 1/8$.   
\end{itemize}






\end{document}
