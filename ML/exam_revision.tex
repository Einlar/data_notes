%&latex
%
\documentclass[../template.tex]{subfiles}

\begin{document}

\chapter{Exam revision}

\section{Simple Learning Framework}
The simplest learning task is to \textit{classify} features in two classes, in the case where there exists a unique and deterministic map between them. 
\begin{itemize}
    \item \textbf{Domain set} (\textit{Instance space}) $\mathcal{X}$, where elements $\bm{x} \in \mathcal{X}$ are \textit{vectors of features}.
    \item \textbf{Label set} $\mathcal{Y}$
    \item \textbf{Training data}: sequence (order matters, there can be repetitions) of elements in $\mathcal{X}\times \mathcal{Y}$: $S=((x_1 , y_1 ), \dots, (x_m, y_m))$. $m$ denotes the number of training samples.
    \item The ML algorithm $A$, given $S$, outputs a \textbf{hypothesis function} (prediction rule, or \textit{predictor}) $A(S) = h_S\colon \mathcal{X} \to \mathcal{Y}$ that represents the mapping between $\mathcal{X}$ and $\mathcal{Y}$ \textit{learned} by the algorithm.
    \item We assume a \textbf{simple data generation model} for constructing $S$. $\mathcal{X}$ is generated by sampling a distribution (pdf) $\mathcal{D}$ which is \textit{not known} by the learning model, and are then labelled by a function $f\colon \mathcal{X} \to \mathcal{Y}$ (labelling function, representing the \textit{ground truth}).  $\mathcal{D}$ allows to assign probabilities to \textit{events}, i.e. subsets $A \subset \mathcal{X}$. We introduce the following notation:
    \begin{align*}
        \mathcal{D}(A) = \mathbb{P}[x \in A] \qquad A \subset \mathcal{X}
    \end{align*} 
    Equivalently, we can denote subsets $A$ using their \textit{characteristic function} $\pi_A(x) \colon \mathcal{X} \to \{0,1\}$:
    \begin{align*}
        \pi_A(x) = \begin{cases}
            1 & x \in A\\
            0 & x \not\in A
        \end{cases} \qquad A = \{x \in A \colon \pi(x) = 1\}
    \end{align*} 
    And so $\mathcal{D}(A) = \mathbb{P}_{x \sim \mathcal{D}}[\pi_A(x)]$.
    \item We define the \textbf{generalization error} (or \textbf{risk}, \textbf{true error})  of $h_S$ as the probability of randomly choosing a sample from $\mathcal{D}$ so that $h_S(x) \neq f(x)$:
    \begin{align*}
        L_{\mathcal{D},f}(h_S) \equiv \underset{x \sim \mathcal{D}}{\mathbb{P}}[h_S(x) \neq f(x)] \equiv \mathcal{D}(\{x \colon h_S(x) \neq f(x)\})
    \end{align*} 
    Note that $L_{\mathcal{D},f}$ depends on the \textit{data generation} ($\mathcal{D}$, $f$), and so cannot be known by the learner. For simplicity, we denote $L_{\mathcal{D}} \equiv L_{\mathcal{D},f}$.
    \item The \textbf{empirical risk} (or \textbf{training error}) is defined as:
    \begin{align*}
        L_S(h) \equiv \frac{|\{1 \leq i \leq m\colon h(x_i) \neq y_i\}|}{m} 
    \end{align*} 
    (Recall that $|\{\dots \}|$ denotes the cardinality, i.e. the number of elements, of the set $\{\dots\}$).
    \item \textbf{Empirical Risk Minimization} (ERM): the paradigm for choosing $A$ so that it minimizes $L_S(h)$, hoping that this will translate to a minimum $L_{\mathcal{D}}(h)$ as well. Formally, this is written as:
    \begin{align*}
        \mathrm{ERM}(S) \in \arg\min L_S(h)
    \end{align*}  
    Note that there could be \textit{many} possible choiches for $h$ so that $L_S(h)$ is minimum, and the algorithm simply returns one of them. 
    \item \textbf{Overfitting}: when $L_S(h) \sim 0$, but $L_{\mathcal{D}}(h) \gg 0$.  This happens if the ERM search is not limited, meaning that the algorithm can simply \textit{memorize} the training set $S$, leading to a performance on a different dataset which is no better than chance.
    \item \textbf{Hypothesis class}: set of predictors $h \in \mathcal{H}$ that are available for the ERM search (meaning that the final $h_S$ must be $\in \mathcal{H}$). We denote a \textit{constrained } ERM algorithm with $\mathrm{ERM}_{\mathcal{H}}$, so that:
    \begin{align*}
        \mathrm{ERM}_{\mathcal{H}}(S) \in \arg\min_{h\in \mathcal{H}} L_S(h) 
    \end{align*}    
    The choice of $\mathcal{H}$ introduce a \textit{inductive bias} in the learner, and should be made according to some prior knowledge about the problem. 
    \item \textbf{PAC learnability}. A hypothesis class $\mathcal{H}$ is PAC learnable if, given a sufficient number of examples, an ERM algorithm will output an hypothesis which is \textit{probably approximately correct}, meaning that with probability $1-\delta$ it is $\epsilon$-accurate. We require the existence of a sufficient $m$ for any choice of $\epsilon$ and $\delta$, meaning that the model can be made more robust and accurate with more training samples.

    \medskip

    More formally, a hypothesis class $\mathcal{H}$ is PAC learnable if there exist a function (\textbf{sample complexity}) $m_{\mathcal{H}} \colon (0,1) \times (0,1) \to \mathbb{N}$, $(\epsilon,\delta) \mapsto m_{\mathcal{H}}(\epsilon, \delta)$, and a learning algorithm that $\forall \epsilon, \delta \in (0,1)$ and every distribution $\mathcal{D}$ over $\mathcal{X}$, and for every labelling function $f\colon \mathcal{X} \to \{0,1\}$, assuming realizability, when running over $m \geq m_{\mathcal{H}}(\epsilon, \delta)$ i.i.d. samples generated by $\mathcal{D}$ and labelled by $f$, the algorithm returns a hypothesis $h$ such that, with probability $\geq 1-\delta$ (over the choice of the samples) satisfies $L_{\mathcal{D},f} (h) \leq \epsilon$. We also require $m_{\mathcal{H}}$ to be the minimum integer that guarantees the last property.


    

\end{itemize}


\subsection{Theorems}

\subsubsection{PAC for finite hypothesis classes}
The idea is to find a minimum number of samples $m$ that are needed to construct an $S$ so that the model $h_S$ learned by the ERM algorithm will be $\epsilon-$\textit{accurate}, i.e. satisfying:
\begin{align*}
    L_{\mathcal{D},f}(h_S) \leq \epsilon
\end{align*}
with probability $\geq 1-\delta$ (\textit{confidence}) over the choice of an i.i.d. sample $S$ of size $m$ from the $\mathcal{D}$ distribution.
\medskip

\textbf{Hypotheses}
\begin{enumerate}
    \item \textbf{Finite hypothesis class}: $h_S \in \mathcal{H}$, with $|\mathcal{H}| < \infty$.
    \item \textbf{Realizability}. There exists a \q{perfect} $h^* \in \mathcal{H}$, such that $L_{\mathcal{D}, f}(h^*) = 0$. By the definition of \textit{generalization error}, this means that $h^*(x) = f(x)$ with probability $1$ for a random sample $x \in \mathcal{X}$, implying that $L_S(h^*) = 0$.
    \item \textbf{i.i.d. assumption}. The examples in the training dataset $S$ are \textbf{independently} and \textit{identically distributed} according to $\mathcal{D}$, meaning that $S \sim \prod_{i=1}^m \mathcal{D}= \mathcal{D}^m$.    
\end{enumerate}
 

\textbf{Proof}. $\delta$ is the maximum probability of the learner failing, that is of not being $\epsilon$-accurate:
\begin{align*}
     \mathcal{D}^m(\{S \colon L_{\mathcal{D},f} (h_S) > \epsilon \}) \leq \delta
\end{align*}
The learner fails only for certain \q{bad hypotheses}, that are inside a set $\mathcal{H}_B$:
\begin{align*}
    \mathcal{H}_B = \{h \in \mathcal{H}\colon L_{\mathcal{D},f}(h) > \epsilon\}
\end{align*}
The $\mathrm{ERM}_{\mathcal{H}}$ algorithm will choose an $h_S$ that verifies $L_S(h_S) = 0$, due to realizability (generally $h_S \neq h^*$, because there could be many $h_S$ with a $0$ training error). So, it may choose a \textit{bad hypothesis} only if it \q{appears good} on $S$, that is if $S$ is an element of the set of \textit{misleading training sets}: 
\begin{align*}
    M = \{S \colon \exists h \in \mathcal{H}_B, L_S(h) = 0 \}
\end{align*}   
Note that any set $S$ that results in the learner failing must be in $M$ (because it contains all sets that \textit{may be chosen} and result in failing), and so:
\begin{align*}
    \{S\colon L_{\mathcal{D},f}(h_S) > \epsilon\} \subseteq M
\end{align*}
The converse is not necessarily true: maybe there are certain sets in $M$ that, even if misleading, result nonetheless in a good performing $h_S$, because they are compatible with more than one hypothesis.

We can now rewrite $M$ as the union of misleading sets:
\begin{align}
    \{S\colon L_{\mathcal{D},f}(h_S) > \epsilon\} \subseteq M = \bigcup_{h \in \mathcal{H}_B} \{S \colon L_S(h) = 0 \}
    \label{eqn:set_inequality1}
\end{align}
Applying the probability measure to (\ref{eqn:eqn:set_inequality1}) leads to:
\begin{align}
    \mathcal{D}^m(\{S\colon L_{\mathcal{D},f}(h_S) > \epsilon\}) \leq \mathcal{D}^m(M) = \mathcal{D}^m \left( \bigcup_{h \in \mathcal{H}_B} \{S \colon L_S(h) = 0 \}\right)
    \label{eqn:inequality1}
\end{align}
Recall the \textbf{union bound}, that is the measure of the union of two sets is less or equal to the sum of the measure of each set (because there could be a non-empty intersection):
\begin{align*}
    \mathcal{D}(A \cup B) \leq \mathcal{D}(A) + \mathcal{D}(B)
\end{align*} 
Applying it to the right hand side of (\ref{eqn:inequality1}) we have:
\begin{align*}
     \mathcal{D}^m(\{S\colon L_{\mathcal{D},f}(h_S) > \epsilon\}) \leq \sum_{h \in \mathcal{H}_B} \mathcal{D}^m (\{S \colon L_S(h) = 0\})
\end{align*}
Fix a certain \textit{bad} hypothesis $h \in \mathcal{H}_B$. The event $L_S(h) = 0$ is equivalent to $\forall i $ $h(x_i) = f(x_i)$, and so:
\begin{align*}
    \mathcal{D}^m(\{S \colon L_S(h) = 0\}) &= \mathcal{D}^m (\{S \colon \forall i, h(x_i) = f(x_i)\})
\intertext{which is just the product of probabilities that each $h(x_i) = f(x_i)$, as $x_i$ are i.i.d:}
    &= \prod_{i=1}^m \mathcal{D}(\{x_i \colon h(x_i) = f(x_i)\})
\end{align*}
The probability of a good prediction is $1-$ the probability of error, which is $L_{\mathcal{D},f}$. As $h \in \mathcal{H}_B$, $L_{\mathcal{D},f} > \epsilon$, and so:
\begin{align*}
    \mathcal{D}(\{x_i \colon h(x_i) = y_i\}) = 1- L_{\mathcal{D},f} (h) \leq 1 - \epsilon
\end{align*} 
Substituting back leads to:
\begin{align*}
    \mathcal{D}^m (\{S \colon L_S(h) = 0\})\leq (1-\epsilon)^m \leq e^{- \epsilon m}
\end{align*}
And finally:
\begin{align*}
     \textcolor{Red}{\mathcal{D}^m(\{S\colon L_{\mathcal{D},f}(h_S) > \epsilon\})} \leq \sum_{h \in \mathcal{H}_B} \mathcal{D}^m (\{S \colon L_S(h) = 0\}) \leq |\mathcal{H}_B| e^{-\epsilon m} \leq |\mathcal{H}| e^{-\epsilon m}
\end{align*}
We now want to choose $m$ so that the red term is $\leq \delta$. So we impose:
\begin{align*}
    |\mathcal{H}|e^{-\epsilon m} \leq \delta \Rightarrow 
    m \geq \frac{\log(|\mathcal{H}|/\delta)}{\epsilon} 
\end{align*}

This means that \textit{every finite hypothesis class is PAC learnable with sample complexity}:
\begin{align*}
    m_{\mathcal{H}}(\epsilon, \delta) \leq \frac{\log(|\mathcal{H}|/\delta)}{\epsilon} 
\end{align*}  

\section{Agnostic Simple Learning}
We consider now a binary classification task where features are not sufficient to uniquely predict labels. In other words, a couple of exactly equal features can lead to opposite labels.

\begin{itemize}
    \item \textbf{Data generation}. Consider a distribution $\mathcal{D}$ over $\mathcal{X}\times \mathcal{Y}$, with $\mathcal{Y}= \{0,1\}$. The probability of sampling a certain datapoint $(x, y)$ is given by:
    \begin{align*}
    \mathbb{P}(x,y) = \mathbb{P}(x) \mathbb{P}(y|x)    
    \end{align*}
    For example $x=4$ can be associated to $y=0$ in $75\%$ of the cases, and to $y=1$ in the other $25\%$. In such a case realizability cannot hold, as the learner is deterministic, and must choose one output or the other, meaning that it is bound to make a certain error.
    \item \textbf{True error}. The risk of a predictor $h$ is given by the probability of sampling $(x,y) \sim \mathcal{D}$ so that $h(x) \neq y$:
    \begin{align*}
        L_{\mathcal{D}}(h) \equiv \underset{(x,y) \sim \mathcal{D}}{\mathbb{P}} [h(x) \neq y] \equiv \mathcal{D}(\{(x,y) \colon h(x) \neq y\})
    \end{align*} 
    \item \textbf{Empirical risk} maintains the same formula as before:
    \begin{align*}
    L_S(h) \equiv \frac{|\{1 \leq 1 \leq m\colon h(x_i) \neq y_i\}|}{m}     
    \end{align*} 
    \item \textbf{Bayes Optimal Predictor} Given any probability distribution $\mathcal{D}$ over $\mathcal{X}\times \{0,1\}$, the best label predicting function $\mathcal{X} \to \{0,1\}$ is:
    \begin{align*}
        f_{\mathcal{D}}(x) = \begin{cases}
            1 & \mathbb{P}[y=1|x] \geq 1/2\\
            0 & \text{otherwise}
        \end{cases}
    \end{align*} 
    That is, predict $1$ if the probability of $x$ being $y=1$ is greater than chance ($1/2$), and $0$ otherwise.
    \item \textbf{Agnostic PAC learnability}. A hypothesis class $\mathcal{H}$ is agnostic PAC learnable if there exists a function $m_{\mathcal{H}}\colon (0,1)^2 \to \mathbb{N}$ and a learning algorithm with the following property: for every $\epsilon, \delta \in (0,1)$ and every distribution $\mathcal{D}$ over $\mathcal{X}\times \mathcal{Y}$, when running the algorithm on $m \geq m_{\mathcal{H}}(\epsilon, \delta)$ i.i.d. samples generated by $\mathcal{D}$, the algorithm returns an hypothesis $h$ such that, with probability $\geq 1 -\delta$ it satisfies:
    \begin{align*}
        L_{\mathcal{D}}(h) \leq \min_{h' \in \mathcal{H}} L_{\mathcal{D}} (h') + \epsilon
    \end{align*} 

\end{itemize}

\section{Agnostic PAC learnable}
Finally, we generalize the previous case to a \textit{general} learning task, i.e. classification with a larger number of classes, or regression.

\begin{itemize}
    \item \textbf{Generalized Loss Functions}. A loss function $\ell \colon \mathcal{H}\times Z \to \mathbb{R}_+$, with $Z$ being a certain domain, is a function that \textit{evaluates} the performance of a model $h \in \mathcal{H}$ on the domain $Z$.
    \item \textbf{Risk function} is defined just as the expected value of the loss:
    \begin{align*}
        L_{\mathcal{D}}(h) \equiv \mathbb{E}_{z \sim \mathcal{D}}[\ell(h,z)]
    \end{align*}
    \item The \textbf{empirical risk} is the average loss over the training set:
    \begin{align*}
        L_S(h) \equiv \frac{1}{m} \sum_{i=1}^m \ell(h, z_i) 
    \end{align*} 
    \item \textbf{Agnostic PAC learnability with generalized loss function}. A hypothesis class $\mathcal{H}$ is agnostic PAC learnable with respect to a domain $Z$ and a loss function $\ell\colon \mathcal{H}\times Z \to \mathbb{R}_+$ if there exist a function $m_{\mathcal{H}}\colon (0,1)^2 \to \mathbb{N}$ and a learning algorithm with the following property: for every $\epsilon,\delta \in (0,1)$, and every distribution $\mathcal{D}$ over $Z$, running the algorithm over $m \geq m_{\mathcal{H}}$ i.i.d. samples generated by $\mathcal{D}$ results in a hypothesis $h \in \mathcal{H}$ that, with probability $1-\delta$, satisfies:
    \begin{align*}
        L_{\mathcal{D}}(h) \leq \min_{h' \in \mathcal{H}} L_{\mathcal{D}}(h') + \epsilon
    \end{align*}
    where $L_{\mathcal{D}}(h) = \mathbb{E}_{z \sim \mathcal{D}}[\ell(h,z)]$.
\end{itemize}



\end{document}
