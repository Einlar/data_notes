%&latex
%
\documentclass[../template.tex]{subfiles}

\begin{document}

\chapter{Exam revision}

\section{Simple Learning Framework}
The simplest learning task is to \textit{classify} features in two classes, in the case where there exists a unique and deterministic map between them. 
\begin{itemize}
    \item \textbf{Domain set} (\textit{Instance space}) $\mathcal{X}$, where elements $\bm{x} \in \mathcal{X}$ are \textit{vectors of features}.
    \item \textbf{Label set} $\mathcal{Y}$
    \item \textbf{Training data}: sequence (order matters, there can be repetitions) of elements in $\mathcal{X}\times \mathcal{Y}$: $S=((x_1 , y_1 ), \dots, (x_m, y_m))$. $m$ denotes the number of training samples.
    \item The ML algorithm $A$, given $S$, outputs a \textbf{hypothesis function} (prediction rule, or \textit{predictor}) $A(S) = h_S\colon \mathcal{X} \to \mathcal{Y}$ that represents the mapping between $\mathcal{X}$ and $\mathcal{Y}$ \textit{learned} by the algorithm.
    \item We assume a \textbf{simple data generation model} for constructing $S$. $\mathcal{X}$ is generated by sampling a distribution (pdf) $\mathcal{D}$ which is \textit{not known} by the learning model, and are then labelled by a function $f\colon \mathcal{X} \to \mathcal{Y}$ (labelling function, representing the \textit{ground truth}).  $\mathcal{D}$ allows to assign probabilities to \textit{events}, i.e. subsets $A \subset \mathcal{X}$. We introduce the following notation:
    \begin{align*}
        \mathcal{D}(A) = \mathbb{P}[x \in A] \qquad A \subset \mathcal{X}
    \end{align*} 
    Equivalently, we can denote subsets $A$ using their \textit{characteristic function} $\pi_A(x) \colon \mathcal{X} \to \{0,1\}$:
    \begin{align*}
        \pi_A(x) = \begin{cases}
            1 & x \in A\\
            0 & x \not\in A
        \end{cases} \qquad A = \{x \in A \colon \pi(x) = 1\}
    \end{align*} 
    And so $\mathcal{D}(A) = \mathbb{P}_{x \sim \mathcal{D}}[\pi_A(x)]$.
    \item We define the \textbf{generalization error} (or \textbf{risk}, \textbf{true error})  of $h_S$ as the probability of randomly choosing a sample from $\mathcal{D}$ so that $h_S(x) \neq f(x)$:
    \begin{align*}
        L_{\mathcal{D},f}(h_S) \equiv \underset{x \sim \mathcal{D}}{\mathbb{P}}[h_S(x) \neq f(x)] \equiv \mathcal{D}(\{x \colon h_S(x) \neq f(x)\})
    \end{align*} 
    Note that $L_{\mathcal{D},f}$ depends on the \textit{data generation} ($\mathcal{D}$, $f$), and so cannot be known by the learner. For simplicity, we denote $L_{\mathcal{D}} \equiv L_{\mathcal{D},f}$.
    \item The \textbf{empirical risk} (or \textbf{training error}) is defined as:
    \begin{align*}
        L_S(h) \equiv \frac{|\{1 \leq i \leq m\colon h(x_i) \neq y_i\}|}{m} 
    \end{align*} 
    (Recall that $|\{\dots \}|$ denotes the cardinality, i.e. the number of elements, of the set $\{\dots\}$).
    \item \textbf{Empirical Risk Minimization} (ERM): the paradigm for choosing $A$ so that it minimizes $L_S(h)$, hoping that this will translate to a minimum $L_{\mathcal{D}}(h)$ as well. Formally, this is written as:
    \begin{align*}
        \mathrm{ERM}(S) \in \arg\min L_S(h)
    \end{align*}  
    Note that there could be \textit{many} possible choiches for $h$ so that $L_S(h)$ is minimum, and the algorithm simply returns one of them. 
    \item \textbf{Overfitting}: when $L_S(h) \sim 0$, but $L_{\mathcal{D}}(h) \gg 0$.  This happens if the ERM search is not limited, meaning that the algorithm can simply \textit{memorize} the training set $S$, leading to a performance on a different dataset which is no better than chance.
    \item \textbf{Hypothesis class}: set of predictors $h \in \mathcal{H}$ that are available for the ERM search (meaning that the final $h_S$ must be $\in \mathcal{H}$). We denote a \textit{constrained } ERM algorithm with $\mathrm{ERM}_{\mathcal{H}}$, so that:
    \begin{align*}
        \mathrm{ERM}_{\mathcal{H}}(S) \in \arg\min_{h\in \mathcal{H}} L_S(h) 
    \end{align*}    
    The choice of $\mathcal{H}$ introduce a \textit{inductive bias} in the learner, and should be made according to some prior knowledge about the problem. 
    \item \textbf{PAC learnability}. A hypothesis class $\mathcal{H}$ is PAC learnable if, given a sufficient number of examples, an ERM algorithm will output an hypothesis which is \textit{probably approximately correct}, meaning that with probability $1-\delta$ it is $\epsilon$-accurate. We require the existence of a sufficient $m$ for any choice of $\epsilon$ and $\delta$, meaning that the model can be made more robust and accurate with more training samples.

    \medskip

    More formally, a hypothesis class $\mathcal{H}$ is PAC learnable if there exist a function (\textbf{sample complexity}) $m_{\mathcal{H}} \colon (0,1) \times (0,1) \to \mathbb{N}$, $(\epsilon,\delta) \mapsto m_{\mathcal{H}}(\epsilon, \delta)$, and a learning algorithm that $\forall \epsilon, \delta \in (0,1)$ and every distribution $\mathcal{D}$ over $\mathcal{X}$, and for every labelling function $f\colon \mathcal{X} \to \{0,1\}$, assuming realizability, when running over $m \geq m_{\mathcal{H}}(\epsilon, \delta)$ i.i.d. samples generated by $\mathcal{D}$ and labelled by $f$, the algorithm returns a hypothesis $h$ such that, with probability $\geq 1-\delta$ (over the choice of the samples) satisfies $L_{\mathcal{D},f} (h) \leq \epsilon$. We also require $m_{\mathcal{H}}$ to be the minimum integer that guarantees the last property.


    

\end{itemize}


\subsection{Theorems}

\subsubsection{PAC for finite hypothesis classes}
The idea is to find a minimum number of samples $m$ that are needed to construct an $S$ so that the model $h_S$ learned by the ERM algorithm will be $\epsilon-$\textit{accurate}, i.e. satisfying:
\begin{align*}
    L_{\mathcal{D},f}(h_S) \leq \epsilon
\end{align*}
with probability $\geq 1-\delta$ (\textit{confidence}) over the choice of an i.i.d. sample $S$ of size $m$ from the $\mathcal{D}$ distribution.
\medskip

\textbf{Hypotheses}
\begin{enumerate}
    \item \textbf{Finite hypothesis class}: $h_S \in \mathcal{H}$, with $|\mathcal{H}| < \infty$.
    \item \textbf{Realizability}. There exists a \q{perfect} $h^* \in \mathcal{H}$, such that $L_{\mathcal{D}, f}(h^*) = 0$. By the definition of \textit{generalization error}, this means that $h^*(x) = f(x)$ with probability $1$ for a random sample $x \in \mathcal{X}$, implying that $L_S(h^*) = 0$.
    \item \textbf{i.i.d. assumption}. The examples in the training dataset $S$ are \textbf{independently} and \textit{identically distributed} according to $\mathcal{D}$, meaning that $S \sim \prod_{i=1}^m \mathcal{D}= \mathcal{D}^m$.    
\end{enumerate}
 

\textbf{Proof}. $\delta$ is the maximum probability of the learner failing, that is of not being $\epsilon$-accurate:
\begin{align*}
     \mathcal{D}^m(\{S \colon L_{\mathcal{D},f} (h_S) > \epsilon \}) \leq \delta
\end{align*}
The learner fails only for certain \q{bad hypotheses}, that are inside a set $\mathcal{H}_B$:
\begin{align*}
    \mathcal{H}_B = \{h \in \mathcal{H}\colon L_{\mathcal{D},f}(h) > \epsilon\}
\end{align*}
The $\mathrm{ERM}_{\mathcal{H}}$ algorithm will choose an $h_S$ that verifies $L_S(h_S) = 0$, due to realizability (generally $h_S \neq h^*$, because there could be many $h_S$ with a $0$ training error). So, it may choose a \textit{bad hypothesis} only if it \q{appears good} on $S$, that is if $S$ is an element of the set of \textit{misleading training sets}: 
\begin{align*}
    M = \{S \colon \exists h \in \mathcal{H}_B, L_S(h) = 0 \}
\end{align*}   
Note that any set $S$ that results in the learner failing must be in $M$ (because it contains all sets that \textit{may be chosen} and result in failing), and so:
\begin{align*}
    \{S\colon L_{\mathcal{D},f}(h_S) > \epsilon\} \subseteq M
\end{align*}
The converse is not necessarily true: maybe there are certain sets in $M$ that, even if misleading, result nonetheless in a good performing $h_S$, because they are compatible with more than one hypothesis.

We can now rewrite $M$ as the union of misleading sets:
\begin{align}
    \{S\colon L_{\mathcal{D},f}(h_S) > \epsilon\} \subseteq M = \bigcup_{h \in \mathcal{H}_B} \{S \colon L_S(h) = 0 \}
    \label{eqn:set_inequality1}
\end{align}
Applying the probability measure to (\ref{eqn:eqn:set_inequality1}) leads to:
\begin{align}
    \mathcal{D}^m(\{S\colon L_{\mathcal{D},f}(h_S) > \epsilon\}) \leq \mathcal{D}^m(M) = \mathcal{D}^m \left( \bigcup_{h \in \mathcal{H}_B} \{S \colon L_S(h) = 0 \}\right)
    \label{eqn:inequality1}
\end{align}
Recall the \textbf{union bound}, that is the measure of the union of two sets is less or equal to the sum of the measure of each set (because there could be a non-empty intersection):
\begin{align*}
    \mathcal{D}(A \cup B) \leq \mathcal{D}(A) + \mathcal{D}(B)
\end{align*} 
Applying it to the right hand side of (\ref{eqn:inequality1}) we have:
\begin{align*}
     \mathcal{D}^m(\{S\colon L_{\mathcal{D},f}(h_S) > \epsilon\}) \leq \sum_{h \in \mathcal{H}_B} \mathcal{D}^m (\{S \colon L_S(h) = 0\})
\end{align*}
Fix a certain \textit{bad} hypothesis $h \in \mathcal{H}_B$. The event $L_S(h) = 0$ is equivalent to $\forall i $ $h(x_i) = f(x_i)$, and so:
\begin{align*}
    \mathcal{D}^m(\{S \colon L_S(h) = 0\}) &= \mathcal{D}^m (\{S \colon \forall i, h(x_i) = f(x_i)\})
\intertext{which is just the product of probabilities that each $h(x_i) = f(x_i)$, as $x_i$ are i.i.d:}
    &= \prod_{i=1}^m \mathcal{D}(\{x_i \colon h(x_i) = f(x_i)\})
\end{align*}
The probability of a good prediction is $1-$ the probability of error, which is $L_{\mathcal{D},f}$. As $h \in \mathcal{H}_B$, $L_{\mathcal{D},f} > \epsilon$, and so:
\begin{align*}
    \mathcal{D}(\{x_i \colon h(x_i) = y_i\}) = 1- L_{\mathcal{D},f} (h) \leq 1 - \epsilon
\end{align*} 
Substituting back leads to:
\begin{align*}
    \mathcal{D}^m (\{S \colon L_S(h) = 0\})\leq (1-\epsilon)^m \leq e^{- \epsilon m}
\end{align*}
And finally:
\begin{align*}
     \textcolor{Red}{\mathcal{D}^m(\{S\colon L_{\mathcal{D},f}(h_S) > \epsilon\})} \leq \sum_{h \in \mathcal{H}_B} \mathcal{D}^m (\{S \colon L_S(h) = 0\}) \leq |\mathcal{H}_B| e^{-\epsilon m} \leq |\mathcal{H}| e^{-\epsilon m}
\end{align*}
We now want to choose $m$ so that the red term is $\leq \delta$. So we impose:
\begin{align*}
    |\mathcal{H}|e^{-\epsilon m} \leq \delta \Rightarrow 
    m \geq \frac{\log(|\mathcal{H}|/\delta)}{\epsilon} 
\end{align*}

This means that \textit{every finite hypothesis class is PAC learnable with sample complexity}:
\begin{align*}
    m_{\mathcal{H}}(\epsilon, \delta) \leq \frac{\log(|\mathcal{H}|/\delta)}{\epsilon} 
\end{align*}  

\section{Agnostic Simple Learning}
We consider now a binary classification task where features are not sufficient to uniquely predict labels. In other words, a couple of exactly equal features can lead to opposite labels.

\begin{itemize}
    \item \textbf{Data generation}. Consider a distribution $\mathcal{D}$ over $\mathcal{X}\times \mathcal{Y}$, with $\mathcal{Y}= \{0,1\}$. The probability of sampling a certain datapoint $(x, y)$ is given by:
    \begin{align*}
    \mathbb{P}(x,y) = \mathbb{P}(x) \mathbb{P}(y|x)    
    \end{align*}
    For example $x=4$ can be associated to $y=0$ in $75\%$ of the cases, and to $y=1$ in the other $25\%$. In such a case realizability cannot hold, as the learner is deterministic, and must choose one output or the other, meaning that it is bound to make a certain error.
    \item \textbf{True error}. The risk of a predictor $h$ is given by the probability of sampling $(x,y) \sim \mathcal{D}$ so that $h(x) \neq y$:
    \begin{align*}
        L_{\mathcal{D}}(h) \equiv \underset{(x,y) \sim \mathcal{D}}{\mathbb{P}} [h(x) \neq y] \equiv \mathcal{D}(\{(x,y) \colon h(x) \neq y\})
    \end{align*} 
    \item \textbf{Empirical risk} maintains the same formula as before:
    \begin{align*}
    L_S(h) \equiv \frac{|\{1 \leq 1 \leq m\colon h(x_i) \neq y_i\}|}{m}     
    \end{align*} 
    \item \textbf{Bayes Optimal Predictor} Given any probability distribution $\mathcal{D}$ over $\mathcal{X}\times \{0,1\}$, the best label predicting function $\mathcal{X} \to \{0,1\}$ is:
    \begin{align*}
        f_{\mathcal{D}}(x) = \begin{cases}
            1 & \mathbb{P}[y=1|x] \geq 1/2\\
            0 & \text{otherwise}
        \end{cases}
    \end{align*} 
    That is, predict $1$ if the probability of $x$ being $y=1$ is greater than chance ($1/2$), and $0$ otherwise.
    \item \textbf{Agnostic PAC learnability}. A hypothesis class $\mathcal{H}$ is agnostic PAC learnable if there exists a function $m_{\mathcal{H}}\colon (0,1)^2 \to \mathbb{N}$ and a learning algorithm with the following property: for every $\epsilon, \delta \in (0,1)$ and every distribution $\mathcal{D}$ over $\mathcal{X}\times \mathcal{Y}$, when running the algorithm on $m \geq m_{\mathcal{H}}(\epsilon, \delta)$ i.i.d. samples generated by $\mathcal{D}$, the algorithm returns an hypothesis $h$ such that, with probability $\geq 1 -\delta$ it satisfies:
    \begin{align*}
        L_{\mathcal{D}}(h) \leq \min_{h' \in \mathcal{H}} L_{\mathcal{D}} (h') + \epsilon
    \end{align*} 

\end{itemize}

\section{Agnostic PAC learnable}
Finally, we generalize the previous case to a \textit{general} learning task, i.e. classification with a larger number of classes, or regression.

\begin{itemize}
    \item \textbf{Generalized Loss Functions}. A loss function $\ell \colon \mathcal{H}\times Z \to \mathbb{R}_+$, with $Z$ being a certain domain, is a function that \textit{evaluates} the performance of a model $h \in \mathcal{H}$ on the domain $Z$.
    \item \textbf{Risk function} is defined just as the expected value of the loss:
    \begin{align*}
        L_{\mathcal{D}}(h) \equiv \mathbb{E}_{z \sim \mathcal{D}}[\ell(h,z)]
    \end{align*}
    \item The \textbf{empirical risk} is the average loss over the training set:
    \begin{align*}
        L_S(h) \equiv \frac{1}{m} \sum_{i=1}^m \ell(h, z_i) 
    \end{align*} 
    \item \textbf{Agnostic PAC learnability with generalized loss function}. A hypothesis class $\mathcal{H}$ is agnostic PAC learnable with respect to a domain $Z$ and a loss function $\ell\colon \mathcal{H}\times Z \to \mathbb{R}_+$ if there exist a function $m_{\mathcal{H}}\colon (0,1)^2 \to \mathbb{N}$ and a learning algorithm with the following property: for every $\epsilon,\delta \in (0,1)$, and every distribution $\mathcal{D}$ over $Z$, running the algorithm over $m \geq m_{\mathcal{H}}$ i.i.d. samples $S$ generated by $\mathcal{D}$ results in a hypothesis $h_S \in \mathcal{H}$ that, with probability $1-\delta$, satisfies:
    \begin{align*}
        L_{\mathcal{D}}(h_S) \leq \min_{h' \in \mathcal{H}} L_{\mathcal{D}}(h') + \epsilon
    \end{align*}
    where $L_{\mathcal{D}}(h_S) = \mathbb{E}_{z \sim \mathcal{D}}[\ell(h_S,z)]$.
\end{itemize}

\section{Uniform Convergence}
ERM algorithm minimizes the empirical risk, hoping that this will result in a low true risk as well. So, we want to prove which conditions are needed so that $L_S$ is close to $L_{\mathcal{D}}$.

\begin{itemize}
    \item \textbf{$\epsilon$-representative}: A training set $S$ is called $\epsilon$-representative (with respect to a distribution $\mathcal{D}$ over a domain $Z$, a hypothesis class $\mathcal{H}$, and a loss $\ell$) if:
    \begin{align*}
        \forall h \in \mathcal{H}, \quad |L_S(h) - L_{\mathcal{D}}(h)| \leq \epsilon
    \end{align*} 
    \item \textbf{Uniform convergence}: A hypothesis class $\mathcal{H}$ has the \textit{uniform convergence property} (with respect to a domain $Z$ and a loss function $\ell$) if there exists a function $m_{\mathcal{H}}^{\mathrm{UC}}\colon (0,1)^2 \to \mathbb{N}$ such that for every $\epsilon$, $\delta$ $\in (0,1)$ and for every probability distribution $\mathcal{D}$ over $Z$, if $S$ is a sample of at least $m \geq m_{\mathcal{H}}^{\mathrm{UC}}(\epsilon, \delta)$ examples drawn i.i.d. from $\mathcal{D}$, then, with probability of at least $1-\delta$, $S$ is $\epsilon$-representative.    
\end{itemize}

\subsection{Theorems}
\begin{itemize}
    \item \textbf{Learned hypotheses on representative sample}. If a training set $S$ is $\epsilon/2$ representative, then \textbf{any} output of the learner $\mathrm{ERM}_{\mathcal{H}}(S)$, i.e. any:
    \begin{align*}
        h_S \in \arg\min_{h \in \mathcal{H}}L_S(h)
    \end{align*}
    satisfies:
    \begin{align*}
        L_{\mathcal{D}}(h_S) \leq \min_{h \in \mathcal{H}} L_{\mathcal{D}}(h) + \epsilon
    \end{align*}
    \textbf{Proof}. By the definition of $\epsilon/2$-representative we have:
    \begin{align} \label{eqn:e-inequality}
        -\frac{\epsilon}{2} \leq L_{S}(h) - L_{\mathcal{D}}(h) \leq \frac{\epsilon}{2} 
    \end{align}
    Rearranging to isolate $L_{\mathcal{D}}(h)$:
    \begin{align*}
        -\frac{\epsilon}{2} - L_S(h) \leq -L_{\mathcal{D}}(h) \leq \frac{\epsilon}{2} - L_{S}(h) \Rightarrow \frac{\epsilon}{2} + L_S(h) \geq L_{\mathcal{D}}(h) \geq -\frac{\epsilon}{2} + L_S(h)     
    \end{align*}
    This inequality holds for any $h\in \mathcal{H}$, and in particular for the $h_S$ outputted by the learner:
    \begin{align*}
        L_{\mathcal{D}}(h_S) \leq L_S(h_S) + \frac{\epsilon}{2} 
    \end{align*}
    $h_S$ is a minimum point for $L_S$ (by the definition of ERM algorithm) and so $L_S(h_S) \leq L_S(h)$, leading to:
    \begin{align} \label{eqn:ineq}
        L_{\mathcal{D}}(h_S) \leq L_S(h_S) + \frac{\epsilon}{2} \leq L_S(h) + \frac{\epsilon}{2} 
    \end{align}
    We can now reuse (\ref{eqn:e-inequality}) to get an inequality for $L_S(h)$:
    \begin{align*}
        L_S(h) \leq L_{\mathcal{D}}(h) + \frac{\epsilon}{2} 
    \end{align*}
    That can be applied in (\ref{eqn:ineq}) completing the chain:
    \begin{align*}
        L_{\mathcal{D}}(h_S) \leq L_S(h_S) + \frac{\epsilon}{2} \leq L_S(h) + \frac{\epsilon}{2} \leq L_{\mathcal{D}}(h) + \frac{\epsilon}{2} + \frac{\epsilon}{2} = L_{\mathcal{D}}(h) + \epsilon  
    \end{align*}
    This holds for any $h \in \mathcal{H}$, and so we can pick the \textit{minimum} $L_{\mathcal{D}}(h)$ to get the stronger condition:
    \begin{align*}
        L_{\mathcal{D}}(h_S) \leq \min_{h \in \mathcal{H}} L_{\mathcal{D}}(h) + \epsilon
    \end{align*}  
    In other words, if $S$ is $\epsilon$-representative, then the true risk and the empirical risk are close to each other.

    \item \textbf{Uniform convergence implies PAC learnability}. If a class $\mathcal{H}$ has the uniform convergence property with a sample complexity $m_{\mathcal{H}}^{\mathrm{UC}}$ then the class is agnostically PAC learnable with the sample complexity $m_{\mathcal{H}}(\epsilon, \delta) \leq m_{\mathcal{H}}^{\mathrm{UC}}(\epsilon/2, \delta)$.
    
    \medskip

    \textbf{Proof}. Thanks to uniform convergence, we are certain that we can use the ERM algorithm to get \textit{nice} results, meaning empirical risks that are close to true risks, and so, using the previous theorem, we reach the PAC learning condition.

    \item \textbf{Finite Classes are agnostic PAC learnable}. 
    We want to prove that uniform convergence holds for finite classes, implying that they are agnostic PAC learnable.

    Fix some accuracy $\epsilon$ and confidence $\delta$. We need to find a sufficient $m = |S|$ so that:
    \begin{align*}
        \mathcal{D}^m(\{S \colon \forall h \in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h)| \leq \epsilon \}) \geq 1-\delta
    \end{align*}
    That is, if we pick $m$ samples from $\mathcal{D}$ to generate $S$, then with probability $1-\delta$, uniformly for \textit{any} hypothesis $h \in \mathcal{H}$, the empirical risk and true risk are $\epsilon$-close: $L_S(h) - L_{\mathcal{D}}(h)| \leq \epsilon$.
    
    If the probability of an event $p$ is $\geq 1-\delta$, then the probability of the opposite event $\bar{p}$ is $< 1-(1-\delta) = \delta$:
    \begin{align*}
        \mathcal{D}^m(\{S \colon \exists h \in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon \}) < \delta
    \end{align*} 
    (Note how $\forall \to \exists$).

    Consider now the argument of $\mathcal{D}^m$. This is the set of all samples sets $S$ for which there is an hypothesis that satisfies some property. Equivalently, we could select all samples with that property on a \textit{fixed} $h$, do the same for every other $h$, and join all the results: 
    \begin{align*}
        \{S \colon \exists h \in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon\} = \bigcup_{h \in \mathcal{H}} \{S \colon |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon \}
    \end{align*} 
    Applying $\mathcal{D}^m$ to both sides along with the union bound leads to:
    \begin{align*}
        \mathcal{D}^m(\{S \colon \exists h \in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon \}) \leq \sum_{h \in \mathcal{H}} \mathcal{D}^m (\{S\colon |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon\})
    \end{align*}
    We now consider a generic element of the sum, and search for a upper bound. The idea is that $L_S(h)$ is an \textit{estimator} of $L_{\mathcal{D}}(h)$ (it is a sample mean vs an expected value), and so it should be \textit{closer to it} if we increase the sample size $m$. In fact:
    \begin{align*}
        L_{\mathcal{D}}(h) = \mathbb{E}_{z \sim \mathcal{D}}[\ell(h, z)] \qquad L_S(h) = \frac{1}{m} \sum_{i=1}^m \ell(h, z_i) 
    \end{align*}   
    meaning that $L_{\mathcal{D}}(h) = \mathbb{E}_{z \sim \mathcal{D}}[L_S(h)]$. The gap between averages (over i.i.d. samples) and their expected value can be quantified through \textbf{Hoeffding inequality}.
    
    \medskip

    \textbf{Hoeffding inequality}. Let $\theta_1, \dots, \theta_m$ be a sequence of i.i.d. random variables and assume that for all $i$, $\mathbb{E}[\theta_i] = \mu$ and $\mathbb{P}[a \leq \theta_i \leq b] =1$ (i.e. the support of the probability distribution lies in $[a,b]$). Then, for any $\epsilon > 0$:
    \begin{align*}
        \mathbb{P} \left[\left| \frac{1}{m} \sum_{i=1}^m \theta_i - \mu  \right | > \epsilon\right] \leq 2 \exp\left(-\frac{2 m \epsilon^2}{(b-a)^2} \right)
    \end{align*}
    
    \medskip

    In our case, this means that:
    \begin{align*}
        \mathcal{D}^m(\{S\colon |L_S(h) - L_{\mathcal{D}}(h)|> \epsilon\}) \leq 2 \exp\left(-\frac{2m \epsilon^2}{(b-a)^2} \right)
    \end{align*}
    We assume $\ell \in [0,1]$, meaning that $b-a=1$. Then, computing the sum:
    \begin{align*}
        \textcolor{Red}{\mathcal{D}^m(\{S \colon \exists h \in \mathcal{H}, |L_S(h) - L_{\mathcal{D}}(h)| > \epsilon \})} \leq \sum_{h \in \mathcal{H}} 2 \exp(-2 m \epsilon^2) = 2 |\mathcal{H}| \exp(-2 m \epsilon^2)
    \end{align*}
    For our thesis, we want the red term to be $< \delta$, and so we impose:
    \begin{align*}
        2 |\mathcal{H}| \exp(-2 m \epsilon^2) < \delta \Rightarrow m \geq \frac{1}{2 \epsilon^2} \log\left(\frac{2|\mathcal{H}|}{\delta} \right) 
    \end{align*}
    
    So, let $\mathcal{H}$ be a finite hypothesis class, let $Z$ be a domain, and let $\ell \colon \mathcal{H}\times Z \to [0,1]$ be a loss function. Then $\mathcal{H}$ enjoys the uniform convergence property with sample complexity:
    \begin{align*}
        m_{\mathcal{H}}^{\mathrm{UC}}(\epsilon, \delta) \leq \left \lceil \frac{1}{2 \epsilon^2} \log(\frac{2|\mathcal{H}|}{\delta} )\right \rceil
    \end{align*}
    (Note that previously we found a \textit{general} sufficient condition for the minimum size of $m$ needed for uniform convergence. In a specific case, it is probable that a much lower $m$ would allow the same property.)

\end{itemize}

\section{Bias-complexity tradeoff}
\begin{itemize}
    \item \textbf{Error decomposition}. The true risk $L_{\mathcal{D}}(h_S)$ of an output $h_S$ of the $\mathrm{ERM}_{\mathcal{H}}$ algorithm is a combination of the \textit{bias} contained in the choice of $\mathcal{H}$ (which could be not optimal) - the \textbf{approximation error} - and the inefficiency of the ERM algorithm in minimizing the true risk (as it minimizes the empirical risk) - the \textbf{estimation error}:
    \begin{align*}
        L_{\mathcal{D}}(h_S) = \epsilon_{\mathrm{approximation}} + \epsilon_{\mathrm{estimation}} \span \\
        \epsilon_{\mathrm{app}} = \min_{h \in \mathcal{H}} L_{\mathcal{D}}(h); \qquad \epsilon_{\mathrm{est}} = L_{\mathcal{D}} (h_S) - \epsilon_{\mathrm{app}}
    \end{align*}
    \begin{itemize}
        \item $\epsilon_{\mathrm{app}}$ usually decreases when enlarging $|\mathcal{H}|$, and does not depend on $m$. If realizability holds, it is $0$, otherwise it includes always the error of the Bayes optimal predictor.
        \item $\epsilon_{\mathrm{est}}$ increases logarithmically with $|\mathcal{H}|$ (because it is easier to overfit) and decreases with $m$.
    \end{itemize}    
    $|\mathcal{H}|$ too large makes $\epsilon_{\mathrm{est}}$ dominate (overfitting), while $|\mathcal{H}|$ too small makes $\epsilon_{\mathrm{app}}$ dominate (underfitting). 
\end{itemize}

\subsection{Theorems}
\begin{itemize}
    \item \textbf{No free lunch}. Let $A$ be any learning algorithm for the task of binary classification with respect to the $0-1$ loss over a domain $\mathcal{X}$. Let $m$ be any number smaller than $|\mathcal{X}|/2$, representing a training set size (so the algorithm knows \textit{less} than half the elements). Then, there exists a distribution $\mathcal{D}$ over $\mathcal{X}\times \{0,1\}$ such that: 
    \begin{enumerate}
        \item There exists a function $f \colon \mathcal{X}-> \{0,1\}$ with $L_{\mathcal{D}} (f) = 0$
        \item With probability of at least $1/7$ over the choice of $S \sim \mathcal{D}^m$ we have that $L_{\mathcal{D}}(A(S)) \geq 1/8$.  
    \end{enumerate}
    \item \textbf{No prior knowledge implies not PAC learnable}. Let $\mathcal{X}$ be an \textit{infinite} domain set and let $\mathcal{H}$ be the set of \textbf{all} functions $\mathcal{X} \to \{0,1\}$ (no bias, no prior knowledge). Then $\mathcal{H}$ is not PAC learnable.  
    \medskip

    \textbf{Proof}. Just pick $\epsilon < 1/8$ and $\delta < 1/7$, suppose (by absurdity) that $\mathcal{H}$ is PAC learnable, and apply the no-free-lunch theorem to get a contradiction (no amount of $m$, as $|\mathcal{H}| = \infty$ is $> |\mathcal{H}|/2$, and so with probability $> \delta > 1/7$, $L_{\mathcal{D}}(A(S)) > 1/8 > \epsilon$). 
\end{itemize}

\section{VC-Dimension}

\begin{itemize}
    \item \textbf{Restriction of a hypothesis class to a set of samples}. Let $\mathcal{H}$ be a class of functions $\mathcal{X} \to \{0,1\}$ and let $C = \{c_1, \dots, c_m\} \subset \mathcal{X}$. The restriction of $\mathcal{H}$ to $C$ is the set of functions $C \to \{0,1\}$ that can be derived from $\mathcal{H}$. 
    
    We can identify these functions by their outcomes on the elements of $C$:
    \begin{align*}
        \mathcal{H}_C = \{(h(c_1), \dots, h(c_m)) \colon h \in \mathcal{H}\}
    \end{align*}
    Note that if two $h$ result in the same outcomes on $C$, then they are effectively \textit{the same function} on the restricted class. So $\mathcal{H}_C$ is the \textit{intersection} between the set of all functions $C \to \{0,1\}$ and $\mathcal{H}^{|C|}$. 

    \item \textbf{Shattering}. A hypothesis class $\mathcal{H}$ shatters a finite set $C \subset \mathcal{X}$ if the restriction of $\mathcal{H}$ to $C$ is the set of all functions $C \to \{0,1\}$. That is, $|\mathcal{H}_C| = 2^{|C|}$.
    
    \medskip

    \textbf{Example}. Let $\mathcal{H}$ be the hypothesis class of threshold functions $h_a \colon \mathbb{R} \to \{0,1\}$:
    \begin{align*}
        \mathcal{H}\ni h_a (x) = \begin{cases}
            1 & x > a\\
            0 & x \leq a
        \end{cases}
    \end{align*} 
    Consider $C = \{c_1\}$. If we take $a = c_1 - 1$, then $h_a(c_1) = 1$, but for $b = c_1 +1$ we have $h_b(c_1) = 0$. As the set of all functions $C \to \{0,1\}$ is just $\{0,1\}$, this means that $\mathcal{H}$ shatters $C$.

    However, consider $D = \{c_1,c_2\}$ with $c_1 < c_2$. Here the set of all functions $D \to \{0,1\}$ is $\{(0,0), (0,1), (1,0), (1,1)\}$. By correctly choosing $a$, we can construct only $\{0,0\}$, $\{1,1\}$ or $\{0,1\}$, meaning that $\mathcal{H}$ does not shatter $D$.

\end{itemize}

\subsection{Theorems}
\begin{itemize}
    \item \textbf{Shattering and No-Free-Lunch}. Let $\mathcal{H}$ be a hypothesis class of functions $\mathcal{X} \to\{0,1\}$. Let $m$ be a training set size. Assume that there exists a set $C \subset \mathcal{X}$ of double the size ($|C| = 2m$) that is shattered by $\mathcal{H}$. Then, for \textit{any} learning algorithm $A$, there exist a distribution $\mathcal{D}$ over $\mathcal{X}\times \{0,1\}$ and a predictor $h \in \mathcal{H}$ such that $L_{\mathcal{D}}(h) = 0$ but with probability of at least $1/7$ over the choice of $S \sim \mathcal{D}^m$ we have that $L_{\mathcal{D}}(A(S)) \geq 1/8$.   
\end{itemize}

%Finish

\section{Linear predictors}

\begin{itemize}
    \item \textbf{Affine Functions}
    \begin{align*}
        L_d= \{h_{\bm{w},b} \colon \bm{w} \in \mathbb{R}^d, b \in \mathbb{R}\} \qquad h_{\bm{w},b}(\bm{x}) = \langle \bm{w}, \bm{x} \rangle + b = \left(\sum_{i=1}^d w_i x_i\right) + b_i
    \end{align*} 
    An affine function returns the dot product of the argument $\bm{x}$ by a \textit{weight vector} $\bm{w}$, and sums a \textit{bias} $b$ to the result. These can be written in a more compact way by defining $\bm{w}' = (b, \bm{w}) \in \mathbb{R}^{d+1}$ and $\bm{x}' = (1,\bm{x})$, so that:
    \begin{align*}
        h_{\bm{w},b} = \langle \bm{w}', \bm{x}' \rangle = \langle \bm{w}, \bm{x} \rangle + b
    \end{align*}
    So an affine function of dimension $d$ can be written as a linear function in dimension $d+1$ (homogeneous coordinates).
    \item \textbf{Linear functions}.
    Just a dot product with a weight $\bm{w}$ vector:
    \begin{align*}
        L_d '= \{h_{\bm{w}} \colon \bm{w} \in \mathbb{R}^d \} \qquad h_{\bm{w}}(\bm{x}) = \langle \bm{w}, \bm{x} \rangle
    \end{align*}
    \item \textbf{Linear predictor}. The hypothesis class $\mathcal{H}$ of linear predictors contains all the functions $\Phi \circ L_d$, where $\Phi \colon \mathbb{R} \to \mathcal{Y}$ \q{connects} the output of the affine functions in $L_d$ to the desired set of labels $\mathcal{Y}$.
    \begin{itemize}
        \item \textbf{Halfspaces}. If $\mathcal{Y} = \{-1,+1\}$ (binary classification), and $\mathcal{X}=\mathbb{R}^d$, we define the class of halfspaces as follows:
        \begin{align*}
            HS_d = \operatorname{sign} \circ L_d = \{\bm{x} \mapsto \operatorname{sign}(h_{\bm{w},b}(\bm{x})) \colon h_{\bm{w},b} \in L_d \}
        \end{align*}  
        That is, we classify an element $\bm{x} \in \mathcal{X}$ as $+1$ if $h_{\bm{w},b}(\bm{x}) > 0$, and to $-1$ otherwise.
        Geometrically, $\bm{w} \cdot \bm{x} + b = 0$ is the hyperplane in $\mathbb{R}^d$ that is $\perp \bm{w}$, and is $b/\norm{\bm{w}}$ away from the origin. Consider $\bm{w}$ as an affine vector \q{starting} from $\bm{w} b/\norm{\bm{w}}^2$. Then any point that lies at an acute angle with respect to $\bm{w}$ is classified as $+1$, and all the others as $-1$.
        
        \begin{expl}
            The distance between a point $\bm{x_0}$ and an hyperplane $\bm{w} \cdot \bm{x} + b = 0$ can be computed by considering the vector $\bm{x_0} - \bm{X}$, where $\bm{X}$ is a generic point on the hyperplane (i.e. such that $\bm{X} \cdot \bm{w} + b= 0$), and projecting it on the perpendicular unit vector $\bm{w}/\norm{\bm{w}}$, leading to:
            \begin{align*}
                \operatorname{dist} = \left|(\bm{x_0} - \bm{X}) \cdot \frac{\bm{w}}{\norm{\bm{w}}}\right|
            \end{align*}
            Then using $\bm{X} \cdot \bm{w} = -b$ (as it is in the hyperplane), leads to:
            \begin{align} \label{eqn:hyperplane-dist}
                \operatorname{dist} = \frac{|\bm{x}_0 \cdot \bm{w} + b|}{\norm{\bm{w}}}  
            \end{align}
        \end{expl}
    The \textbf{realizability} assumption holds if the points to be classified are \textit{linearly separable}, meaning that there exist an halfspace that perfectly separates the ones labelled as $+1$ from the other ones.
    
    \end{itemize}
    \item \textbf{Linear regression}.  Take $\mathcal{H}_{\mathrm{reg}} = L_d$, with $L_2$ loss $\ell (h, (\bm{x}, y)) = (h(\bm{x}) - y)^2$. This leads to the Mean Squared Error as empirical risk:
    \begin{align*}
        L_S(h) = \frac{1}{m} \sum_{i=1}^m (h(\bm{x_i})- y_i)^2 
    \end{align*} 
    \item \textbf{Polynomial regression}. Find the one dimensional polynomial of degree $n$ that better predicts the data. $p(x) = a_0 + a_1 x + a_2 x^2 + \dots + a_nx^n$. The idea is to \textit{produce nonlinear features} with the mapping $\psi\colon \mathbb{R} \to \mathbb{R}^{n+1}$, $x \mapsto (1,x,x^2,\dots,x^n) = \bm{\psi}(x)$, and then note that $p(x) = \langle \bm{a}, \bm{\psi}(x) \rangle$, and find $\bm{a}$ with least squares.
    \item \textbf{Logistic regression}. Choose $\mathcal{H}\colon \Phi_
    \mathrm{sig} \circ L_d$, where $\Phi_{\mathrm{sig}}\colon \mathbb{R} \to [0,1]$ is the sigmoid function:
    \begin{align*}
        \Phi_{\mathrm{sig}}(z) = \frac{1}{1 + e^{-z}} 
    \end{align*}  
    Output is probability of being in class $+1$.\\
    Use loss: $\ell(h_{\bm{w}}, (\bm{x}, y)) = \log(1+ e^{-y \langle \bm{w}, \bm{x} \rangle})$. Note that $y \langle \bm{w}, \bm{x} \rangle > 0$ for correctly classified samples, meaning that in this case the loss is low.
\end{itemize}

\subsection{Theorems}

\begin{itemize}
    \item \textbf{ERM for halfspaces is a linear program}. 
A linear program is a way to maximize a linear function subject to linear inequalities, that is finding $\bm{w}$ such that:
\begin{align*}
    \max_{\bm{w} \in \mathbb{R}^d} \langle \bm{u}, \bm{w} \rangle \> \land  \> A \bm{w} \geq \bm{v}
\end{align*}
with $A$ is a $m\times d$ matrix and $\bm{v}\in \mathbb{R}^m$, $\bm{u} \in \mathbb{R}^d$ are vectors.
    
Assume a \textbf{realizable}, \textbf{homogeneous} case, meaning that we take $b = 0$ (hypotheses are from $L_d'$, or from $L_{d-1}$ with homogeneous coordinates). Let $S = \{(\bm{x_i}, y_i)\}_{i=1}^m$ be the training dataset. From realizability, $L_S(h_S) = 0$. 

We search for a $\bm{w} \in \mathbb{R}^d$ so that:
\begin{align*}
    \operatorname{sign}(\langle \bm{w}, \bm{x_i} \rangle) = y_i \qquad \forall i = 1, \dots, m
\end{align*}
This is equivalent to:
\begin{align*}
    y_i \langle \bm{w}, \bm{x_i} \rangle > 0 \qquad \forall i = 1, \dots, m
\end{align*}
In fact, if $y_i = +1$ then a correct $\bm{w}$ will lead to $\langle \bm{w}, \bm{x_i} \rangle > 0$, and so $y_i \langle \bm{w}, \bm{x_i} \rangle > 0$. On the other hand, if $y_i = -1$, then we will have $\langle \bm{w}, \bm{x_i} \rangle < 0$, and so again $y_i \langle \bm{w}, \bm{x_i} \rangle > 0$.

We now show that there exist a $\bar{\bm{w}}$ so that $y_i \langle \bm{w}, \bm{x_i} \rangle \geq 1$, and that this inequality can be mapped to the LP case.

We construct $\bar{\bm{w}}$ starting from any $\bm{w^*}$ for which $y_i \langle \bm{w^*}, \bm{x_i} \rangle$ (which exists due to realizability). Then consider the minimum value of that expression:
\begin{align*}
    \gamma \equiv \min_{1 \leq i \leq m} y_i \langle \bm{w^*}, \bm{x_i} \rangle
\end{align*}
Then:
\begin{align*}
    y_i \langle \bm{w^*}, \bm{x_i} \rangle \geq \gamma  \Rightarrow y_i \langle \frac{\bm{w^*}}{\gamma}, \bm{x_i}  \rangle \geq 1
\end{align*}
and so $\bar{\bm{w}} = \bm{w^*}/\gamma$. We can now rewrite the previous inequality in vector form:
\begin{align*}
    y_i \langle \bar{\bm{w}}, \bm{x_i} \rangle \geq 1 \Rightarrow y_i \sum_{j=1}^d w_j (\bm{x_i})_j = \sum_{j=1}^d y_i (\bm{x}_i)_j w_j \geq 1 \Rightarrow X \bm{w} \geq \bm{1} 
\end{align*}
where $X$ is the $m\times d$ matrix with $X_{ij} = y_i (\bm{x_i})_j$ ($(\bm{x_i})_j$ is the $j$-th component of the $i$-th sample $\bm{x_i}$). So, if we take $\bm{v} = (1,\dots,1)\in \mathbb{R}^d$ we got the linear inequality needed for the linear program. As any $\bm{w}$ that satisfy that constraint is a valid solution, we do not need to maximize any function, and so we can set $\bm{u} = \bm{0} \in \mathbb{R}^d$.

\item \textbf{Perceptron algorithm for halfspaces}. Let $S=\{(\bm{x_i}, y_i)\}_{i=1,\dots,m}$. Initialize $\bm{w}^{(1)} = \bm{0}$.
At every step $t$, pick a misclassified sample, that is a $i$ such that $y_i \langle \bm{w}^{(t)}, \bm{x_i} \rangle \leq 0$. Then update:
\begin{align*}
    \bm{w}^{(t+1)} = \bm{w}^{(t)} + y_i \bm{x_i}
\end{align*} 
Note that:
\begin{align*}
    y_i \langle \bm{w}^{(t+1)}, \bm{x_i} \rangle = y_i \langle  \bm{w}^{(t)} + y_i \bm{x_i}, \bm{x_i} \rangle = \underbrace{y_i \langle  \bm{w}^{(t)}, \bm{x_i} \rangle}_{\leq 0} + \norm{\bm{x_i}}^2
\end{align*}
So now it is \textit{closer} to being $>0$ (correct). The algorithm stops when there are no more misclassified samples (in the realizable case).

\textbf{Stopping condition}. Let $R = \max_i \norm{\bm{x_i}}$ (maximum norm of samples), and $B=\min\{\norm{\bm{w}} \colon \forall i \in [m], y_i \langle \bm{w}, \bm{x_i} \rangle \geq 1\}$ (minimum norm of a correct weight). Then the algorithm stops after at most $(RB)^2$ iterations.
\medskip

\textbf{Proof}. We show that if $T$ is the minimum number of iterations needed to reach the stopping condition, then $T \leq (RB)^2$. Let $\bm{w^*}$ be the solution with the smaller norm, meaning that:
\begin{align} \label{eqn:stop-condition}
    y_i \langle \bm{w^*}, \bm{x_i} \rangle \geq 1 \qquad \norm{\bm{w^*}} = B
\end{align}
We expect $\bm{w}^{(t)}$ to be pointing closer and closer to the direction of $\bm{w^*}$ as $t$ increases. Recall that the cosine of the angle $\alpha$ between two vectors $\bm{a}$ and $\bm{b}$ is:
\begin{align*}
    1 \geq \cos(\alpha) =\frac{\langle \bm{a}, \bm{b} \rangle}{\norm{\bm{a}}\norm{\bm{b}}}
\end{align*}
In our case we examine:
\begin{align*}
    1 \geq\frac{\langle \bm{w^*}, \bm{w}^{(t)} \rangle}{\norm{\bm{w^*}} \norm{\bm{w}^{(t)}}} \frac{}{}  
\end{align*}
We expect this quantity to \textit{increase} at every iteration, reaching a certain threshold after $T$ iterations, after which the algorithm stops. We want to know what is this threshold.\\

Let's examine how $\langle \bm{w^*}, \bm{w}^{(t)} \rangle$ changes at every iteration. We start from:
\begin{align*}
    \langle \bm{w^*}, \bm{w}^{(1)} \rangle = 0
\end{align*}
At iteration $t$, if the update sample is $(\bm{x_i}, y_i)$, that scalar product increases by:
\begin{align*}
    \langle \bm{w^*}, \bm{w}^{(t+1)} \rangle - \langle \bm{w^*}, \bm{w}^{(t)} \rangle &= \langle \bm{w^*}, \bm{w}^{(t+1)} - \bm{w}^{(t)} \rangle =\\
    &= \langle \bm{w^*},    \bm{w}^{(t)} + y_i \bm{x_i}- \bm{w}^{(t)} \rangle =\\
    &= \langle \bm{w^*}, y_i \bm{x_i} \rangle = y_i \langle \bm{w^*}, \bm{x_i} \rangle \underset{(\ref{eqn:stop-condition})}{\geq}  1
\end{align*}
So $\langle \bm{w^*}, \bm{w}^{(t)} \rangle$ \textit{increases} by more than $1$ at every iteration, meaning that if the algorithm stops at $T$ iterations, we have:
\begin{align*}
    \langle \bm{w^*}, \bm{w}^{(T+1)} \rangle = \sum_{t=1}^T \left( \langle \bm{w^*}, \bm{w}^{(t+1)} \rangle - \langle \bm{w^*}, \bm{w}^{(t)} \rangle  \right) \geq T
\end{align*}

Let's now examine how $\norm{\bm{w}^{(t)}}$ changes. If at iteration $t$ the sample $(\bm{x_i}, y_i)$ is selected, then:
\begin{align*}
    \norm{\bm{w}^{(t+1)}}^2 &= \norm{\bm{w}^{(t)} + y_i \bm{x_i}}^2 = \\
    &= \norm{\bm{w}^{(t)}}^2 + 2\underbrace{ y_i \langle \bm{w}^{(t)}, \bm{x_i} \rangle}_{\leq 0} +\underbrace{ y_i^2 }_{1}\norm{\bm{x_i}}^2 \\
    &\leq \norm{\bm{w}^{(t)}}^2 + (\max_i \norm{\bm{x_i}})^2 = \norm{\bm{w}^{(t)}}^2 + R^2
\end{align*}
As $\norm{\bm{w}^{(1)}} = 0$, then $\norm{\bm{w}^{(T+1)}}^2 \leq TR^2$ and so $\norm{\bm{w}^{(T+1)}} \leq \sqrt{T} R$. This means that:
\begin{align*}
    1 \geq\frac{\langle \bm{w^*}, \bm{w}^{(T+1)} \rangle}{\norm{\bm{w^*}} \norm{\bm{w}^{(T+1)}}}  \geq \frac{T}{\sqrt{T} R B} = \frac{\sqrt{T}}{R B} \Rightarrow T \leq (RB)^2 
\end{align*}

\item \textbf{Least squares}. An algorithm that solves the ERM problem for linear regression with respect to the $L_2$ loss. It outputs the $\bm{w}$ weight that satisfies:
\begin{align*}
    \arg\min_{\bm{w} \in \mathbb{R}^d} L_S(h_{\bm{w}}) = \arg\min_{\bm{w}} \frac{1}{m} \sum_{i=1}^m (\langle \bm{w}, \bm{x_i} \rangle - y_i)^2 
\end{align*} 
To solve this, compute the gradient of the empirical risk and set it to $0$:
\begin{align*}
    \grad_{\bm{w}} L &= \frac{2}{m} \sum_{i=1}^m (\langle \bm{w}, \bm{x_i}\rangle - y_i )  \bm{x_i} \overset{!}{=} \bm{0} \\
    &= \frac{2}{m} \sum_{i=1}^m \bm{x_i}(\bm{x_i}^T \bm{w} - y_i) = \frac{2}{m}\Big [ \underbrace{\left(\sum_{i=1}^m \bm{x_i} \bm{x_i}^T\right) }_{A}\bm{w} - \underbrace{\sum_{i=1}^m y_i \bm{x_i}}_{\bm{b}} \Big] = \frac{2}{m}\left[A \bm{w}-\bm{b}\right] = 0\\
    &\Rightarrow \bm{w} = A^{-1} \bm{b}
\end{align*}
(assuming $A$ is invertible).

\item \textbf{Logistic regression and MLE} %To be completed 
\end{itemize}

\section{Validation}
\begin{itemize}
    \item \textbf{Cross-validation}. 
\end{itemize}


\subsection{Theorems}
\begin{itemize}
    \item \textbf{Bound for model selection}.
    Let $\mathcal{H}= \{h_1, \dots, h_r\}$ be an arbitrary set of predictors and assume that the loss function is in $[0,1]$. Assume that a validation set $V$ of size $m_v$ is sampled independent of $\mathcal{H}$. Then, with probability $\geq 1- \delta$ over the choice of $V$ we have:
    \begin{align*}
        \forall h \in \mathcal{H}, |L_{\mathcal{D}}(h) - L_V(h)| \leq \sqrt{\frac{\log (2 |\mathcal{H}|/\delta)}{2 m_v} }
    \end{align*}  
\end{itemize}

\section{Regularization}
\begin{itemize}
    \item \textbf{Regularized Loss Minimization} (RLM): learning paradigm that generalizes ERM to make it more stable, by adding a \textbf{regularization function} $R \colon \mathbb{R}^d \to \mathbb{R}$ (where $d$ is the number of parameters to be optimizes) to the empirical risk:
    \begin{align*}
        \text{Select $h_S$ as } \arg\min_{\bm{w}} (L_S(\bm{w})+ \textcolor{Red}{R(\bm{w})})
    \end{align*}  
    \item \textbf{Tikhonov Regularization}. Use $L_2$ norm for $R$:
    \begin{align*}
        R(\bm{w}) = \lambda\norm{\bm{w}}^2 = \lambda\sum_{i=1}^d w_i^2 \qquad R\colon \mathbb{R}^d \to \mathbb{R}_+
    \end{align*} 
    $\norm{\bm{w}}^2$ measures the \textit{complexity} of the hypothesis defined by $\bm{w}$, and $\lambda$ controls the trade-off between \textit{empirical risk} and \textit{complexity}. Generally, a higher $\norm{\bm{w}}^2$ can reach a lower $L_S$, at the risk of overfitting (high $L_{\mathcal{D}}(h)$).
    \item \textbf{Ridge regression}. Linear regression with $L_2$ loss (least squares) and Tikhonov Regularization
    \begin{align*}
        \bm{w} = \arg\min_{\bm{w}} \left(\lambda\norm{\bm{w}}^2 + \frac{1}{\textcolor{Red}{2}m} \sum_{i=1}^m (\langle \bm{w}, \bm{x_i} \rangle - y_i)^2  \right)
    \end{align*}  
    The $2$ is added for convenience (it will go away when computing the gradient).
    \item \textbf{Small perturbation}. Let $S = (z_1, \dots, z_m)$ be a training set of $m$ samples. A \textit{small perturbation} of $S$ is denoted as $S^{(i)}$, and corresponds substituting the $i$-th element of $S$ with another arbitrary sample $z' \not\in S$:
    \begin{align*}
        S^{(i)} = (z_1, \dots, z_{i-1}, z', z_{i+1}, \dots, z_m)
    \end{align*}  
    \item \textbf{Overfitting}. Let $A$ be a learning algorithm and $S$ a training set. We say that $A$ is overfitting if the difference $L_{\mathcal{D}}(A(S)) - L_S(A(S))$ is very large.
    \item \textbf{On-Average-Replace-One-Stable} (OAROS). Intuitively, an algorithm is stable if by replacing an element in $S$ it's prediction does not change much, that is if $\ell(A(S^{(i)}), z_i) - \ell(A(S), z_i)$ is small (note that $A(S^{(i)}))$ does not \textit{see} $z_i$ in training, as $z_i \not\in S^{(i)}$, and so it will be likely higher). Moreover, we expect this difference to \textit{decrease} with increasing $m$, because the perturbation becomes \q{diluted} in a large $S$ (i.e. changing a single sample on a large $S$ should have a negligible impact).
    
    So, we say that an algorithm is OAROS if that difference is bounded by a decreasing function in $m$. More precisely, let $\epsilon\colon \mathbb{N} \to \mathbb{R}$ be a monotonically decreasing function. We say that a learning algorithm $A$ is \textit{on-average-replace-one-stable} with rate $\epsilon(m)$ if, for every distribution $\mathcal{D}$:
    \begin{align*}
        \underset{(S, z') \sim \mathcal{D}^{m+1}, i \sim U(m)}{\mathbb{E}} [\ell(A(S^{(i)}), z_i) - \ell(A(S), z_i)] \leq \epsilon(m)
    \end{align*}  
    Here we take the expected value over $S \sim \mathcal{D}^m$ and $z' \sim \mathcal{D}$, choosing uniformly the index $i$ to \textit{perturb}.  
    \item \textbf{Convex functions}.
    Recall that a function $f\colon \mathbb{R}^d \to \mathbb{R}$ is \textit{convex} if it stays \textit{under} every segment drawn between two points of its graph:
    \begin{align}
        f(\alpha \bm{x} + (1-\alpha)\bm{y}) \leq \alpha g(\bm{x}) + (1- \alpha) g(\bm{y}) \qquad \forall \bm{x}, \bm{y} \in \mathbb{R}^d, \alpha \in [0,1] \label{eqn:convex-def1}
    \end{align} 
    The right hand side is the \textit{segment} joining $(\bm{x}, f(\bm{x}))$ and $(\bm{y}, f(\bm{y}))$. Equivalently, a convex function is greater than its tangents:
    \begin{align*}
        f(\bm{y}) \geq f(\bm{x}) + \grad f(\bm{x}) \cdot (\bm{y} - \bm{x})
    \end{align*}

    \item \textbf{Strongly convex function}. A function $f\colon \mathbb{R}^d \to \mathbb{R}$ is $\lambda$-strongly convex if it remains convex even after subtracting a quadratic term:
    \begin{align}
        g(\bm{x}) = f(\bm{x}) - \frac{\lambda}{2} \norm{\bm{x}}^2  \text{ is convex} \label{eqn:strongly-convex-def}
    \end{align} 
    That is:
    \begin{align} \nonumber
        g(\bm{y}) \geq g(\bm{x}) + \grad g(\bm{x}) \cdot (\bm{y} - \bm{x}) \span \\ \nonumber
        &\Rightarrow f(\bm{y}) - \frac{\lambda}{2}\norm{\bm{y}}^2 \geq f(\bm{x}) - \frac{\lambda}{2}\norm{\bm{x}}^2 + (\grad f(\bm{x}) - \lambda\bm{x}) \cdot (\bm{y} - \bm{x})\\ \nonumber
        &\Rightarrow f(\bm{y}) \geq f(\bm{x}) + \grad f(\bm{x})\cdot  (\bm{y} - \bm{x})+ \frac{\lambda}{2} \norm{\bm{y}}^2  -\frac{\lambda}{2} \norm{\bm{x}}^2 +\lambda \norm{\bm{x}}^2 -\lambda \bm{x} \cdot \bm{y}  \\ 
        &\Rightarrow f(\bm{y}) \geq f(\bm{x}) + \grad f(\bm{x})\cdot  (\bm{y} - \bm{x}) + \frac{\lambda}{2} \norm{\bm{y}-\bm{x}}^2 
        \label{eqn:strongly-convex-def3}
    \end{align}  
    In the one-dimensional case this means that $f''(x) \geq \lambda$. In fact, by comparison with the second-order Taylor expansion with the Lagrange remainder we get:
    \begin{align*}
        f(y) &= f(x) + f'(x)(y-x) + \frac{1}{2} f''(x)(y-x)^2 + \frac{1}{3!}f^{(3)}(\textcolor{Red}{z})(y-x)^3   \qquad z \in (x,y)\\
        & \geq f(x) + f'(x)(y-x) + \frac{\lambda}{2} (y-x)^2 \\
        \Rightarrow f''(x) +\frac{1}{3! \cdot 2} f^{(3)}(z)(y-x) \geq \lambda  \xrightarrow[y \to x ]{ } f''(x) \geq \lambda  \span
    \end{align*}
    This generalizes to $\operatorname{Hess}f - \lambda \mathbb{I}$ is positive definite.
    \medskip

    Applying (\ref{eqn:convex-def1}) to (\ref{eqn:strongly-convex-def}) leads to an equivalent definition:
    \begin{align}
        f(\alpha\bm{x} + (1-\alpha)\bm{y}) \leq \alpha f(\bm{x}) + (1-\alpha)f(\bm{y}) - \frac{\lambda}{2} \alpha (1-\alpha) \norm{\bm{x} - \bm{y}}^2 \quad \forall \bm{x}, \bm{y} \in \mathbb{R}^d, \alpha \in [0,1] \label{eqn:strongly-convex-def2}
    \end{align} 
    \item \textbf{Lipschitzness}. Let $C \subset \mathbb{R}^d$. A function $f\colon \mathbb{R}^d \to \mathbb{R}^k$ is $\rho$-Lipschitz over $C$  if:
    \begin{align*}
        \norm{f(\bm{w_1}) - f(\bm{w_2})} \leq \rho \norm{\bm{w_1} - \bm{w_2}} 
    \end{align*}
    If $f$ is differentiable, this means that its derivative is bounded by $\rho$. 
    \item \textbf{Fitting-Stability tradeoff}.  The expected risk of a learning algorithm can be written as:
    \begin{align*}
        \underset{S \sim \mathcal{D}^m}{\mathbb{E}} [L_{\mathcal{D}}(A(S))] = \underset{S \sim \mathcal{D}^m}{\mathbb{E}} [L_S(A(S))] + \underbrace{\underset{S \sim \mathcal{D}^m}{\mathbb{E}}[L_{\mathcal{D}}(A(S)) - L_S(A(S))]}_{\leq \epsilon(m)}
    \end{align*}
    where $\epsilon(m)$ is the bounding function for an OAROS algorithm, that, in the case of RLM is:
    \begin{align*}
        \epsilon(m) =  \frac{2 \rho^2}{\lambda m}
    \end{align*}
    So increasing $\lambda$ decreases the overfitting, but at the same time reduces the relative importance of $L_S(A(S))$ in the optimization, meaning that $L_S(A(S))$ will be larger. The following bound can be proven:
    \begin{align*}
        \underset{S \sim \mathcal{D}^m}{\mathbb{E}} [L_{\mathcal{D}}(A(S))] \leq L_{\mathcal{D}}(\bm{w^*}) + \lambda \norm{\bm{w^*}}^2 + \frac{2 \rho^2}{\lambda m}
    \end{align*}
    where $\bm{w^*}$ is an hypothesis with low risk. 
\end{itemize}

\subsection{Theorems}
\begin{itemize}
    \item \textbf{Ridge regression solution}. The function being optimized is:
    \begin{align*}
        f(\bm{w}, S) = \lambda \norm{\bm{w}}^2 + \frac{1}{2m} \sum_{i=1}^m (\langle \bm{w}, \bm{x_i} \rangle - y_i)^2 
    \end{align*} 
    Computing the gradient and setting it to $0$:
    \begin{align*}
        \nabla_{\bm{w}} f(\bm{w},S) &= 2 \lambda \bm{w} + \frac{1}{m} \sum_{i=1}^m \bm{x_i} (\langle \bm{w}, \bm{x_i} \rangle - y_i)  \overset{!}{=} \bm{0} \\
        &= 2 \lambda \bm{w} + \frac{1}{m} \underbrace{\left(\sum_{i=1}^m \bm{x_i} \bm{x_i}^T\right) }_{A}\bm{w} -\frac{1}{m} \underbrace{\left(\sum_{i=1}^m y_i \bm{x_i}\right) }_{\bm{b}}= 0\\
        &= 2 \lambda \bm{w} + \frac{1}{m}(A \bm{w}) - \frac{1}{m} \bm{b} = \cancel{\frac{1}{m}}(2 \lambda m \bb{I}_d + A) \bm{w} - \cancel{\frac{1}{m}} \bm{b} = 0  \\
        &\Rightarrow \bm{w} = (2 \lambda m \bb{I}_d + A)^{-1}\bm{b} 
    \end{align*}
    \item \textbf{OAROS and overfitting}. Let $\mathcal{D}$ be a distribution, and $S = (z_1, \dots, m)$ an i.i.d. sequence of examples and let $z'$ be another i.i.d. example. Let $U(m)$ be the uniform distribution over $[m]$. Then, for any learning algorithm:
    \begin{align} \label{eqn:OAROS}
        \underset{S \sim \mathcal{D}^m}{\mathbb{E}} [L_{\mathcal{D}}(A(S)) - L_S(A(S))] = \underset{(S,z') \sim \mathcal{D}^{m+1}, i \sim U(m)}{\mathbb{E}} [\ell(A(S^{(i)}), z_i) - \ell(A(S), z_i)]
    \end{align} 
    That is, the \textit{response} from a perturbation tells us the \textit{distance} between empirical and true risk.
    
    \textbf{Proof}. Recall that the true risk of $A(S)$ is the expected value of the loss over $\mathcal{D}$. So:
    \begin{align*}
        \underset{{S \sim \mathcal{D}^m}}{\mathbb{E}}  [L_{\mathcal{D}}(A(S))] = \underset{(S,z') \sim \mathcal{D}^{m+1}}{\mathbb{E}}[\ell(A(S), z')] = \underset{{(S,z_i) \sim \mathcal{D}^{m+1}}}{\mathbb{E}} [\ell(A(S^{(i)}), z_i)]
    \end{align*}
    Note that here we evaluate the loss over samples that are not in the training set ($z' \not\in S$, $z_i \not\in S^{(i)}$). If we were to use samples \textit{from the training set} we would get the expectation of the empirical risk:
    \begin{align*}
        \mathbb{E}_{S}[L_S(A(S))] = \underset{S \sim \mathcal{D}^m, i \sim U(m)}{\mathbb{E}} [\ell(A(S), z_i)]
    \end{align*}
    Substituting in the left hand side of (\ref{eqn:OAROS}) and using the linearity of $\mathbb{E}$ we get the thesis.
    \item \textbf{Strongly convex functions}. The following conditions hold:
    \begin{enumerate}
        \item The function $f(\bm{w}) = \lambda \norm{\bm{w}}^2 $ is $2\lambda$-strongly convex.
        \item If $f$ is $\lambda$-strongly convex and $g$ is convex, then $f+g$ is $\lambda$-strongly convex.
        \item If $f$ is $\lambda$-strongly convex and $\bm{u}$ minimizes $f$, then:
        \begin{align*}
            f(\bm{w}) - f(\bm{u}) \geq \frac{\lambda}{2} \norm{\bm{w}- \bm{u}}^2 
        \end{align*}
    \end{enumerate}
    \textbf{Proof.} For (1) and (2) just use the definition (\ref{eqn:strongly-convex-def2}), and for (3) the definition (\ref{eqn:strongly-convex-def3}) and the fact that $\grad f(\bm{u}) = \bm{0}$ by hypothesis.
    \item \textbf{RLM is stable}. The RLM rule is:
    \begin{align*}
        A(S) = \arg\min_{\bm{w}} (L_S(\bm{w}) + \lambda \norm{\bm{w}}^2)
    \end{align*}
    Assuming that $L_S$ is convex, then as $\lambda \norm{\bm{w}}^2$ is $2\lambda$-strongly convex:
    \begin{align*}
        f_S(\bm{w}) = L_S(\bm{w}) + \lambda \norm{\bm{w}} \text{ is $2\lambda$-strongly convex}
    \end{align*}
    $A(S)$ by definition minimizes $f_S$, and so:
    \begin{align*}
        f_S(\bm{v}) - f_S(A(S)) \geq \lambda \norm{\bm{v} - A(S)}^2 \qquad \forall \bm{v} \in \mathbb{R}^d
    \end{align*}
    Then note that:
    \begin{align*}
        f_S(\bm{v}) - f_S(\bm{u}) &= L_S(\bm{v}) + \lambda\norm{\bm{v}}^2 - (L_S(\bm{u}) + \lambda \norm{\bm{u}}^2)
    \end{align*}
    Recall that $S^{(i)}$ is obtained from $S$ by removing $z_i$ and adding $z'$ in its place, meaning that:
    \begin{align*}
        L_S(\bm{v}) = L_{S^{(i)}}(\bm{v}) + \frac{\ell (\bm{v}, z_i) - \ell(\bm{v}, z')}{m} 
    \end{align*}
    And so, if $\bm{u}$ minimizes $f_S$:
    \begin{align*}
        f_S(\bm{v}) - f_S(\bm{u}) &= L_{S^{(i)}}(\bm{v}) + \lambda\norm{\bm{v}}^2 - (L_{S^{(i)}}(\bm{u}) - \lambda \norm{\bm{u}}^2) +\\
        &\quad \>  +\frac{\ell(\bm{v}, z_i) - \ell(\bm{u}, z_i)}{m} + \frac{\ell(\bm{u}, z') - \ell(\bm{v}, z')}{m}  \geq \lambda \norm{\bm{v} - A(S)}^2
    \end{align*}
    Let $\bm{v} = A(S^{(i)})$ and $\bm{u} = A(S)$, leading to:
    \begin{align*}
        \underbrace{f_S(A(S^{(i)})) - f_S(A(S))}_{A} &= \textcolor{Red}{\underbrace{L_{S^{(i)}}(A(S^{(i)})) + \lambda\norm{A(S^{(i)})}^2}_{f_{S^{(i)}}(A(S^{(i)}))} -\underbrace{ (L_{S^{(i)}}(A(S)) - \lambda \norm{A(S)}^2)}_{f_{S^{(i)}}(A(S))}}  +\\
        &\quad \>  +\underbrace{\frac{\ell(A(S^{(i)}), z_i) - \ell(A(S), z_i)}{m} + \frac{\ell(A(S), z') - \ell(A(S^{(i)}), z')}{m}}_{B} 
    \end{align*}
    Note that $A(S^{(i)})$ minimizes $f_{S^{(i)}}$ by definition, and so $f_{S^{(i)}}(A(S^{(i)})) \leq f_{S^{(i)}}(A(S))$, meaning that the red term is $<0$, i.e. $=-|\epsilon|$. So we have $A = B - |\epsilon$, implying $A \leq B$:
    \begin{align*}
        f_S(A(S^{(i)})) - f_S(A(S)) \leq\frac{\ell(A(S^{(i)}), z_i) - \ell(A(S), z_i)}{m} + \frac{\ell(A(S), z') - \ell(A(S^{(i)}), z')}{m}  
    \end{align*}
    And from before we have:
    \begin{align*}
       \lambda \norm{A(S^{(i)}) - A(S)}^2 \leq f_S(A(S^{(i)})) - f_S(A(S))
    \end{align*}
    Meaning that:
    \begin{align}
        \lambda \norm{A(S^{(i)}) - A(S)}^2 \leq\frac{\ell(A(S^{(i)}), z_i) - \ell(A(S), z_i)}{m} + \frac{\ell(A(S), z') - \ell(A(S^{(i)}), z')}{m}  \label{eqn:ineq1}
    \end{align}

    Suppose that the loss as a function of the model ($h \mapsto \ell(h, z_i)$) is $\rho$-Lipshitz. This means that the loss over a sample \textit{outside} the training dataset can't grow too much compared to that of an element inside the sample: 
    \begin{align*}
        \ell(A(S^{(i)}), z_i) - \ell(A(S), z_i) \leq \rho\norm{A(S^{(i)}) - A(S)}
    \end{align*}  
    Similarly, as $z' \notin S$, but $z' \in S^{(i)}$:
    \begin{align*}
        \ell(A(S), z') - \ell(A(S^{(i)}), z') \leq \rho \norm{A(S^{(i)})-A(S)} 
    \end{align*}
    Substituting in (\ref{eqn:ineq1}):
    \begin{align*}
        \lambda \norm{A(S^{(i)}) - A(S)}^2 \leq \frac{2 \rho \norm{A(S^{(i)}) - A(S)} }{m} 
    \end{align*}
    Rearranging:
    \begin{align*}
        \norm{A(S^{(i)}) - A(S)} \leq \frac{2 \rho}{m \lambda} 
    \end{align*}
    And so:
    \begin{align*}
        \ell(A(S^{(i)}), z_i) - \ell(A(S), z_i) \leq \frac{2 \rho^2}{m \lambda} 
    \end{align*}
    Taking the expected value, this is equivalent to:
    \begin{align*}
        \underset{S \sim \mathcal{D}^m}{\mathbb{E}} [L_{\mathcal{D}}(A(S)) - L_S(A(S))] \leq\frac{2 \rho^2}{\lambda m}  
    \end{align*}
\end{itemize}

\section{Gradient Descent}
\begin{itemize}
    \item Find minimum of $f \colon \mathbb{R}^d \to \mathbb{R}$ by moving in the opposite direction of the gradient. Start with $\bm{w^{(0)}} = \bm{0} \in \mathbb{R}_d$, and then apply:
    \begin{align*}
        \bm{w}^{(t+1)} = \bm{w}^{(t)} - \eta \nabla f(\bm{w}^{(t)})
    \end{align*}
    where $\epsilon \in \mathbb{R}$ is the learning rate.
    \item If $f$ is a \textbf{convex} $\rho$-\textbf{Lipschitz} function, and the minimization domain is $\norm{\bm{w}} \leq B$, then after $T$ steps with $\eta = \sqrt{B^2/(\rho^2 T)}$ GD produces a $\bar{\bm{w}}$ so that:
    \begin{align*}
        f(\bar{\bm{w}}) - f(\bm{w^*}) \leq \frac{B \rho}{T} 
    \end{align*}
    where $\bm{w^*}$ is the minimum point.
    \item \textbf{Cons:} Needs all the training set at each iteration (high computational time)
    \item \textbf{Pros:} stable. 
\end{itemize}

\section{Stochastic Gradient Descent}
\begin{itemize}
    \item Instead of computing $\nabla f(\bm{w})$, pick a random $\bm{v_t}$ so that $\mathbb{E}[\bm{v_t} | \bm{w}^{(t)}] = \nabla f(\bm{w}^{(t)})$ (i.e. with expected value equal to the real gradient of $f$). Then apply the rule as before:
    \begin{align*}
        \bm{w}^{(t+1)} = \bm{w}^{(t)} - \eta\bm{v_t} 
    \end{align*}
    \item Can be used to minimize directly $L_{\mathcal{D}}$ (true risk). Recall that:
    \begin{align*}
        L_{\mathcal{D}}(\bm{w}) = \underset{z \sim \mathcal{D}}{\mathbb{E}} [\ell(\bm{w},z)]
    \end{align*}
    And so:
    \begin{align*}
        \mathbb{E}[\bm{v_t}|\bm{w}^{(t)}] = \underset{z \sim \mathcal{D}}{\mathbb{E}} [\nabla \ell(\bm{w}^{(t)}, z)] = \nabla \underset{z \sim \mathcal{D}}{\mathbb{E}} [\ell(\bm{w}^{(t)}, z)] = \nabla L_{\mathcal{D}}(\bm{w}^{(t)})
    \end{align*}
    \item \textbf{Pros}: Faster, can jump out local minima
    \item \textbf{Cons}: Noisy (can be alleviated with adaptive step size) 
    \item \textbf{In the case of regularization}. We want to minimize:
    \begin{align*}
        f(\bm{w}) = \frac{\lambda}{2} \norm{\bm{w}}^2 + L_S(\bm{w})
    \end{align*} 
    At every step, choose $\bm{v_t} = \nabla \ell(\bm{w}^{(t)}, z)$ for some $z \sim \mathcal{D}$. Then the full gradient of $f$ is $\lambda \bm{w}^{(t)} + \bm{v_t}$. Choosing $\eta = 1/(\lambda t)$ (useful for $2\lambda$-strongly convex functions) leads to:
    \begin{align*}
        \bm{w}^{(t+1)} &= \bm{w}^{(t)} - \frac{1}{\lambda t}\left(\lambda \bm{w}^{(t)} + \bm{v_t}\right) =\\
        &= \left(1-\frac{1}{t} \right)\bm{w}^{(t)} - \frac{1}{\lambda t} \bm{v_t}  =\\
        &= \frac{t-1}{t} \bm{w}^{(t)}  - \frac{1}{\lambda t} \bm{v_t}  =\\
        &\underset{(a)}{=}  \frac{t-1}{t} \left(\frac{t-2}{t-1} \bm{w}^{(t-1)} - \frac{1}{\lambda(t-1)}\bm{v_t}  \right) - \frac{1}{\lambda t} \bm{v_t} =\\
        &= \frac{t-1}{t} \frac{t-2}{t-1} \bm{w}^{(t-1)} - \frac{1}{\lambda t} (\bm{v_t} + \bm{v_{t-1}}) =\\
        &= \frac{(t-1)!}{t!} \underbrace{\bm{w}^{(0)}}_{0} - \frac{1}{\lambda t} (\bm{v_t} + \dots + \bm{v_0}) = -\frac{1}{\lambda t} \sum_{i=1}^t \bm{v_i} 
    \end{align*}
    where in (a) we reiterated the last row with $t \to t-1$ to express $\bm{w}^{(t)}$.
\end{itemize}


\chapter{SVM}
\begin{itemize}
    \item \textbf{Hard-SVM}. Consider a \textbf{separable} training set. The algorithm picks the ERM solution with the largest \textit{margin}, i.e. the one where the halfspace boundary is furthest away from the samples. Let $\bm{w}$ be the unit vector $\perp$ to the separating hyperplane. The distance between a sample $\bm{x_i}$ and the hyperplane is $|\langle \bm{w}, \bm{x_i} \rangle + b|$ (\ref{eqn:hyperplane-dist}). The margin is the \textit{minimum} distance between a sample and the hyperplane:
    \begin{align*}
        \text{Margin} = \min_{i \in [m]} |\langle \bm{w}, \bm{x_i} \rangle + b|
    \end{align*} 
    The hard-SVM problem chooses $(\bm{w},b)$ so to maximize the margin (while still classificating correctly all the examples), i.e.:
    \begin{align} \label{eqn:svm-1}
        \underset{(\bm{w}, b)\colon \norm{\bm{w}} = 1}{\operatorname{argmax}} \min_{i \in [m]} |\langle \bm{w}, \bm{x_i} \rangle + b| \> \text{ such that } \forall i, \> y_i (\langle \bm{w}, \bm{x_i} \rangle + b) > 0
    \end{align}
    In the separable case, if $\bm{w^*}$ is a solution, note that:
    \begin{align} \label{eqn:equivalence}
        |\langle \bm{w^*}, \bm{x_i} \rangle + b| = y_i(\langle \bm{w^*}, \bm{x_i} + b\rangle)
    \end{align}
    And so the \textit{constraint} can be integrated in the optimization:
    \begin{align} \label{eqn:hard-svm}
        \underset{(\bm{w}, b)\colon \norm{\bm{w}} = 1}{\operatorname{argmax}} \min_{i \in [m]} y_i(\langle \bm{w}, \bm{x_i} + b\rangle)
    \end{align}
    In fact, by separability we know that there exist a $\bm{w^*}$ that satisfies $\forall i, \> y_i (\langle \bm{w^*}, \bm{x_i} \rangle + b) > 0$, and in particular:
    \begin{align*}
        \min_{i \in [m]} y_i (\langle \bm{w^*}, \bm{x_i} \rangle + b) > 0
    \end{align*}
    Then:
    \begin{align*}
        \max_{(\bm{w},b) \colon \norm{\bm{w}=1}} \min_{i \in [m]} y_i(\langle \bm{w}, \bm{x_i} + b\rangle) \geq  \min_{i \in [m]} y_i (\langle \bm{w^*}, \bm{x_i} \rangle + b) > 0
    \end{align*}
    Meaning that (\ref{eqn:hard-svm}) indeed generates a $\bm{w}$ that separates the data, justifying the use of (\ref{eqn:equivalence}) and thus the equivalence with (\ref{eqn:svm-1}).

    The hard-SVM problem is also equivalent (see theorem below) to:
    \begin{align*}
        \bm{w_0} = \arg\min_{\bm{w}} \norm{\bm{w}}^2 \text{ s.t. } \forall i, y_i (\langle \bm{w}, \bm{x_i} \rangle  + b_i) \geq 1 
    \end{align*}
    We can always use homogeneous coordinates, including the bias in $\bm{w}$, arriving to:
    \begin{align*}
        \bm{w_0} = \arg\min_{\bm{w}} \norm{\bm{w}}^2 \text{ s.t. } \forall i, y_i \langle \bm{w}, \bm{x_i} \rangle \geq 1 
    \end{align*}
    There is some difference as now the bias appears in the $\norm{\bm{w}}^2$, which proves to be not much significant in practice.
    
    \item \textbf{Soft SVM}. Allow some violation of the inequalities $y_i (\langle \bm{w}, \bm{x_i} \rangle + b) \geq 1$, by instead requiring:
    \begin{align*}
        y_i (\langle \bm{w}, \bm{x_i} \rangle + b) \geq 1- \xi_i
    \end{align*}
    The $\bm{\xi}$ are called \textit{slack variables}, and measure how much each inequality is violated (i.e. how much misclassified samples are far from the separating hyperplane). We now minimize both $\norm{\bm{w}}^2$ (as before) and the average of $\bm{\xi}$ (reduce the average violation), controlling the trade-off between the two with a $\lambda$ hyper-parameter:
    \begin{align*}
        \min_{\bm{w}, b, \bm{\xi}} \left(\lambda \norm{\bm{w}}^2 + \frac{1}{m} \sum_{i=1}^m \xi_i \right) \text{ s.t. } \forall i, \> y_i (\langle \bm{w}, \bm{x_i} \rangle + b) \geq 1- \xi_i, \quad \xi_i \geq0  
    \end{align*}
    This can be rewritten using the \textbf{hinge loss}:
    \begin{align*}
        \ell^{\mathrm{hinge}}((\bm{w}, b), (\bm{x}, y)) = \max(0, 1- y(\langle \bm{w}, \bm{x} \rangle + b))
    \end{align*} 
    For a correctly classified sample, $y (\langle \bm{w}, \bm{x} \rangle + b) \geq 1$, and so the loss is $0$. If the sample is misclassified it is $> 0$, and increases with the distance from the hyperplane. Note that $\xi_i = \ell^{\mathrm{hinge}}((\bm{w}, b), (\bm{x_i}, y_i))$ (to prove this, fix $\bm{w}, \bm{x}$ and $i$ in the soft-svm rule and minimize for $\xi_i$) and so:
    \begin{align*}
        \min_{\bm{w},b} \Big(\lambda \norm{\bm{w}}^2 + \underbrace{\frac{1}{m} \sum_{i=1}^m \ell^{\mathrm{hinge}}((\bm{w}, b), (\bm{x_i}, y_i))}_{L_S^{\mathrm{hinge} }(\bm{w},b)} \Big)
    \end{align*} 
\end{itemize}

\section{Theorems}
\begin{itemize}
    \item \textbf{Hard-SVM is equivalent to a quadratic program}. A quadratic program solves an optimization problem in which the objective is a convex quadratic function and the constraints are linear inequalities.

    Start from the Hard-SVM formulation in the separable case:
    \begin{align*}
        \underset{(\bm{w},b)\colon \norm{\bm{w}}=1}{\operatorname{argmax}}  \min_{i \in [m]} y_i (\langle \bm{w}, \bm{x_i} \rangle + b)
    \end{align*}

    Let $(\bm{w^*}, b^*)$ be a solution, and let $\gamma^*$ be the margin of that solution, i.e.:
    \begin{align*}
        \gamma^* = \min_{i \in [m]} y_i (\langle \bm{w^*}, \bm{x_i} \rangle +b^*) \Rightarrow y_i(\langle \bm{w^*}, \bm{x_i} \rangle + b) \geq \gamma^* 
    \end{align*}
    And dividing by $\gamma^*$:
    \begin{align*}
        y_i \left(\langle \frac{\bm{w^*}}{\gamma^*}, \bm{x_i}  \rangle +\frac{b^*}{\gamma^*}\right) \geq1 
    \end{align*} 
    So, the pair $(\bm{w^*}, b^*)/\gamma^*$ is one of the solutions in the constraints of quadratic programming, which solves:
    \begin{align*}
        (\bm{w_0}, b_0) = \arg\min_{(\bm{w},b)} \norm{\bm{w}}^2 \text{ s.t. } \forall i, \> y_i (\langle \bm{w}, \bm{x_i} + b \rangle) \geq 1 
    \end{align*} 
    As $\norm{\bm{w}}^2$ is minimized, the final solution will have $\norm{\bm{w_0}} \leq \norm{\bm{w^*}}/\gamma^* = 1/\gamma^*$.\\
    
    To prove the equivalence, we need also to show that any normalized output $\hat{\bm{w}} = \bm{w_0}/\norm{\bm{w_0}}$, $\hat{b} = b_0/\norm{\bm{w_0}}$ of the quadratic problem is a solution. To do this, we evaluate the main condition:
    \begin{align*}
        y_i (\langle \hat{\bm{w}}, \bm{x_i} \rangle+ \hat{b}) = \frac{1}{\norm{\bm{w_0}}} \underbrace{y_i (\langle \bm{w_0}, \bm{x_i} \rangle + b_0)}_{\geq 1}  \geq \frac{1}{\norm{\bm{w_0}}} \geq \gamma^*
    \end{align*}
    Which proves that it is indeed a solution.

    Note that the quadratic paradigm is just a linear predictor with \q{regularization}.

    \item \textbf{SGD for Soft SVM}.  Recall the hinge loss:
    \begin{align*}
        \ell^{\mathrm{hinge}}(\bm{w}, (\bm{x}, y)) = \max\{0, 1- y \langle \bm{w}, \bm{x} \rangle\}
    \end{align*}
    (Here we assume the homogeneous case). The gradient becomes:
    \begin{align*}
        \bm{v} = \begin{cases}
            \bm{0} & (1- y \langle \bm{w}, \bm{x}) \leq 1\\
            -y \bm{x} & \text{otherwise}
        \end{cases}
    \end{align*}
    For the update rule we can use the one found for $\lambda$-strongly convex functions (i.e. with regularization):
    \begin{align*}
        \bm{w}^{(t+1)} = -\frac{1}{\lambda t} \sum_{j=1}^{t} \bm{v}^{(t)} 
    \end{align*}
    \item \textbf{Duality}.  
\end{itemize}

\section{Clustering}
\begin{itemize}
    \item \textbf{Clustering}: Divide a set of objects ($N$-dimensional vectors) into groups (clusters), such that \textit{similar objects end up in the same group} and \textit{dissimilar objects are separated into different groups}. 
    
    There are two main problems:
    \begin{itemize}
        \item These two requirements may \textbf{contradict} each other, because similarity is not transitive (imagine a chain of similar elements, where $x_i$ is similar to $x_{i+1}$, but the first element $x_0$ is very different from the last $x_n$). In such cases, one of the two objectives \textit{dominates} over the other. 
        \item In general there is no unique solution (ground truth) to the problem: several alternatives are acceptable following different \textit{implicit} notions of similarity. This means that it's difficult to \textbf{evaluate\textbf{ the performance}}
    \end{itemize}
    
    \item \textbf{Clustering Model}. Let's formalize the clustering problem and introduce a common notation.
    \begin{itemize}
        \item \textbf{Input}: \textit{set of elements to be grouped} $\mathcal{X}$, and a \textit{distance function} $d\colon \mathcal{X}\times \mathcal{X} \to \mathbb{R}_+$ (symmetric $d(x,y)=d(y,x)$, definite positive $d(x,y)\geq 0$, $d(x,x) = 0$, satisfies the triangle inequality $d(x,z) \leq d(x,y) + d(y,z)$).
        \item \textbf{Output}: a \textit{partition} of $\mathcal{X}$ into $k$ disjoint clusters: $C = (C_1, C_2, \dots, C_k)$ such that $\cup_{i=1}^k C_i = \mathcal{X}$, $\forall i \neq j\colon C_i \cap C_j = \varnothing$.
    \end{itemize}

    \item \textbf{Linkage-based clustering}. All points are initially clusters. At every step, merge two clusters $A$ and $B$ that are the closest according to some condition, i.e. minimize $D(A,B)$. Some examples of $D$:
    \begin{itemize}
        \item \textbf{Single linkage}: minimum distance between an element of $A$ and one of $B$:
        \begin{align*}
            D(A,B) = \min \{d(\bm{x}, \bm{x'}) \colon \bm{x} \in A, \bm{x'} \in B \} 
        \end{align*}
        \item \textbf{Max linkage}: maximum distance between an element of $A$ and one of $B$:
        \begin{align*}
            D(A,B) = \max \{d(\bm{x}, \bm{x'}) \colon \bm{x} \in A, \bm{x'} \in B \} 
        \end{align*}
        \item \textbf{Average linkage}: average distance between elements of $A$ and elements of $B$:
        \begin{align*}
            D(A,B) = \frac{1}{|A||B|} \sum_{\bm{x} \in A} \sum_{\bm{x'} \in B} d(\bm{x}, \bm{x'})
        \end{align*} 
    \end{itemize}
    Reiterate until a \textit{terminating condition} (e.g. $k$ clusters, all clusters are $>r$ apart, all points are a cluster (output the total tree - dendrogram)) 

    \item \textbf{Cost-based clustering}. Define an objective function $G \colon (\mathcal{X},d), C \mapsto \mathbb{R}_+$, such that it evaluates a clustering $C$ of $\mathcal{X}$ using the distance $d$. 
    
    \item \textbf{K-Means}. The $k$-means objective function measures the squared distance between each point in $\mathcal{X}$ to the centroid of its cluster (assume that $\mathcal{X}\subseteq \mathcal{X}'$, with $(\mathcal{X}',d)$ a metric space, otherwise it would not make sense to consider centroids that are $\not\in \mathcal{X}$). The centroid $\mu_i(C_i)$ of $C_i$ is defined as:
    \begin{align*}
        \bm{\mu_i}(C_i) = \arg\min_{\bm{\mu}\in \mathcal{X}'} \sum_{\bm{x} \in C_i} d(\bm{x},\bm{\mu})^2 = \frac{1}{|C_i|} \sum_{\bm{x} \in C_i} \bm{x} 
    \end{align*}
    That is, it's the point $\in \mathcal{X}'$ that lies at the minimum square distance from all the other points in $C_i$.
    
    The \textit{loss} of every cluster is the value at that minimum, and the objective is the sum of that loss over all the clusters:
    \begin{align*}
        G_{\mathrm{k-means}}((\mathcal{X},d), (C_1, \dots, C_k)) = \sum_{i=1}^k \sum_{\bm{x} \in C_i} d(\bm{x}, \bm{\mu_i}(C_i))^2
    \end{align*} 
    
    \textbf{Algorithm}. 
    \begin{enumerate}
        \item Select $k$ random centroids, each representing a cluster.
        \item Assigning each point to the closest centroid
        \begin{align*}
            \forall i \colon C_i = \{\bm{x} \in \mathcal{X}\colon i = \arg\min_{j \in [k]} \norm{\bm{x}-\bm{\mu_j}}\}
        \end{align*}
        \item Compute new centroids for the newly created clusters:
        \begin{align*}
            \forall i\colon \bm{\mu_i} = \frac{1}{|C_i|} \sum_{\bm{x} \in C_i} \bm{x} 
        \end{align*}
        \item Reiterate 2-3 until convergence (e.g. if $\Delta G$ is lower than a certain threshold).
    \end{enumerate}
     
\end{itemize}


\subsection{Theorems}
\begin{itemize}
    \item \textbf{Each iteration of k-means does not increase the objective function}. Fix $\mathcal{X}$ and $d = \norm{\cdot, \cdot}^2$. The objective function of $k$-means becomes:
    \begin{align*}
        G(C_1, \dots, C_k) = \min_{\bm{\mu_1}, \dots, \bm{\mu_k} \in \mathbb{R}^n} \sum_{i=1}^k \sum_{\bm{x} \in C_i} \norm{\bm{x} - \bm{\mu_i}}^2
    \end{align*}
    Note that:
    \begin{align*}
        \min_{\bm{\mu_i} \in \mathbb{R}^n}\sum_{\bm{x} \in C_i} \norm{\bm{x}-\bm{\mu_i}}^2 = \sum_{\bm{x} \in C_i} \norm{\bm{x}-\bm{\mu}(C_i)}^2 \qquad \bm{\mu}(C_i) = \frac{1}{|C_i|}\sum_{\bm{x} \in C_i} \bm{x} 
    \end{align*}
    Denote the partition at step $t$ with $C_i^{(t)}$. So we have:
    \begin{align*}
        G(C_1^{(t)}, \dots, C_k^{(t)}) = \min_{\bm{\mu_1}, \dots, \bm{\mu_k} \in \mathbb{R}^n} \sum_{i=1}^k \sum_{\bm{x} \in C_i^{(t)}} \norm{\bm{x} - \bm{\mu_i}^{(t)}}^2 \leq\sum_{i=1}^k   \sum_{\bm{x} \in C_i^{(t)}} \norm{\bm{x} - \bm{\mu_i}^{\textcolor{Red}{(t-1)}}}^2 
    \end{align*}
    By definition of minimum. The new partition $C_i^{(t)}$ is chosen by assigning each points to the closest $\bm{\mu_i}^{(t-1)}$, meaning that:
    \begin{align*}
        \sum_{i=1}^k   \sum_{\bm{x} \in C_i^{(t)}} \norm{\bm{x} - \bm{\mu_i}^{\textcolor{Red}{(t-1)}}}^2 = \min_{\{C_i\}} \sum_{i=1}^k \sum_{\bm{x} \in C_i} \norm{\bm{x} - \bm{\mu_i}^{{(t-1)}}}^2 \leq \sum_{i=1}^k \sum_{\bm{x} \in C_i^{\textcolor{Red}{(t-1)}}} \norm{\bm{x} - \bm{\mu_i}^{(t-1)}}^2
    \end{align*}
    Putting it all together:
    \begin{align*}
        G(C_1^{(t)}, \dots, C_k^{(t)})  \leq\sum_{i=1}^k \sum_{\bm{x} \in C_i^{\textcolor{Red}{(t-1)}}} \norm{\bm{x} - \bm{\mu_i}^{(t-1)}}^2 = G(C_1^{(t-1)}, \dots, C_k^{(t-1)})
    \end{align*}
\end{itemize}

\section{PCA}
\begin{itemize}
    \item \textbf{Principal Component Analysis}. Let $\bm{x_1}, \dots, \bm{x_m}$ be vectors in $\mathbb{R}^d$. The idea is to convert them to a \textit{lower dimensionality representation} and then back again, trying to lose the least information possible. 
    
    Consider a $n\times d$ matrix $W$ with $n < d$ (compression matrix), representing a linear transformation $\bm{x} \mapsto W \bm{x}$, so that $W \bm{x} \in \mathbb{R}^n$ is the lower dimensionality representation of $\bm{x}$. Then consider a \textit{recovering matrix} $d \times n$ $U$ that \textit{recovers} the vectors, i.e. $\mathbb{R}^n \ni \bm{y} \mapsto U\bm{y} \in \mathbb{R}^d$. We want to choose $W$ and $U$ so that $\bm{x_i}$ and the \textit{compressed-recovered} vector $UW\bm{x_i}$ are close, i.e.:
    \begin{align} \label{eqn:PCA-problem}
        \underset{W \in \mathcal{M}_{n\times d}(\mathbb{R}), U\in \mathcal{M}_{d \times n}(\mathbb{R})}{\arg\min} \sum_{i=1}^m \norm{\bm{x_i} - UW \bm{x_i}}^2
    \end{align}

    There exists a solution $(U^*, W^*)$ such that $U^*$ is orthogonal ($(U^*)^T U^* = \mathbb{I}$) and $W^* = (U^*)^T$. To see this, let $(U,V)$ be generic. Then note that $\bm{x} \mapsto W\bm{x} \in \mathbb{R}^n$, and so the range $R$ of $U (W\bm{x})$ is \q{$U \mathbb{R}^n$}, which is a $n$-dimensional linear subspace of $\mathbb{R}^d$.
    So $UW\bm{x} \in R$ $\forall \bm{x} \in \mathbb{R}^d$.

    Now choose a ON basis of $R$, which consists of $n$ vectors of $d$ dimension, that can be arranged as columns in a matrix $V$ $d \times n$, with $V^T V = \mathbb{I}$. We can write any vector $\bm{y} \in \mathbb{R}^n$ \q{in coordinates} with respect to that basis, i.e. $\forall \bm{r} \in R, \> \exists \bm{y} \in \mathbb{R}^n$ s.t. $\bm{r} = V\bm{y}$, where $\bm{y}$ are the \textit{coordinates} of $\bm{r}$ in the $V$ basis. Now pick any $\bm{x} \in \mathbb{R}^d$ and consider its distance with an arbitrary element $V\bm{y}$ of $R$:
    \begin{align*}
        \norm{\bm{x} - V \bm{y}}^2 = \norm{\bm{x}}^2 + (V\bm{y}) \cdot V(\bm{y}) - 2 (V\bm{y}) \cdot \bm{x}
    \end{align*}
    And by writing dot products as matrix multiplications $\bm{a} \cdot \bm{b} = \bm{a}^T \bm{b}$ we arrive to:
    \begin{align*}
        \norm{\bm{x} - V \bm{y}}^2 &= \norm{\bm{x}}^2 + (V\bm{y})^T V\bm{y} - 2(V \bm{y})^T \bm{x} =\\
        &= \norm{\bm{x}}^2 + \bm{y}^T \underbrace{V^T V}_{\mathbb{I}} \bm{y} - 2\bm{y}^T V^T \bm{x} =\\
        &= \norm{\bm{x}}^2 + \norm{\bm{y}}^2 -\bm{2y}^T (V^T \bm{x})
    \end{align*} 
    Call $V\bm{y} = \tilde{\bm{x}} \in R$. For a fixed $\bm{x} \in \mathbb{R}^d$, the closest $\tilde{\bm{x}} \in R$ is given by:
    \begin{align*}
        \tilde{\bm{x}} = \arg\min_{\bm{y} \in \mathbb{R}^n} \norm{\bm{x} - V\bm{y}}^2 = \arg\min_{\bm{y} \in \mathbb{R}^n} \left(\norm{\bm{x}}^2 + \norm{\bm{y}}^2 -2 \bm{y}^T (V^T \bm{x})\right)
    \end{align*}
    We can minimize this expression by computing the gradient $\grad_{\bm{y}}$ and setting it to $0$:
    \begin{align*}
       \bm{\nabla}_{\bm{y}}\left(\norm{\bm{x}}^2 + \norm{\bm{y}}^2 -2 \bm{y}^T (V^T \bm{x})\right) =  2 \bm{y} - 2 V^T \bm{x} \overset{!}{=}  0 \Rightarrow \bm{y}_0 = V^T \bm{x} 
    \end{align*}
    And so the solution is $\tilde{\bm{x}} = V \bm{y}_0 = V V^T \bm{x}$. We can compute the $\tilde{\bm{x}}_i$ closest to $\bm{x}_i$ for $1 \leq i \leq m$, resulting in:
    \begin{align*}
        \sum_{i=1}^m \norm{\bm{x_i} - UW \bm{x_i}}^2 \geq \sum_{i=1}^m \norm{\bm{x_i} - V V^T \bm{x_i}}^2
    \end{align*}
    for any generic $U$ and $W$. So $(U,W) = (V,V^T)$ is a solution of (\ref{eqn:PCA-problem}), meaning that we can rewrite it as:
    \begin{align*}
        \underset{U \in \mathcal{M}_{d \times n}, U^T U = \mathbb{I}}{\arg\min} \sum_{i=1}^m \norm{\bm{x_i} - U U^T \bm{x_i} }^2
    \end{align*}

    Let's simplify the distance expression a bit more:
    \begin{align*}
        \norm{\bm{x} - U U^T \bm{x} }^2 &= \norm{\bm{x}}^2 + (UU^T\bm{x})^T UU^T \bm{x} - 2 (U U^T \bm{x})^T \bm{x} =\\
        &= \norm{\bm{x}}^2 + \bm{x}^T U \underbrace{U^T U}_{\mathbb{I}} U^T \bm{x} - 2 \bm{x}^T U U^T \bm{x} =\\
        &= \norm{\bm{x}}^2 + \bm{x}^T U U^T\bm{x} - 2\bm{x}^T U U^T\bm{x} =\\
        &= \norm{\bm{x}}^2 -\bm{x}^T U U^T \bm{x} =\\
        &= \norm{\bm{x}}^2 - (U^T \bm{x})^T U^T\bm{x} = \norm{\bm{x}}^2 - (U^T\bm{x}) \cdot (U^T \bm{x}) =\\
        &= \norm{\bm{x}}^2 - \sum_{i=1}^d \sum_{j,k=1}^d (U^T)_{ij} x_j (U^T)_{ik} x_k  =\\
        &= \norm{\bm{x}}^2 -\sum_{i=1}^d \sum_{j,k=1}^d (U^T)_{ij} x_j x_k U_{ki} = \norm{\bm{x}}^2 - \sum_{i=1}^d (U^T [\bm{x} \bm{x}^T]U)_{i i} =\\
        &= \norm{\bm{x}}^2 - \operatorname{trace}(U^T \bm{x} \bm{x}^T U) 
    \end{align*}
    Note that $\norm{\bm{x}}^2$ is constant, and so can be removed from the optimization. The trace is linear and so:
    \begin{align*}
        \sum_{i=1}^m \operatorname{Trace} (U^T \bm{x_i} \bm{x_i}^T U) = \operatorname{Trace} \left(U^T \sum_{i=1}^m \bm{x_i}\bm{x_i}^T U\right) 
    \end{align*} 
    And finally the $-$ sign transforms a $\min$ into a $\max$, leading to the reformulation:
    \begin{align} \label{eqn:PCA-2}
        \underset{U \in \mathcal{M}_{d \times n}(\mathbb{R}), U^T U = \mathbb{I}}{\arg\max} \operatorname{Trace} \Big(U^T\underbrace{ \sum_{i=1}^m \bm{x_i}\bm{x_i}^T }_{A}U\Big)
    \end{align}
    Now $A$ is symmetric and so (spectral theorem) can be orthogonally diagonalized into $A = VDV^T$ with $D$ diagonal and $V^TV = V V^T = \mathbb{I}$ with the columns of $V$ being the eigenvectors of eigenvalue the elements of $D$.

    The solution to the PCA is to set $U$ to the matrix with columns equal to the $n$ eigenvectors with highest eigenvalues, and $W=U^T$.

    In fact, let $A = VDV^T$ be the spectral decomposition. Let's compute (\ref{eqn:PCA-2}). Let $U$ be a generic $d\times n$ matrix with $U^T U = \mathbb{I}$. We start by evaluating:
    \begin{align*}
        U^T A U = U^T VDV^T U = \underbrace{(V^T U)^T}_{B^T}  D \underbrace{(V^TU)}_{B} = B^T D B
    \end{align*}
    and so as $B$ is $d \times n$ and $D$ is $d\times d$:
    \begin{align*}
        \operatorname{Trace}(U^T A U) &= \operatorname{Trace}(B^T D B) = \sum_{i=1}^n \sum_{k,j=1}^d B_{ki} D_{kj}\delta_{kj} B_{ji} = \sum_{i=1}^n \sum_{j=1}^d D_{jj} B_{ji}^2 =\\
        &= \sum_{j=1}^d D_{jj} \underbrace{\sum_{i=1}^n B_{ji}^2}_{\beta_j} 
    \end{align*}
    $\beta_j$ is the norm of the $j$-th row of $B$. We now show that it's $\leq 1$.

    Note that the columns of $B$ are orthonormal, in fact:
    \begin{align*}
        B^T B = (V^T U)^T V^T U = U^T V V^T U = \mathbb{I}
    \end{align*}
    this implies that the columns of $B$ have unit-norm:
    \begin{align*}
        \sum_{\textcolor{Red}{j}=1}^d B_{ji}^2 = 1 
    \end{align*}
    Then, as $B$ is $d \times n$ with $n < d$, the norm of its rows must be $\leq 1$. In fact, we can extend the ON basis of $\mathbb{R}^n$ (the columns of $B$) to a ON basis of $\mathbb{R}^d$, constructing a $d\times d$ $\tilde{B}$ that is equal to $B$ for the first $n$ columns. Then:
    \begin{align*}
        \sum_{\textcolor{Red}{i}=1}^d \tilde{B}_{ji}^2 = 1 \geq \sum_{i=1}^n B_{ji}^2
    \end{align*}

    For the maximization, consider that $\sum_{j=1}^d B_j= n$, because they are rows of a $d \times n$ matrix with orthonormal columns. So:
    \begin{align*}
        \operatorname{Trace}(U^T A U) = \sum_{j=1}^d D_{jj} \beta_j \leq \max_{\beta_j \in [0,1]; \norm{\bm{\beta}} \leq n} \sum_{j=1}^d D_{jj} \beta_j = \sum_{j=1}^{\textcolor{Red}{n}} D_{jj}
    \end{align*}
    For the last part, consider $D_{jj}$ ordered so that $D_{11}$ is maximum and $D_{d d}$ is minimum. To maximize the sum, choose $B_j = 1$ for $j\leq n$, and $\beta_j = 0$ for $j > n$ (as $\sum_j B_j$ must be $n$).  

    Finally, note that the inequality is saturated by choosing $U$ to be the matrix whose columns are the $n$ leading eigenvectors of $A$, and this concludes the proof.
\end{itemize}










\end{document}
