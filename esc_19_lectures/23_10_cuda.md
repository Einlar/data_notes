# GPUs
A **CPU** consists of:
- a big *control unit*, able to optimize execution (dynamic scheduling, branch prediction...)
- several *Arithmetic Logic Units* doing operations (one for each core), that access local caches for speed. So it is more convenient to have threads working on contiguous chunks of data (e.g. first thread goes 0-10, second 10-20, etc.) 
- A common (and larger) cache

On the other hand a **GPU** consists of:
- Several *Streaming Multiprocessors* (SM), each with many ALUs, executes kernels (i.e. functions) in a parallel way (they apply a kernel on a vector), using hundreds of threads.
- Each SM has access to a shared local cache. ALUs acts in SIMD mode in groups of 32, called **warps**. 
This is different from a CPU, where each ALU has its cache - resulting in a very different optimization. It is then convenient to have each thread acting on each element (first thread gets 0, second gets 1, etc.). Loads are shared, meaning that if a thread issues a load, all other threads (in the same warp) will be affected (and loaded).
- There is not branch prediction, but **branch predicament**. For example, suppose that an if statement is issued, with its condition being true for only a group of threads (within the same warp), but not all (e.g. `thread_id % 2 == 0` will be true for exactly half of the threads). Then, the GPU will explore each possibility, activating only part of the threads at a time (discarding the results of the others). So, in the example, the first cycle will involve all the even threads, the second all the odd ones (halving the velocity). Note how inserting if-clauses significantly reduces the speed in a GPU.
- The **theoretical trhoughput** is the maximum amount of data that a kernel can produce in the unit time, and is equal to:
$$ 2 \times \mathrm{access\ width} \times \mathrm{mem\ freq}$$
The 2 comes from the DDR (Double Data Rate), i.e. a data transfer is made both on the rising and falling edges of the clock signal.

## Simple architecture
In a normal program, a serial code working on the CPU loads data in the GPU, and launches there a kernel, that executes on the SMs producing some results, which are then copied back to the main memory, accessed by the CPU.

To be executed on a GPU, a program (written in C++, with some limitations) is divided into blocks (logical structure), which are mapped to physical warps (e.g. if a block requires 15 parallel operations, it will be executed by a single warp, discarding 32-15 threads; if a block consists of 35 operations, it will be mapped to 2 warps, and so on). Each block can access a local cache (really fast) or a global memory (accessible by the entire *grid* of *blocks*), which is much slower. So, it is convenient to store data in the local (per block) cache, execute a computation, use the results (which are automatically stored in the same cache) for the next operations, and swap memory only at the end.

Note that CUDA 10.1 currently supports only gcc up to 7.3.0, and C++ 14.

Code is separated between host and device: the former is compiled by `gcc`, the latter by `nvcc`. A kernel is denoted by the keyword `__global__`, meaning a parallel function that runs on the device and it's called from the host. A kernel is executed with `mykernel<<<1,1>>>()`.

Kernels use **pointers** as argument. Note that pointers can reference device or host memory, they can be passed around, but only dereferenced in their native environment (a pointer pointing to memory on the host can be dereferenced only on the host). To pass data between host and device a copy operation is needed.

## Memory management
- On the **host**: the usual `malloc(), free(), memcpy()`
- On the **device**: `cudaMalloc(), cudaFree(), cudaMemcpy()`
Syntax is:
`cudaMemcpy(pointer to, pointer from, size to be copied, direction of copy (Host -> Device, or Device -> Host)`
where the direction is specified by the keywords `cudaMemcpyHostToDevice`, `cudaMemcpyHostToDevice`.
`cudaMalloc(address of pointer, n_bytes)` will modify the pointer to point to a location on the device. 

Allocations/copies can be done implicitly, but usually it is better to manually specify them (as they are computationally expensive).

## Kernel invocation
To execute a kernel the syntax is: `kernel_name<<<x,y>>>(args)`.
This command launches a **grid** (representing a task), made of $x$ blocks, each with $y$ threads ($x$ can be any, $y$ less than $1024$). Execution is asynchronous, and the synchronization is made on the following memory copy.

Each invocation can be tailored to a specific block by referring to local ids:
```cpp
__global__ void add(const int *a, const int *b, int *c) {
    c[blockIdx.x] = a[blockIdx.x] + b[blockIdx.x];
}
```
In that way each block does a *different* thing (memory checks apply, so you cannot go outside allocated bounds, otherwise the launch will fail). 

Actually, blocks are logically distributed in a 3D space - this is way we have `blockIdx`). So the full kernel invocation is:
```cpp
kernel_name<<<(x,y,z),(x,y,z)>>>(args)
```
where `(x,y,z)` is an object of class `dim3`.

The same tailoring can be done on the threads side, by using `threadIdx.x`. However, if there are multiple threads per block AND multiple blocks, note that the indexes for threads start at 0 at every block! So an unique identifier for each *specific* thread is:
```cpp
unsigned int index = threadIdx.x + blockIdx.x * M
```
where $M$ is the number of threads per block, which is stored in the built-in variable `blockDim.x`.

Usually, the number of operations to do will not be an exact multiple of `blockDim.x`, and so the `index` can easily go out of bounds. So it is better to insert a check: