# Shared Memory
Inside each *block*, threads can share resources. This can lead to a speedup if the kernel only uses elements in this shared memory (that can be controlled). Physically, these operations act on the L1 cache, the fastest one. Note that data shared in a block is not visible to threads in other blocks. 

Consider a *stencil kernel*, computing the sum of all elements inside a certain radius $r$ centered on each element.
```cpp
__global__ void stencil_1d(const int *in, int *out) {
    __shared__ int temp[BLOCK_SIZE + 2 * RADIUS]; //shared memory on this block (because a block is guaranteed to be executed on the same SM)
    //it is a scarce resource! See with deviceQuery
    //for device0 (49152 bytes/block, 1024 threads/block)

    int g_index = threadIdx.x + blockIdx.x * blockDim.x; //global index (unique identifier for threads)
    int s_index = threadIdx.x + RADIUS; //local index on this specific block + offset given by halo

    /*** Initialization ***/
    //check on g_index omitted for simplicity (assume to be in the middle of a large array)
    temp[s_index] = in[g_index]; //copy core part of the array in parallel (threadIdx.x starts from 0 and goes to blockDim.x)

    if (threadIdx.x < RADIUS) { //copy the halo parts
        temp[s_index - RADIUS] = in[g_index - RADIUS];
        temp[s_index + BLOCK_SIZE] = in[g_index + BLOCK_SIZE];
    }

    //here we have to be sure that temp is completely filled...
    __syncthreads(); //sync all threads (if removed, there will be a datarace!)

    /*** Apply the stencil ***/
    int result = 0; //for this thread
    for (int offset = -RADIUS; offset <= RADIUS; offset++)
        result += temp[s_index + offset];

    /*** Store the result ***/
    out[g_index] = result;
}
```
