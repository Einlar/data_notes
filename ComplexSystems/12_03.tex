%&latex
%
\documentclass[../template.tex]{subfiles}
\begin{document}

\section{Gaussian Integrals}
In Statistical Mechanics, many integrals involve \textbf{gaussian} functions. 

These can be computed analytically in some quite general cases, as we now show, starting from the simplest case:
\begin{align}\label{eqn:gauss-int}
    I(b) \equiv \int_{\mathbb{R}} \dd{x} e^{-ax^2 + bx} = \sqrt{\frac{\pi}{a} } \exp\left(\frac{b^2}{4a} \right) \qquad a \in \mathbb{R}^+, \> b \in \mathbb{C}
\end{align}

\textbf{Proof}. When $b \in \mathbb{R}$, we can just \textit{complete the square} in the exponential argument:
\begin{align*}
    -ax^2 + bx = -a\left(x-\frac{b}{2a} \right)^2 + \frac{b^2}{4a} 
\end{align*}  
So that the integral becomes:
\begin{align*}
    I(b) &= \int_{\mathbb{R}} \dd{x} \exp\Bigg[-\underbrace{a\left(x-\frac{b}{2a} \right)^2 }_{y^2}+ \frac{b^2}{4a} \Bigg]
\shortintertext{We then \textit{extract} the constant term from the integral, and change variables:}
y &= \left(x - \frac{b}{2a} \right) \sqrt{a}
\shortintertext{So that $I(b)$ reduces to computing the integral of a standard gaussian:}
I(b) &= \frac{1}{\sqrt{a}} \exp\left(\frac{b^2}{4a} \right)  \underbrace{\int_{\mathbb{R}} \dd{y} e^{-y^2} }_{\sqrt{\pi}} = \sqrt{\frac{\pi}{a} } \exp\left(\frac{b^2}{4a} \right)
\end{align*}

In the case of $b = i \alpha$ (pure imaginary) we have:
\begin{align*}
    I(i \alpha) = \varphi_0(\alpha)
\end{align*}
where $\varphi_0$ is the characteristic function for the gaussian (with $a=1/(2\sigma^2) \Rightarrow \sigma = (2a)^{-1/2}$) already computed in example \ref{exa:char-gauss} at pag. \pageref{exa:char-gauss}, which confirms the result (\ref{eqn:gauss-int}). %Ref. will appear when compilating the entire document

\begin{exo}[General case]
    Prove formula (\ref{eqn:gauss-int}) in the most general case, where $b = \beta + i \alpha$. 

    \medskip

    \textit{Hint: translate the integration path in the complex plane, close it with the real line and apply the Cauchy Integral theorem.} 
\end{exo}

It is useful to generalize (\ref{eqn:gauss-int}) to the multidimensional case.

Let $\bm{x} \in \mathbb{R}^d$ and $A$ a $d\times d$ \textbf{symmetric} and \textbf{positive definite} matrix, i.e. such that:
\begin{align}\label{eqn:pos-def}
    \bm{x}^T A \bm{x} \equiv \sum_{i,j=1}^d x_i A_{ij} x_j > 0 \qquad \forall \bm{x} \neq \bm{0}
\end{align}  
The multi-dimensional Gaussian integral is defined as:
\begin{align*}
    I(A) \equiv \int_{\mathbb{R}^d} \dd[d]{\bm{x}} e^{-\bm{x}^T A \bm{x}}
\end{align*}
To solve it, the idea is to \textit{decouple} all the $n$ components, so that the integral becomes the product of $n$ gaussian integrals of the type (\ref{eqn:gauss-int}).

\medskip

This is done by using the spectral decomposition of $A$. As it is symmetric, it has $d$ eigenvectors $\bm{v_{\cdot\alpha}} = (v_{1\alpha}, \dots, v_{d \alpha})^T$ of eigenvalue $\lambda_\alpha$ (with $\alpha =1, \dots, d$) that form an orthonormal basis of $\mathbb{R}^d$ (spectral theorem):
\begin{align*}
    \bm{v}_\alpha^T \bm{v_\beta} = \delta_{\alpha \beta}
\end{align*}
If we then use the definition of eigenvector:
\begin{align*}
    A \bm{v_\alpha} = \lambda_\alpha \bm{v_\alpha}
\end{align*}
we arrive to:
\begin{align}
    \bm{v_\beta}^T A \bm{v_\alpha} = \lambda_\alpha \delta_{\alpha \beta} \label{eqn:on-decomp1}
\end{align}
Let $V$ be the matrix with the orthonormal eigenvectors of $A$ as columns: $V=(v_{\alpha \beta})_{\alpha, \beta=i,\dots, d}$. Then (\ref{eqn:on-decomp1}) can be put in matrix form:
\begin{align}\label{eqn:decomp2}
    V^T A V = \Lambda \underset{(a)}{\Rightarrow}  A =V \Lambda V^T
\end{align}
where $\Lambda = \operatorname{diag}(\{\lambda_\alpha\}_{\alpha=1,\dots,d}) = (\delta_{\alpha \beta} \lambda_\alpha)_{\alpha \beta=1, \dots, d}$, and in (a)
we used the fact that $V$ is an orthogonal matrix (as its columns are orthogonal to each other), and so $V^{-1} = V^T$. Equation (\ref{eqn:decomp2}) gives the \textbf{spectral decomposition}  of $A$.

\medskip

Substituting in the integral we get:
\begin{align*}
    I(A) = \int_{\mathbb{R}^d} \dd[d]{\bm{x}} e^{-\bm{x}^T A \bm{x}} = \int_{\mathbb{R}^d} \dd[d]{\bm{x}} \exp(-\bm{x}^T V \Lambda V^T \bm{x})
\end{align*}
Then we change variables $\bm{y} = V^T \bm{x} \Leftrightarrow \bm{x} = V \bm{y}$. The determinant of the jacobian is unitary, because $V$ is orthogonal. In fact, starting from $V^T V = \mathbb{I}$, we have:
\begin{align}\nonumber
   1 = \operatorname{det} \mathbb{I} = \operatorname{det}(V^T V) \underset{(a)}{=}  (\operatorname{det}V^T) (\operatorname{det}V) \underset{(b)}{=} (\operatorname{det} V) (\operatorname{det} V) = (\operatorname{det} V)^2\\
   \Rightarrow |\operatorname{det} V|=1 \span \label{eqn:unitary-det}
\end{align}
where in (a) we applied Binet's formula, and in (b) $\operatorname{det} V^T = \operatorname{det} V$.  

\medskip

So we arrive to:
\begin{align*}
    I(A) = \int_{\mathbb{R}^d} \dd[d]{\bm{y}} e^{-\bm{y}^T \Lambda \bm{y}} = \prod_{\alpha=1}^d \int_{\mathbb{R}} \dd{y_\alpha} e^{-y_\alpha^2 \lambda_a} \underset{(a)}{=}  \prod_{\alpha=1}^d \sqrt{\frac{\pi}{\lambda_\alpha} }
\end{align*}
where in (a) we use (\ref{eqn:gauss-int}), as all components are now decoupled. 
To use (\ref{eqn:gauss-int}) we need all $\lambda_\alpha$ to be positive - which is indeed true, because the matrix $A$ is positive definite: 
\begin{align*}
    0 \underset{(\ref{eqn:pos-def})}{<}  \bm{v_\alpha}^T A \bm{v_\alpha} = \lambda_\alpha \bm{v_\alpha}^T \bm{v_\alpha} = \lambda_a \norm{\bm{v_\alpha}}^2 \Leftrightarrow \lambda_\alpha > 0
\end{align*}

Finally, note that:
\begin{align*}
    \operatorname{det}(A) \underset{(\ref{eqn:decomp2})}{=}
    \operatorname{det}(V \Lambda V^T) = 
    \textcolor{Red}{\cancel{\operatorname{det}(V^T)}} \operatorname{det}(\Lambda) \textcolor{Red}{\cancel{\operatorname{det}(V)}} \underset{(\ref{eqn:unitary-det})}{=}   \operatorname{det}\Lambda = \prod_{\alpha=1}^d \lambda_\alpha 
\end{align*}
And so we can rewrite:
\begin{align}\label{eqn:mult-gaussian}
    I(A) = \int_{\mathbb{R}^d} \dd[d]{\bm{x}} e^{-\bm{x}^T A \bm{x}} = \prod_{\alpha=1}^d \sqrt{\frac{\pi}{\lambda_\alpha}} = \pi^{d/2} (\operatorname{det}A )^{-1/2}
\end{align}

\begin{exo}[Generalization]
    Prove that:
    \begin{align*}
        I(A,\bm{b}) \equiv \int_{\mathbb{R}^d} \dd[d]{\bm{x}} e^{-\bm{x}^T A \bm{x} + \bm{b}^T \bm{x}} = \pi^{d/2} (\operatorname{det}A)^{-1/2} \exp\left(\frac{1}{4} \bm{b}^T A^{-1} \bm{b} \right)
    \end{align*}
    where $A$ is a \textbf{symmetric}, \textbf{positive definite} $d\times d$ real matrix, and $\bm{b} \in \mathbb{C}^d$.  
\end{exo}

\section{Saddle Point approximation}
Let $f(\bm{x})$ be a function with a single minimum at $\bm{x_0}$ and such that:
\begin{align*}
    I_f = \int_{D} e^{-N f(\bm{x})} \dd[d]{\bm{x}} \qquad D \subset \mathbb{R}^d
\end{align*}
Suppose that $\bm{x_0}$ \textit{is not on the boundary of $D$}, meaning that there exists some $r > 0$ so that the sphere centred on $\bm{x_0}$ of radius $r$ is entirely inside $D$:
\begin{align*}
    \exists r > 0 \text{ s.t. }\{\bm{x} \in \mathbb{R}^d \colon |\bm{x}-\bm{x_0}| < r\} \subset D
\end{align*} 
We want to show that:
\begin{align}\label{eqn:saddle-point}
    I_f \equiv \int_D e^{-N f(\bm{x})} \dd[d]{\bm{x}} = e^{-N f(\bm{x_0})} \left(\frac{2\pi}{N} \right)^{d/2} [\operatorname{det}(\partial_\alpha\partial_\beta f(\bm{x_0})) ]^{-1/2} \cdot \left[1+O\left(\frac{1}{N} \right)\right] \quad N \gg 1
\end{align}
where $\partial_\alpha \partial_\beta f(\bm{x_0})$ is the Hessian of $f(\bm{x})$ evaluated at $\bm{x} = \bm{x_0}$.

In other words, $I_f$ for a \textit{large enough} $N$, is equal (up to the gaussian pre-factor) to the integrand $e^{-N f(\bm{x})}$ evaluated at the minimum $\bm{x_0}$.

\medskip

To prove (\ref{eqn:saddle-point}) we start by Taylor expanding $f(\bm{x})$ around its minimum $\bm{x_0}$:
\begin{align*}
    f(\bm{x}) &= f(\bm{x_0}) + \sum_{\alpha=1}^d  [x_\alpha - (\bm{x_{0}})_\alpha] \partial_\alpha f(\bm{x_0}) + \frac{1}{2} \sum_{\alpha,\beta=1}^d [x_\alpha - (\bm{x_{0}})_\alpha] [x_\beta - (\bm{x_{0}})_\beta] \partial_\alpha \partial_\beta f(\bm{x_0}) + \\
    &\qquad\> + \frac{1}{3!} \sum_{\alpha,\beta, \gamma=1}^d [x_\alpha - (\bm{x_{0}})_\alpha][x_\beta - (\bm{x_{0}})_\beta] [x_\gamma - (\bm{x_{0}})_\gamma]\partial_\alpha \partial_\beta \partial_\gamma f(\bm{x_0}) + \dots
\end{align*}
Since $\bm{x_0}$ is a stationary point for $f$, all first derivatives vanish: $\partial_\alpha f(\bm{x_0}) = 0$. Substituting in the integral we get:
\begin{align*}
    I_f &= \int_D \dd[d]{\bm{x}}\exp\Bigg(-N \Big[f(\bm{x_0}) + \frac{1}{2} \sum_{\alpha,\beta=1}^d [x_\alpha - (\bm{x_{0}})_\alpha] [x_\beta - (\bm{x_{0}})_\beta] \partial_\alpha \partial_\beta f(\bm{x_0}) +\\
    &\quad \> + \frac{1}{3!} \sum_{\alpha,\beta, \gamma=1}^d [x_\alpha - (\bm{x_{0}})_\alpha][x_\beta - (\bm{x_{0}})_\beta] [x_\gamma - (\bm{x_{0}})_\gamma]\partial_\alpha \partial_\beta \partial_\gamma f(\bm{x_0}) + \dots
    \Big]\Bigg)
\end{align*}
Note that $f(\bm{x_0})$ is constant and can be brought outside the integral. The second term can be recognized as a multi-dimensional gaussian integral. In fact, the Hessian $\partial_\alpha \partial_\beta f(\bm{x_0})$ is surely symmetric (Schwartz theorem) and positive definite ($\bm{x_0}$ is a minimum). This suggests the following change of variable:
\begin{align}\label{eqn:cov1}
    \bm{y} = \sqrt{N} (\bm{x} - \bm{x_0}); \qquad \operatorname{det}\left|\pdv{\bm{x}}{\bm{y}} \right| = N^{-d/2}
\end{align}
leading to:
\begin{align*}
    I_f &= \frac{\exp(-N f(\bm{x_0}))}{N^{d/2}} \int_{D'} \dd[d]{\bm{y}} \exp\left(-\frac{1}{2} \bm{y}^T A \bm{y}\right) \cdot \\
    &\quad \> \cdot \exp\left( - \frac{1}{3! \sqrt{N}} \sum_{\alpha, \beta, \gamma=1}^d y_\alpha y_\beta y_\gamma \partial_\alpha \partial_\beta \partial_\gamma f(\bm{x_0}) + O\left(\frac{1}{N} \right) \right)
\end{align*}
where the matrix $A$ is the Hessian: $A_{\alpha \beta} \equiv \partial_\alpha \partial_\beta f(\bm{x_0})$, and $D'$ is the new integration domain. Finally, we expand the second exponential to first order ($e^{-z} = 1 - z + \cdots$):
\begin{align}\label{eqn:before1}
    I_f &= \frac{\exp(-N f(\bm{x_0}))}{N^{d/2}} \int_{D'} \dd[d]{\bm{y}} \hlc{Yellow}{\exp\left(-\frac{1}{2} \bm{y}^T A \bm{y}\right) }\cdot\\ \nonumber
    &\quad \> \cdot \left(\hlc{Yellow}{1}-\frac{1}{3! \sqrt{N}} \sum_{\alpha, \beta, \gamma=1}^d \hlc{SkyBlue}{y_\alpha y_\beta y_\gamma} \partial_\alpha \partial_\beta \partial_\gamma f(\bm{x_0}) + O\left(\frac{1}{N} \right)\right)
\end{align}
Note that the yellow term is \textit{even} in $\bm{y}$, while the blue one is \textit{odd}. So, when integrating, the third derivatives vanish, and only the even terms remain:
\begin{align} \nonumber
    I_f &= \frac{\exp(-N f(\bm{x_0}))}{N^{d/2}} \int_{D'} \dd[d]{\bm{y}} {\exp\left(-\frac{1}{\textcolor{Red}{2}} \bm{y}^T A \bm{y}\right) } \left(1+ O\left(\frac{1}{N} \right)\right) =\\
    \shortintertext{All that's left is to compute the Gaussian integral:}
    &\underset{(\ref{eqn:mult-gaussian})}{=}  e^{-N f(\bm{x_0})} \left(\frac{\textcolor{Red}{2} \pi}{N} \right)^{d/2} (\operatorname{det}A)^{-1/2} \label{eqn:result-saddle}
\end{align}
Note that we are implicitly assuming that $D'$ is symmetric about $\bm{x_0}$ (for the symmetry argument), and also that $D' = \mathbb{R}^d$ (for the gaussian integral). This is indeed true in the limit $N \to \infty$. In fact, we required that the original domain $D$ contains a (small) spherical neighbourhood of $\bm{x_0}$. Then, the change of variables (\ref{eqn:cov1}) \q{stretches} $D$, such that the size of the resulting $D'$ scales with $N$. It can be shown that approximating $D'$ with the entire $\mathbb{R}^d$ leads to an error that \textit{vanishes exponentially}, and which is $\ll O(1/N)$. This is explored explicitly in the $d=1$ in ex. \ref{ex:saddle-domain}.

\begin{expl}
    Q: Why is it called \textbf{saddle point method}?
\end{expl}

\begin{exo}[Gamma function]
    The $\Gamma$ function is defined as:
    \begin{align*}
        \Gamma(n) = \int_0^{+\infty} \dd{x} x^{n-1} e^{-x}
    \end{align*}
    Show that:
    \begin{enumerate}[label=\alph*.]
        \item $\Gamma(n+1) = n \Gamma(n)$. Since $\Gamma(1)=1$, we have that $\Gamma(n+1) = n!$ when $n \in \mathbb{N}$, meaning that the $\Gamma$ function is a \textit{generalization} of the factorial to the real case. 
        \item $\Gamma(n+1) = \sqrt{2 \pi n} \exp(n \ln n-n) (1+O(1/n))$. This leads to \textbf{Stirling's approximation}:
        \begin{align*}
            \ln n! \underset{n \gg 1}{\approx}  n \ln n - n + \frac{1}{2} \ln (2 \pi n) + O\left(\frac{1}{n} \right) 
        \end{align*} 
    \end{enumerate}
\end{exo}

\begin{exo}[10]\label{ex:saddle-domain}
    TO DO
\end{exo}

TO DO

\section{Statistical Ensembles}
%Following chap. 3 of Sethna, James. Statistical mechanics: entropy, order parameters, and complexity. Vol. 14. Oxford University Press, 2006

The goal of \textbf{equilibrium statistical mechanics} is to \textbf{connect} the macroscopic behaviour of a system to the dynamics of its microscopical constituents. This allows to \textit{rederive} the results of thermodynamics from a very different approach: instead of focusing directly on the observables, we try to deeply understand the \textit{nature} of the system at hand.

\medskip

We start our analysis from the case of a completely \textbf{isolated} system of $N$ particles, encased in a volume $V$, which does not exchange particles nor energy with the outside world.

Denote with $\bm{r_i}$ and $\bm{v_i}$, with $i=1, \dots, N$, the positions and velocities of the system's particles, and with $m_i$ their mass. We can organize all the positions and momenta in two $3N$-dimensional vectors:
\begin{align*}
    \mathbb{Q} &\equiv \Big( \underbrace{x_1, y_1, z_1}_{\bm{r_1}},\> \underbrace{x_2, y_2, z_2}_{\bm{r_2}},\> \dots,\> \underbrace{x_n, y_n, z_n}_{\bm{r_N}}   \Big) \in \mathbb{R}^{3N}\\
    \mathbb{P} &\equiv \Big(\underbrace{p_{1x}, p_{1y}, p_{1z}}_{\bm{p_1}=m_1 \bm{v_1}},\> \underbrace{p_{2x}, p_{2y}, p_{2z}}_{\bm{p_2} = m_2 \bm{v_2}}, \>\cdots,\> \underbrace{p_{Nx}, p_{Ny}, p_{Nz}}_{\bm{p_N} = m_N \bm{v_N}} \Big) \in \mathbb{R}^{3N}
\end{align*}


Let $\bm{F_i}$ be the force acting on the $i$-th particle. In the classical case, the system's dynamics is entirely determined by Newton's laws of motion, which - for the $i$-th particle - state:
\begin{align}\label{eqn:equations}
    \begin{dcases}
        \dot{\bm{r_i}} = \dv{\bm{r_i}}{t} = \frac{\dot{\bm{p_i}}}{m_i}\\ 
        \dot{\bm{p_i}} = \dv{\bm{p_i}}{t} = \bm{F_i}(\mathbb{Q})
    \end{dcases}
\end{align}

For a macroscopic system, $N$ is in the order of $10^{23}$ (Avogadro's number), meaning that:
\begin{enumerate}
    \item It is not feasible to solve so many equations at the same time. 
    \item Even if we were able to solve $1$, there would be no way to measure the required $6N$ initial conditions with a sufficient accuracy, nor to \textit{store} such amount of information.
    \item Even if $1$ and $2$ could be solved, the system's dynamics would likely
prove to be \textbf{chaotic}, meaning that even for a simple choice of interaction, the trajectories would be extremely sensitive to initial conditions. In practice this would severely limit the timescale at which the model is accurate.
\end{enumerate}
Furthermore, merely solving the dynamics with numerical methods does not produce any \textbf{understanding} of the system's behaviour - and so this huge computational task would give very little insight in the phenomena of interest.
In fact, we are more interested in \textit{global} observables, such as pressure or temperature, and not in the exact impact positions of molecules on a container's wall.

\medskip

So, instead of tackling the system in (\ref{eqn:equations}) directly, in Statistical Mechanics we take a different approach. Consider the system's \textit{phase space}, which is the space of coordinates $(\mathbb{Q},\mathbb{P}) \in \mathbb{R}^{6N}$. Equations (\ref{eqn:equations}) describe a trajectory in phase-space. Given the chaotic dynamics of the particles, we suppose that, at \textbf{equilibrium}, the system can reach any state in the phase-space (compatibly with energy conservation), independently of the starting condition.

Moreover, we assume that all \textit{possible states} (the ones with same energy) have the \textbf{same probability} of happening. We denote this postulate with $\mathcal{H}$. 

\medskip

All these states are \q{possible versions} of the same system. In principle, at any given time the system is at a given point $(\mathbb{Q}_0, \mathbb{P}_0)$, and follows a uniquely determined trajectory in phase-space. However, from the macroscopic point of view, all of these states are completely undistinguishable. So, with our imperfect knowledge, we can best describe the system only with a \textit{probability distribution}, and talk about \textit{expected values} of observables given that pdf. We call this distribution, made of \q{copies} of the system with very different microscopical dynamics but same macroscopic observables, a \textbf{statistical ensemble}. The $\mathcal{H}$ postulate tells us that this pdf should be \textit{uniform} - meaning that every state in the ensemble is treated equally. We call this specific pdf \textbf{microcanonical ensemble}.   

\medskip

So, let's find an expression for that probability distribution. We start by allowing the system to have an energy in an interval $(\mathcal{E}, \mathcal{E}+\delta E)$, and we will then take the limit $\delta \mathcal{E} \downarrow 0$. 

\medskip

This follows closely what can be done experimentally. In practice, we may not know the \textit{exact} value of $\mathcal{E}$ - every measurement will have a certain \textit{uncertainty}. So, to us, systems with very similar energies will look the same - meaning that we need to account for that uncertainty in the statistical ensemble that we are constructing\footnote{Recall that we introduced a probability distribution at first because of our imperfect knowledge of the microscopical states}. In principle, by repeating measurements for an \textit{infinite} time, we could \textit{restrict} the possible energy interval to a single value $\mathcal{E}'$.

\medskip

So, if the energy is in $(\mathcal{E}, \mathcal{E}+ \delta \mathcal{E})$, the possible states $(\mathbb{Q},\mathbb{P})$ in phase-space are contained in a \textit{thin} \textbf{energy shell} in that high-dimensional space, comprised between the hypersurfaces at fixed $\mathcal{E}$ and $\mathcal{E}+ \delta \mathcal{E}$. Then $\mathcal{H}$ states that we can describe an isolate system with a \textbf{uniform distribution} over such energy shell. 

%Insert picture TO DO

\medskip

The energy associated with a certain microstate is given by the \textbf{Hamiltonian}  function:
\begin{align*}
    H(\mathbb{Q},\mathbb{P}) = \sum_{i=1}^N \frac{\norm{\bm{p_i}}^2}{2m_i} + U(\mathbb{\mathbb{Q}}) 
\end{align*}
where $U(\mathbb{Q})$ is the potential energy associated with the (conservative) forces $\bm{F_i}$:
\begin{align*}
    \bm{F_i}(\mathbb{Q}) = - \grad_{\bm{r_i}} U = \left(-\pdv{U}{x_i}, - \pdv{U}{y_i}, -\pdv{U}{z_i}\right)^T
\end{align*}
We can rewrite Newton's equations (\ref{eqn:system}) by using the Hamiltonian as follows:
\begin{align*}
    \dot{q}_\alpha &=  \pdv{H(\mathbb{Q},\mathbb{P})}{p_\alpha} && q_\alpha \in \{x_i, y_i, z_i \colon i=1, \dots, N\}\\
    \dot{p}_\alpha &= -\pdv{H(\mathbb{Q},\mathbb{P})}{q_\alpha}
    && p_\alpha \in \{p_{xi}, p_{yi}, p_{zi} \colon i = 1, \dots, N\}
\end{align*}

So the energy shell $D$ between $\mathcal{E}$ and $\mathcal{E}+ \delta \mathcal{E}$ can be written as:
\begin{align*}
    D \equiv \{(\mathbb{Q},\mathbb{P}) \colon \mathcal{E} \leq H(\mathbb{Q}, \mathbb{P}) < \mathcal{E}+ \delta \mathcal{E}\} \subset \mathbb{R}^{6N}
\end{align*}
Then, as consequence of $\mathcal{H}$, we construct a uniform distribution over $D$:
\begin{align*}
    \mathbb{P}(\mathbb{Q},\mathbb{P}) &= \frac{1}{\mathcal{Z}} [\theta(\mathcal{E}+ \delta \mathcal{E} - H(\mathbb{Q},\mathbb{P})) - \theta(\mathcal{E}-H(\mathbb{Q},\mathbb{P}))] =\\
    &= \begin{cases}
        \frac{1}{\mathcal{Z}} & \mathcal{H}\in [\mathcal{E}, \mathcal{E}+\delta \mathcal{E}]\\
        0 & \text{otherwise} 
    \end{cases}
\end{align*}
where $\mathcal{Z}$ is just the normalization constant:
\begin{align}\label{eqn:Z-def}
    Z = \int_{\mathbb{R}^{6N}} \underbrace{\dd[3N]{\bm{q}}\dd[3N]{\bm{p}}}_{\dd{\mathbb{P}}} [\theta(\mathcal{E}+\delta \mathcal{E-H}) - \theta(\mathcal{E}-H)]
\end{align}
where $H\equiv H(\mathbb{Q},\mathbb{P})$ for brevity.

\medskip

We then expand the difference of $\theta$ functions to first order:
\begin{align} \label{eqn:heav-deriv}
    \theta(\mathcal{E}+\delta \mathcal{E}-H) - \theta(\mathcal{E}-H) = \delta \mathcal{E} \pdv{\mathcal{E}} \theta(\mathcal{E}-H) \approx \delta \mathcal{E}\, \delta(\mathcal{E}-H)
\end{align} 
where we used the \textit{distributional derivative} for the Heaviside function, which is the Dirac delta $\delta$. %Insert remainder to chap 2 of Baiesi notes 

We neglect all higher oders in $\delta\mathcal{E}$, as, as we will then send $\delta \mathcal{E} \to 0$.

\medskip

Substituting (\ref{eqn:heav-deriv}) in (\ref{eqn:Z-def}) we get:
\begin{align*}
    \mathcal{Z}= \int_{\mathbb{R}^{6N}} \dd{\mathbb{P}} \delta(\mathcal{E}-H)\delta \mathcal{E} \equiv \Omega(\mathcal{E}) \delta \mathcal{E}
\end{align*}
where we defined:
\begin{align*}
    \Omega(\mathcal{E}) = \int_{\mathbb{R}^{6N}} \dd{\mathbb{P}} \delta(\mathcal{E}-H(\mathbb{Q},\mathbb{P}))
\end{align*}
\end{document}

